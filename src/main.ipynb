{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JukQtCy2FUCf"
      },
      "outputs": [],
      "source": [
        "### DWI(ê¸°ìƒì§€ìˆ˜) ì‚°ì¶œì½”ë“œ\n",
        "\n",
        "# ë‚ ì§œÃ—ì¢Œí‘œë³„ë¡œ ERA5 ê¸°ë°˜ ê¸°í›„ë³€ìˆ˜(Tmean, RH, WSPD, TP_mm)ì™€ DEMì„ ì¶”ì¶œí•˜ëŠ” ì½”ë“œ\n",
        "# ì…ë ¥: CSV (í•„ìˆ˜ ì»¬ëŸ¼: date(YYYY-MM-DD), lat, lon  / ìˆœì„œëŠ” ë¬´ê´€)\n",
        "# ì¶œë ¥: fri_inputs_by_row.csv\n",
        "#   - ì»¬ëŸ¼: date, pid, lon, lat, Tmean(â„ƒ), RH(%), WSPD(m/s), TP_mm(mm/day), DEM(m)\n",
        "# íŠ¹ì§•:\n",
        "#   - ë‚ ì§œë³„ ë°°ì¹˜ ì²˜ë¦¬ë¡œ getInfo í˜¸ì¶œ ìˆ˜ë¥¼ ì œì–´\n",
        "#   - ERA5-Landì— ê°’ì´ ì—†ì„ ë•Œ ERA5(Global)ë¡œ ìë™ ë³´ì™„(unmask)\n",
        "#   - ERA5 í•´ìƒë„(~9km)ì— ë§ì¶˜ ê¸°ë³¸ scale/ë²„í¼ ì‚¬ìš©(í•„ìš”ì‹œ ì¡°ì • ê°€ëŠ¥)\n",
        "#   - ê°„ë‹¨í•œ QC ë¡œê·¸ë¡œ NaN ë¹„ìœ¨, ì¢Œí‘œ ë²”ìœ„ ë“±ì„ í™•ì¸\n",
        "\n",
        "!pip -q install earthengine-api pandas tqdm\n",
        "\n",
        "# 1) CSV ì—…ë¡œë“œ: ì¢Œí‘œÂ·ë‚ ì§œ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "from google.colab import files\n",
        "uploaded = files.upload()   # ì˜ˆ: latlon_with_date.csv\n",
        "CSV_FILENAME = next(iter(uploaded))\n",
        "CSV_PATH = f\"/content/{CSV_FILENAME}\"\n",
        "\n",
        "# 2) ê¸°ë³¸ ì„¤ì •: ìƒ˜í”Œ ë°˜ê²½, ìŠ¤ì¼€ì¼, ë‚ ì§œ ë³´ì • ì˜µì…˜\n",
        "# ERA5 ê²©ì í¬ê¸°ê°€ ì•½ 9kmì´ë¯€ë¡œ, ê¸°ë³¸ ë°˜ê²½ê³¼ ìŠ¤ì¼€ì¼ì„ 9000më¡œ ì„¤ì •\n",
        "SAMPLE_RADIUS_M = 9000      # í¬ì¸íŠ¸ ì£¼ë³€ í‰ê· ì„ ë‚¼ ì› ë°˜ê²½(m)\n",
        "SAMPLE_SCALE_M  = 9000      # reduceRegionsì— ì‚¬ìš©ë  ìŠ¤ì¼€ì¼(m)\n",
        "OUT_CSV = \"/content/fri_inputs_by_row.csv\"\n",
        "\n",
        "# KST ê¸°ì¤€ ì¼í‰ê· ì„ ë§ì¶”ê³  ì‹¶ì€ ê²½ìš° ë‚ ì§œë¥¼ í•˜ë£¨ ì´ë™ì‹œí‚¤ëŠ” ë“± ë³´ì • ê°€ëŠ¥\n",
        "# ê¸°ë³¸ì€ 0 (ë³´ì • ì—†ìŒ)\n",
        "DATE_SHIFT_DAYS = 0\n",
        "\n",
        "# 3) GEE ì´ˆê¸°í™”: í”„ë¡œì íŠ¸ ì„¤ì • ë° ì¸ì¦\n",
        "import ee, pandas as pd, datetime as dt\n",
        "from tqdm import tqdm\n",
        "\n",
        "PROJECT_ID = \"solid-time-472606-u0\"  # ë³¸ì¸ GEE í”„ë¡œì íŠ¸ ID\n",
        "try:\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "except Exception:\n",
        "    ee.Authenticate(project=PROJECT_ID)\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "print(\"[GEE] initialized\")\n",
        "\n",
        "# 4) ì…ë ¥ ë¡œë“œ ë° ì •ê·œí™”: í—¤ë” ì •ë¦¬, í˜• ë³€í™˜, í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬\n",
        "# - BOM ì œê±°, ì†Œë¬¸ì í—¤ë” í†µì¼, date/lat/lon í˜•ì‹ ì •ë¦¬, ê²°ì¸¡/ì¤‘ë³µ ì œê±°\n",
        "df = pd.read_csv(CSV_PATH, encoding=\"utf-8-sig\", engine=\"python\")\n",
        "df.columns = [c.strip().lower().replace(\"\\ufeff\", \"\") for c in df.columns]\n",
        "\n",
        "need = {\"lat\", \"lon\", \"date\"}\n",
        "if not (need <= set(df.columns)):\n",
        "    raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½. í˜„ì¬ í—¤ë”: {list(df.columns)} (í•„ìš”: {sorted(need)})\")\n",
        "\n",
        "df[\"lon\"]  = pd.to_numeric(df[\"lon\"], errors=\"coerce\")\n",
        "df[\"lat\"]  = pd.to_numeric(df[\"lat\"], errors=\"coerce\")\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
        "\n",
        "# ë‚ ì§œ ë³´ì •ì´ í•„ìš”í•  ë•Œë§Œ ì ìš© (UTC/KST ê²½ê³„ ì´ìŠˆ ì™„í™”ìš©)\n",
        "if DATE_SHIFT_DAYS != 0:\n",
        "    df[\"date\"] = df[\"date\"].apply(\n",
        "        lambda d: (dt.date.fromisoformat(str(d)) + dt.timedelta(days=DATE_SHIFT_DAYS))\n",
        "        if pd.notnull(d) else d\n",
        "    )\n",
        "\n",
        "df = df.dropna(subset=[\"lon\",\"lat\",\"date\"]).drop_duplicates().reset_index(drop=True)\n",
        "df[\"pid\"] = df.index.astype(int)\n",
        "\n",
        "print(f\"[INFO] rows after cleaning: {len(df)}\")\n",
        "print(df.head())\n",
        "\n",
        "# 5) GEE ë°ì´í„°ì…‹ ì •ì˜: ERA5(ìœ¡ì§€/ì „ì§€êµ¬)ì™€ DEM(SRTM)\n",
        "ERA5_LAND   = ee.ImageCollection(\"ECMWF/ERA5_LAND/HOURLY\")\n",
        "ERA5_GLOBAL = ee.ImageCollection(\"ECMWF/ERA5/HOURLY\")       # ë°”ë‹¤ í¬í•¨\n",
        "SRTM        = ee.Image(\"USGS/SRTMGL1_003\").select(\"elevation\").rename(\"DEM\")\n",
        "\n",
        "# 6) ì‹œê°„ë³„ ERA5 ì´ë¯¸ì§€ë¥¼ ì¼ ë‹¨ìœ„ ìš”ì•½ìœ¼ë¡œ ë³€í™˜\n",
        "def _per_hour_to_vars(im):\n",
        "    # ê¸°ì˜¨(K)ì„ ì„­ì”¨(â„ƒ)ë¡œ ë³€í™˜\n",
        "    T  = im.select(\"temperature_2m\").subtract(273.15).rename(\"Tmean\")\n",
        "    Td = im.select(\"dewpoint_temperature_2m\").subtract(273.15)\n",
        "    # ìƒëŒ€ìŠµë„(%): Magnus ê³µì‹ ê¸°ë°˜ ê·¼ì‚¬\n",
        "    a, b = 17.625, 243.04\n",
        "    RH = Td.expression(\n",
        "        \"100*exp(a*Td/(b+Td) - a*T/(b+T))\",\n",
        "        {\"a\": a, \"b\": b, \"Td\": Td, \"T\": T}\n",
        "    ).rename(\"RH\")\n",
        "    # í’ì†(m/s): u, v ì„±ë¶„ìœ¼ë¡œë¶€í„° ê³„ì‚°\n",
        "    U = im.select(\"u_component_of_wind_10m\")\n",
        "    V = im.select(\"v_component_of_wind_10m\")\n",
        "    WSPD = U.pow(2).add(V.pow(2)).sqrt().rename(\"WSPD\")\n",
        "    # ê°•ìˆ˜ëŸ‰: ëˆ„ì  ê°•ìˆ˜(m)ë¥¼ mmë¡œ í™˜ì‚°\n",
        "    TP = im.select(\"total_precipitation\").multiply(1000).rename(\"TP_mm\")\n",
        "    return T.addBands([RH, WSPD, TP])\n",
        "\n",
        "def _daily_from(ic, date_str):\n",
        "    # ì£¼ì–´ì§„ ë‚ ì§œ [d0, d1) êµ¬ê°„ì˜ ì‹œê°„ ìë£Œë¥¼ ì¼ í‰ê· /ì¼í•©ê³„ë¡œ ì§‘ê³„\n",
        "    d0 = ee.Date(date_str); d1 = d0.advance(1, \"day\")\n",
        "    hourly = ic.filterDate(d0, d1).map(_per_hour_to_vars)\n",
        "    Tmean = hourly.select(\"Tmean\").mean()\n",
        "    RH    = hourly.select(\"RH\").mean()\n",
        "    WSPD  = hourly.select(\"WSPD\").mean()\n",
        "    TP    = hourly.select(\"TP_mm\").sum()\n",
        "    return Tmean.addBands([RH, WSPD, TP])\n",
        "\n",
        "def daily_era5(date_str):\n",
        "    # ERA5-Land ê°’ì„ ìš°ì„  ì‚¬ìš©, ë§ˆìŠ¤í¬ëœ í”½ì…€ì€ ERA5(Global)ë¡œ ë³´ì™„\n",
        "    land = _daily_from(ERA5_LAND, date_str)\n",
        "    globe = _daily_from(ERA5_GLOBAL, date_str)\n",
        "    fused = land.unmask(globe)\n",
        "    # DEM ë°´ë“œì™€ ë‚ ì§œ ì†ì„± ì¶”ê°€\n",
        "    return fused.addBands(SRTM).set({\"date\": date_str})\n",
        "\n",
        "# 7) reduceRegions ê²°ê³¼ì—ì„œ ì›í•˜ëŠ” í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” í—¬í¼\n",
        "def pick(p, *names):\n",
        "    for k in names:\n",
        "        v = p.get(k)\n",
        "        if v is not None:\n",
        "            return v\n",
        "    return None\n",
        "\n",
        "# 8) ë‚ ì§œë³„ë¡œ ë°°ì¹˜ ìƒ˜í”Œë§: ê°™ì€ ë‚ ì§œì˜ í¬ì¸íŠ¸ë¥¼ í•œ ë²ˆì— reduceRegions\n",
        "rows_out = []\n",
        "unique_dates = sorted({d.isoformat() for d in df[\"date\"].unique()})\n",
        "print(f\"[INFO] unique dates in CSV: {len(unique_dates)} â†’ {unique_dates[:5]}{' ...' if len(unique_dates)>5 else ''}\")\n",
        "\n",
        "for d in tqdm(unique_dates, desc=\"[sampling by date]\"):\n",
        "    # í˜„ì¬ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” í–‰ë§Œ ì„ íƒ\n",
        "    sub = df[df[\"date\"] == dt.date.fromisoformat(d)].copy()\n",
        "\n",
        "    # ê° í¬ì¸íŠ¸ë¥¼ ë²„í¼(ì›ì˜ì—­)ë¡œ í™•ì¥í•œ FeatureCollection ìƒì„±\n",
        "    fc = ee.FeatureCollection([\n",
        "        ee.Feature(\n",
        "            ee.Geometry.Point([float(r[\"lon\"]), float(r[\"lat\"])]).buffer(SAMPLE_RADIUS_M),\n",
        "            {\"pid\": int(r[\"pid\"]), \"lon\": float(r[\"lon\"]), \"lat\": float(r[\"lat\"])}\n",
        "        )\n",
        "        for _, r in sub.iterrows()\n",
        "    ])\n",
        "\n",
        "    # í•´ë‹¹ ë‚ ì§œì˜ ì¼ì¼ ERA5+DEM ì´ë¯¸ì§€ ìƒì„±\n",
        "    img = daily_era5(d)\n",
        "\n",
        "    # ë²„í¼ ì˜ì—­ì— ëŒ€í•´ í”½ì…€ í‰ê· (reduceRegions) ìˆ˜í–‰\n",
        "    reduced = img.reduceRegions(\n",
        "        collection=fc,\n",
        "        reducer=ee.Reducer.mean(),\n",
        "        scale=SAMPLE_SCALE_M\n",
        "    ).map(lambda f: f.set({\"date\": d}))\n",
        "\n",
        "    # ê²°ê³¼ë¥¼ í´ë¼ì´ì–¸íŠ¸ë¡œ ê°€ì ¸ì™€ ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì \n",
        "    feats = reduced.getInfo().get(\"features\", [])\n",
        "    for f in feats:\n",
        "        p = f.get(\"properties\", {})\n",
        "        rows_out.append({\n",
        "            \"date\": p.get(\"date\"),\n",
        "            \"pid\":  p.get(\"pid\"),\n",
        "            \"lon\":  p.get(\"lon\"),\n",
        "            \"lat\":  p.get(\"lat\"),\n",
        "            \"Tmean\": pick(p, \"Tmean_mean\", \"Tmean\"),\n",
        "            \"RH\":    pick(p, \"RH_mean\", \"RH\"),\n",
        "            \"WSPD\":  pick(p, \"WSPD_mean\", \"WSPD\"),\n",
        "            \"TP_mm\": pick(p, \"TP_mm_mean\", \"TP_mm\"),\n",
        "            \"DEM\":   pick(p, \"DEM_mean\", \"DEM\", \"elevation_mean\", \"elevation\"),\n",
        "        })\n",
        "\n",
        "# 9) ê²°ê³¼ ì €ì¥, ê°„ë‹¨ QC, íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
        "df_out = pd.DataFrame(rows_out).sort_values([\"date\",\"pid\"]).reset_index(drop=True)\n",
        "df_out.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "# ê²°ì¸¡ì¹˜ QC: ëª¨ë“  ê¸°ìƒë³€ìˆ˜ê°€ NaNì¸ í–‰ê³¼ ì¼ë¶€ë§Œ NaNì¸ í–‰ ê°œìˆ˜ í™•ì¸\n",
        "num_total   = len(df_out)\n",
        "num_all_nan = df_out[[\"Tmean\",\"RH\",\"WSPD\",\"TP_mm\"]].isna().all(axis=1).sum()\n",
        "num_any_nan = df_out[[\"Tmean\",\"RH\",\"WSPD\",\"TP_mm\"]].isna().any(axis=1).sum()\n",
        "print(f\"[SAVED] {OUT_CSV}  rows={num_total}\")\n",
        "print(f\"[QC] all-NaN rows = {num_all_nan} / any-NaN rows = {num_any_nan}\")\n",
        "\n",
        "# í•œêµ­ ëŒ€ëµ ê²½ê³„(ìœ„ë„ 33~39, ê²½ë„ 124~132) ë²—ì–´ë‚œ ì¢Œí‘œê°€ ìˆëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸\n",
        "bad = df[(df[\"lat\"]<33)|(df[\"lat\"]>39)|(df[\"lon\"]<124)|(df[\"lon\"]>132)]\n",
        "if len(bad):\n",
        "    print(\"[WARN] KR bounds outliers (first 5):\")\n",
        "    print(bad[[\"pid\",\"date\",\"lat\",\"lon\"]].head())\n",
        "\n",
        "from google.colab import files\n",
        "files.download(OUT_CSV)\n",
        "\n",
        "### FMI(ì„ìƒì§€ìˆ˜) ì‚°ì¶œì½”ë“œ\n",
        "\n",
        "# ì„ìƒë„ ì½”ë“œ(forest)ë¥¼ ì‚°ë¦¼ ìœ í˜•ë³„ FMIë¡œ ë³€í™˜í•˜ëŠ” ì½”ë“œ\n",
        "# ì…ë ¥: ì—‘ì…€/CSV 1ê°œ (forest ì½”ë“œ ì¹¼ëŸ¼ í¬í•¨)\n",
        "# ì¶œë ¥: ì›ë³¸ + FMI, FMI_missing ì»¬ëŸ¼ì´ ì¶”ê°€ëœ CSV\n",
        "\n",
        "# 0) ì¤€ë¹„: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import io, os, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# 1) íŒŒì¼ ì—…ë¡œë“œ: ì—‘ì…€ ë˜ëŠ” CSV 1ê°œ ì„ íƒ\n",
        "uploaded = files.upload()  # .xlsx, .xls, .csv ëª¨ë‘ í—ˆìš©\n",
        "assert len(uploaded) == 1, \"íŒŒì¼ì„ í•˜ë‚˜ë§Œ ì—…ë¡œë“œí•˜ì„¸ìš”.\"\n",
        "in_fname = list(uploaded.keys())[0]\n",
        "raw = uploaded[in_fname]\n",
        "\n",
        "# 2) íŒŒì¼ ë¡œë”© í—¬í¼: í™•ì¥ìì™€ ì¸ì½”ë”©ì„ ìë™ íŒë³„í•˜ì—¬ DataFrameìœ¼ë¡œ ë¡œë“œ\n",
        "def load_table(name: str, buf: bytes) -> pd.DataFrame:\n",
        "    lower = name.lower()\n",
        "    if lower.endswith((\".xlsx\", \".xls\")):\n",
        "        return pd.read_excel(io.BytesIO(buf))  # ì²« ì‹œíŠ¸ ë¡œë“œ\n",
        "    elif lower.endswith(\".csv\"):\n",
        "        # CSVëŠ” utf-8-sig ìš°ì„ , ì‹¤íŒ¨ ì‹œ cp949 ì‹œë„\n",
        "        try:\n",
        "            return pd.read_csv(io.StringIO(buf.decode(\"utf-8-sig\")))\n",
        "        except Exception:\n",
        "            return pd.read_csv(io.StringIO(buf.decode(\"cp949\")))\n",
        "    else:\n",
        "        raise ValueError(\"ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ìì…ë‹ˆë‹¤. .xlsx/.xls/.csv íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”.\")\n",
        "\n",
        "df = load_table(in_fname, raw)\n",
        "\n",
        "# 3) forest ì½”ë“œ ì»¬ëŸ¼ëª… ì„¤ì • (íŒŒì¼ì— ë§ê²Œ í•„ìš” ì‹œ ì—¬ê¸°ë§Œ ìˆ˜ì •)\n",
        "FOREST_COL = \"forest\"\n",
        "\n",
        "if FOREST_COL not in df.columns:\n",
        "    raise KeyError(f\"ì—…ë¡œë“œí•œ íŒŒì¼ì— '{FOREST_COL}' ì¹¼ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¹¼ëŸ¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "\n",
        "# 4) FMI ë§¤í•‘ í•¨ìˆ˜: ì‚°ë¦¼ì²­ ì½”ë“œ êµ¬ê°„ì„ FMI ì ìˆ˜(ìˆ«ì)ë¡œ ë³€í™˜\n",
        "def map_code_to_fmi(raw_code):\n",
        "    try:\n",
        "        if pd.isna(raw_code):\n",
        "            return np.nan\n",
        "        code = int(raw_code)\n",
        "\n",
        "        # ì¹¨ì—½ìˆ˜ë¦¼(10~21) â†’ FMI=10\n",
        "        if 10 <= code <= 21:\n",
        "            return 10\n",
        "        # í™œì—½ìˆ˜ë¦¼(30~49, 60~68) â†’ FMI=2\n",
        "        elif (30 <= code <= 49) or (60 <= code <= 68):\n",
        "            return 2\n",
        "        # í˜¼íš¨ë¦¼(77) â†’ FMI=3\n",
        "        elif code == 77:\n",
        "            return 3\n",
        "        # ì£½ë¦¼(78) â†’ FMI=8\n",
        "        elif code == 78:\n",
        "            return 8\n",
        "        # ë¬´ë¦½ëª©ì§€(81,82) â†’ FMI=5\n",
        "        elif code in (81, 82):\n",
        "            return 5\n",
        "        # ë¹„ì‚°ë¦¼(91~99, -1) â†’ FMI=0\n",
        "        elif (91 <= code <= 99) or (code == -1):\n",
        "            return 0\n",
        "        # ì •ì˜ë˜ì§€ ì•Šì€ ì½”ë“œ â†’ NaN\n",
        "        else:\n",
        "            return np.nan\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "# 5) FMI ê³„ì‚° ë° ë³´ì¡° í”Œë˜ê·¸ ìƒì„±\n",
        "df[\"FMI\"] = df[FOREST_COL].apply(map_code_to_fmi)\n",
        "# FMI_missing: ë§¤í•‘ ì‹¤íŒ¨ ë˜ëŠ” ê²°ì¸¡(1ì´ë©´ FMIê°€ ë¹„ì–´ìˆìŒ)\n",
        "df[\"FMI_missing\"] = df[\"FMI\"].isna().astype(int)\n",
        "\n",
        "# (ì„ íƒ) forest_type(ë²”ì£¼í˜•)ê¹Œì§€ í•„ìš”í•˜ë©´ ì•„ë˜ ë¸”ë¡ ì£¼ì„ í•´ì œ\n",
        "# def map_forest_type(code):\n",
        "#     try:\n",
        "#         if pd.isna(code): return \"unknown\"\n",
        "#         c = int(code)\n",
        "#         if 10 <= c <= 21: return \"conifer\"\n",
        "#         if (30 <= c <= 49) or (60 <= c <= 68): return \"broadleaf\"\n",
        "#         if c == 77: return \"mixed\"\n",
        "#         if c == 78: return \"bamboo\"\n",
        "#         if c in (81, 82): return \"grassland\"\n",
        "#         if (91 <= c <= 99) or (c == -1): return \"nonforest\"\n",
        "#         return \"unknown\"\n",
        "#     except Exception:\n",
        "#         return \"unknown\"\n",
        "# df[\"forest_type\"] = df[FOREST_COL].apply(map_forest_type)\n",
        "\n",
        "# 6) ê²°ê³¼ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ: ì› íŒŒì¼ëª…ì— _with_fmi ë¶™ì—¬ CSV ì €ì¥\n",
        "base = re.sub(r\"\\.(xlsx|xls|csv)$\", \"\", in_fname, flags=re.IGNORECASE)\n",
        "out_csv = f\"{base}_with_fmi.csv\"\n",
        "df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
        "files.download(out_csv)\n",
        "\n",
        "# (ì„ íƒ) ì—‘ì…€ë¡œë„ ë‚´ë³´ë‚´ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ\n",
        "# out_xlsx = f\"{base}_with_fmi.xlsx\"\n",
        "# df.to_excel(out_xlsx, index=False)\n",
        "# files.download(out_xlsx)\n",
        "\n",
        "### TMI(ì§€í˜•ì§€ìˆ˜) ì‚°ì¶œì½”ë“œ\n",
        "\n",
        "# DEMì„ ì´ìš©í•´ ê° ì¢Œí‘œì˜ Slope(ê²½ì‚¬ë„)ì™€ TMI(ê°„ì´ ì§€í˜•ì§€ìˆ˜)ë¥¼ ê³„ì‚°í•˜ëŠ” ì½”ë“œ\n",
        "# ì…ë ¥: CSV (í•„ìˆ˜ ì»¬ëŸ¼: lon, lat)\n",
        "# ì¶œë ¥: ì›ë³¸ + DEM, Slope_deg, TMIê°€ ì¶”ê°€ëœ points_with_topography.csv\n",
        "\n",
        "!pip -q install earthengine-api pandas chardet\n",
        "\n",
        "import ee, pandas as pd, math, io, chardet\n",
        "from google.colab import files\n",
        "\n",
        "# 0) ì„¤ì •: GEE í”„ë¡œì íŠ¸, DEM ìì‚°, TMI ì»¤ë„ ë°˜ê²½, ì¶œë ¥ íŒŒì¼ëª…\n",
        "PROJECT_ID = 'solid-time-472606-u0'         # ë³¸ì¸ GEE í”„ë¡œì íŠ¸ ID\n",
        "DEM_ASSET  = 'USGS/SRTMGL1_003'            # ë‹¤ë¥¸ DEM ìì‚°ì´ ìˆë‹¤ë©´ êµì²´ ê°€ëŠ¥\n",
        "TMI_KERNEL_M = 300                          # TMI ê³„ì‚°ì— ì‚¬ìš©í•  ì£¼ë³€ ë°˜ê²½(ë¯¸í„°)\n",
        "OUTPUT_CSV = 'points_with_topography.csv'\n",
        "\n",
        "# 1) GEE ì´ˆê¸°í™”: í”„ë¡œì íŠ¸ ê¸°ë°˜ ì¸ì¦ ë° ì—°ê²°\n",
        "try:\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "    print(f'[GEE] Initialized with project = {PROJECT_ID}')\n",
        "except Exception:\n",
        "    print('[GEE] OAuth required. Follow the popupâ€¦')\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "\n",
        "# 2) CSV ì—…ë¡œë“œ: ì¢Œí‘œ íŒŒì¼ ì„ íƒ ë° ì¸ì½”ë”© ìë™ íŒë³„\n",
        "print('[Upload] CSV íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (lon, lat í•„ìˆ˜).')\n",
        "uploaded = files.upload()\n",
        "assert len(uploaded) == 1, 'í•œ ê°œì˜ CSVë§Œ ì—…ë¡œë“œí•˜ì„¸ìš”.'\n",
        "fname, raw = next(iter(uploaded.items()))\n",
        "\n",
        "# ì¸ì½”ë”© ìë™ íŒë³„ í›„, ì‹¤íŒ¨ ì‹œ í”í•œ ì¸ì½”ë”©ìœ¼ë¡œ ì¬ì‹œë„\n",
        "enc = chardet.detect(raw)['encoding'] or 'utf-8'\n",
        "try:\n",
        "    df = pd.read_csv(io.BytesIO(raw), encoding=enc)\n",
        "except Exception:\n",
        "    for alt in ['utf-8-sig','cp949','euc-kr']:\n",
        "        try:\n",
        "            df = pd.read_csv(io.BytesIO(raw), encoding=alt)\n",
        "            enc = alt\n",
        "            break\n",
        "        except Exception:\n",
        "            pass\n",
        "print(f'[Upload] Loaded \"{fname}\" with encoding={enc}')\n",
        "\n",
        "# í•„ìˆ˜ ì¢Œí‘œ ì»¬ëŸ¼ í™•ì¸\n",
        "assert {'lon','lat'}.issubset(df.columns), \"CSVì— lon, lat ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\"\n",
        "\n",
        "# ê° í–‰ì— ê³ ìœ  row_idë¥¼ ë¶€ì—¬í•˜ì—¬ GEE ê²°ê³¼ì™€ ë‹¤ì‹œ ë§¤ì¹­\n",
        "df = df.reset_index().rename(columns={'index':'row_id'})\n",
        "\n",
        "# 3) í¬ì¸íŠ¸ë¥¼ GEE FeatureCollectionìœ¼ë¡œ ë³€í™˜\n",
        "def row_to_feature(row):\n",
        "    return ee.Feature(\n",
        "        ee.Geometry.Point([float(row['lon']), float(row['lat'])]),\n",
        "        {'row_id': int(row['row_id'])}\n",
        "    )\n",
        "fc = ee.FeatureCollection([row_to_feature(r) for r in df.to_dict('records')])\n",
        "\n",
        "# 4) DEM, ê²½ì‚¬ë„, TMI ê³„ì‚°\n",
        "dem   = ee.Image(DEM_ASSET).select(['elevation']).rename('DEM')\n",
        "slope = ee.Terrain.slope(dem).rename('Slope_deg')\n",
        "slope_rad = slope.multiply(math.pi/180.0)\n",
        "\n",
        "# ì£¼ë³€ ì§‘ìˆ˜ë©´ì  ê·¼ì‚¬(A_local): ë°˜ê²½ TMI_KERNEL_M ë‚´ í”½ì…€ ë©´ì  í•©\n",
        "pixel_area = ee.Image.pixelArea()\n",
        "kernel     = ee.Kernel.circle(TMI_KERNEL_M, 'meters', normalize=False)\n",
        "A_local    = pixel_area.reduceNeighborhood(ee.Reducer.sum(), kernel).rename('A_local_m2')\n",
        "\n",
        "# TMI â‰ˆ ln(A_local) - ln(tan(slope)), ê¸‰ê²½ì‚¬ëŠ” tan(slope) ê°’ì´ ì»¤ì§\n",
        "eps = ee.Number(1e-6)  # 0 íšŒí”¼ìš© ì‘ì€ ê°’\n",
        "tmi = A_local.add(1).log().subtract(slope_rad.tan().add(eps).log()).rename('TMI')\n",
        "\n",
        "# DEM, Slope_deg, TMIë¥¼ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ ê²°í•©\n",
        "topo_img = ee.Image.cat([dem, slope, tmi])\n",
        "\n",
        "# 5) sampleRegionsë¡œ ê° í¬ì¸íŠ¸ ìœ„ì¹˜ì—ì„œ DEM, Slope_deg, TMI ì¶”ì¶œ\n",
        "samples = topo_img.sampleRegions(\n",
        "    collection=fc,\n",
        "    properties=['row_id'],\n",
        "    scale=30,           # DEM í•´ìƒë„(ì•½ 30m)ì— ë§ì¶° ìƒ˜í”Œë§\n",
        "    geometries=False\n",
        ")\n",
        "\n",
        "# ê²°ê³¼ë¥¼ í´ë¼ì´ì–¸íŠ¸ë¡œ ë‚´ë ¤ë°›ì•„ DataFrameìœ¼ë¡œ ë³€í™˜\n",
        "res = samples.getInfo()\n",
        "rows = [f['properties'] for f in res['features']]\n",
        "topo_df = pd.DataFrame(rows)  # row_id, DEM, Slope_deg, TMI\n",
        "\n",
        "# ì›ë³¸ dfì™€ ê³„ì‚° ê²°ê³¼ë¥¼ row_id ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©\n",
        "out = df.merge(topo_df, on='row_id', how='left').drop(columns=['row_id'])\n",
        "\n",
        "# 6) CSV ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ\n",
        "out.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')\n",
        "files.download(OUTPUT_CSV)\n",
        "print(f'[DONE] Saved & Downloaded â†’ {OUTPUT_CSV}')\n",
        "\n",
        "\n",
        "### ì¢Œí‘œì¡°ì •\n",
        "\n",
        "# fmi == -1 ì¸ ì¢Œí‘œë¥¼ ERA5-Land ì˜¨ë„ì¥ì„ ì´ìš©í•´\n",
        "# ê°™ì€ ì‹œê°„Â·ë‚ ì§œ ì£¼ë³€ì˜ \"ìµœê³  ê¸°ì˜¨ í”½ì…€\" ìœ„ì¹˜ë¡œ ì´ë™ì‹œí‚¤ëŠ” ìŠ¤í¬ë¦½íŠ¸\n",
        "# ì…ë ¥:\n",
        "#   - CSV (í•„ìˆ˜ ì»¬ëŸ¼: lat, lon, fmi, date ë˜ëŠ” datetime)\n",
        "#   - date: YYYY-MM-DD, datetime: YYYY-MM-DD HH:MM:SS\n",
        "# ì‚¬ìš© ë°ì´í„°:\n",
        "#   - ECMWF/ERA5_LAND/HOURLY ì˜ temperature_2m\n",
        "# ì¶œë ¥:\n",
        "#   - /content/updated_coords.csv        : ì¢Œí‘œ ë³´ì • ê²°ê³¼ (utf-8-sig, Excel í˜¸í™˜)\n",
        "#   - /content/update_summary.txt        : ì „ì²´ ê°±ì‹  í†µê³„ ìš”ì•½\n",
        "#   - /content/update_failures.csv       : ëŒ€ì²´ ì‹¤íŒ¨ í–‰ (ìˆì„ ë•Œë§Œ ì €ì¥)\n",
        "\n",
        "!pip -q install earthengine-api pandas tqdm\n",
        "\n",
        "import ee, pandas as pd, numpy as np, datetime as dt\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# 0) ì„¤ì •: GEE í”„ë¡œì íŠ¸, íƒìƒ‰ ë°˜ê²½, ì‹œê°„ì°½, ì¶œë ¥ ì˜µì…˜\n",
        "PROJECT_ID        = 'solid-time-472606-u0'   # ë³¸ì¸ GEE í”„ë¡œì íŠ¸ ID\n",
        "BUFFER_KM_STEPS   = [5, 8, 12]              # ì£¼ë³€ íƒìƒ‰ ë°˜ê²½(km), ë‹¨ê³„ì ìœ¼ë¡œ í™•ì¥\n",
        "SCALE_M           = 2000                    # ìƒ˜í”Œ í•´ìƒë„(m), ì‘ì„ìˆ˜ë¡ ì •ë°€Â·ëŠë¦¼\n",
        "HOUR_MARGIN       = 1                       # datetime ê¸°ì¤€ Â±HOUR_MARGIN ì‹œê°„ì°½\n",
        "VERBOSE           = True                    # ì¤‘ê°„ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€\n",
        "\n",
        "# 1) GEE ì´ˆê¸°í™”: í”„ë¡œì íŠ¸ ê¸°ë°˜ ì¸ì¦ ë° ì—°ê²°\n",
        "try:\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "except Exception:\n",
        "    ee.Authenticate(project=PROJECT_ID)\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "print('[GEE] Initialized')\n",
        "\n",
        "# 2) íŒŒì¼ ì—…ë¡œë“œ: ì¢Œí‘œÂ·ë‚ ì§œÂ·fmi ì •ë³´ê°€ ë‹´ê¸´ CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "print(\"ğŸ”¹ CSV íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì˜ˆ: input_points.csv)\")\n",
        "uploaded = files.upload()\n",
        "CSV_PATH = list(uploaded.keys())[0]\n",
        "print(f\"ì—…ë¡œë“œ ì™„ë£Œ: {CSV_PATH}\")\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬: lat, lon, fmi ì™€ date ë˜ëŠ” datetime í•„ìš”\n",
        "need = {'lat','lon','fmi'}\n",
        "assert need.issubset(df.columns), f\"CSVì— {need} ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\"\n",
        "has_datetime = 'datetime' in df.columns\n",
        "has_date     = 'date' in df.columns\n",
        "assert has_datetime or has_date, \"CSVì— datetime ë˜ëŠ” date ì»¬ëŸ¼ ì¤‘ í•˜ë‚˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\"\n",
        "\n",
        "# datetime/date íŒŒì‹±: Excel í˜¸í™˜ì„ ìœ„í•´ datetimeì€ ë‚˜ì¤‘ì— ë¬¸ìì—´ë¡œ ë‹¤ì‹œ ì €ì¥\n",
        "if has_datetime:\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
        "if has_date:\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.date\n",
        "\n",
        "# 3) ERA5-Land ì˜¨ë„ ì»¬ë ‰ì…˜ ì •ì˜\n",
        "ERA = ee.ImageCollection(\"ECMWF/ERA5_LAND/HOURLY\").select('temperature_2m')\n",
        "\n",
        "def parse_time_window(row):\n",
        "    \"\"\"í–‰ì—ì„œ ì‹œê°„ì°½(ee.Date start, end) ê³„ì‚°: datetime ìˆìœ¼ë©´ Â±HOUR_MARGIN, ì—†ìœ¼ë©´ í•˜ë£¨\"\"\"\n",
        "    if has_datetime and pd.notnull(row.get('datetime')):\n",
        "        t0 = pd.to_datetime(row['datetime'])\n",
        "        start = ee.Date(t0.tz_localize('UTC').to_pydatetime()).advance(-HOUR_MARGIN, 'hour')\n",
        "        end   = ee.Date(t0.tz_localize('UTC').to_pydatetime()).advance( HOUR_MARGIN+1, 'hour')\n",
        "    else:\n",
        "        d0 = pd.to_datetime(row['date']).date()\n",
        "        start = ee.Date(dt.datetime(d0.year, d0.month, d0.day))\n",
        "        end   = start.advance(1, 'day')\n",
        "    return start, end\n",
        "\n",
        "def find_hottest_point_latlon(lat, lon, start, end):\n",
        "    \"\"\"\n",
        "    ì…ë ¥/ì™¸ë¶€ ì¸í„°í˜ì´ìŠ¤ëŠ” (lat, lon).\n",
        "    ë‚´ë¶€ GEE í¬ì¸íŠ¸ëŠ” (lon, lat).\n",
        "    ë°˜í™˜: (new_lat, new_lon, tmax_K) ë˜ëŠ” None\n",
        "    \"\"\"\n",
        "    p = ee.Geometry.Point([float(lon), float(lat)])\n",
        "    img_time_max = ERA.filterDate(start, end).max()\n",
        "\n",
        "    for buf_km in BUFFER_KM_STEPS:\n",
        "        region = p.buffer(buf_km * 1000)\n",
        "\n",
        "        # 1) ì§€ì • ë°˜ê²½ ë‚´ ìµœëŒ€ ì˜¨ë„ ê³„ì‚°\n",
        "        max_obj = img_time_max.reduceRegion(\n",
        "            reducer=ee.Reducer.max(),\n",
        "            geometry=region,\n",
        "            scale=SCALE_M,\n",
        "            bestEffort=True\n",
        "        )\n",
        "        max_val = ee.Number(max_obj.get('temperature_2m'))\n",
        "\n",
        "        # ê°’ì´ ì—†ìœ¼ë©´ ë” í° ë°˜ê²½ìœ¼ë¡œ ì¬ì‹œë„\n",
        "        try:\n",
        "            _ = max_val.getInfo()\n",
        "        except Exception:\n",
        "            if VERBOSE: print(f\"  â€¢ no value @ {buf_km}km â†’ retry\")\n",
        "            continue\n",
        "\n",
        "        # 2) ìµœëŒ€ ì˜¨ë„ë¥¼ ê°€ì§€ëŠ” í”½ì…€ì˜ ìœ„ê²½ë„ ì¶”ì¶œ\n",
        "        lonlat = ee.Image.pixelLonLat()\n",
        "        max_mask = img_time_max.eq(max_val)\n",
        "        lonlat_masked = lonlat.updateMask(max_mask)\n",
        "\n",
        "        coord = lonlat_masked.reduceRegion(\n",
        "            reducer=ee.Reducer.firstNonNull(),\n",
        "            geometry=region,\n",
        "            scale=SCALE_M,\n",
        "            bestEffort=True\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            new_lon = ee.Number(coord.get('longitude')).getInfo()\n",
        "            new_lat = ee.Number(coord.get('latitude')).getInfo()\n",
        "            tmax_k  = max_val.getInfo()\n",
        "            return new_lat, new_lon, tmax_k\n",
        "        except Exception:\n",
        "            if VERBOSE: print(f\"  â€¢ no coord @ {buf_km}km â†’ retry\")\n",
        "            continue\n",
        "\n",
        "    return None\n",
        "\n",
        "# 4) ë©”ì¸ ë£¨í”„: ê° í–‰ì— ëŒ€í•´ fmi==-1 ì´ë©´ ì£¼ë³€ ìµœê³  ê¸°ì˜¨ í”½ì…€ë¡œ ì¢Œí‘œ ì´ë™\n",
        "updated_lat, updated_lon, picked_tempK = [], [], []\n",
        "updated_mask, fail_rows = [], []\n",
        "\n",
        "records = df.to_dict('records')\n",
        "for i, row in enumerate(tqdm(records, desc='Processing')):\n",
        "    lat, lon = float(row['lat']), float(row['lon'])\n",
        "    fmi = row['fmi']\n",
        "    start, end = parse_time_window(row)\n",
        "\n",
        "    if fmi == -1:\n",
        "        try:\n",
        "            res = find_hottest_point_latlon(lat, lon, start, end)\n",
        "            if res is None:\n",
        "                # ëŒ€ì²´ ì‹¤íŒ¨: ì›ë˜ ì¢Œí‘œë¥¼ ìœ ì§€\n",
        "                updated_lat.append(lat)\n",
        "                updated_lon.append(lon)\n",
        "                picked_tempK.append(np.nan)\n",
        "                updated_mask.append(False)\n",
        "                fail_rows.append(i)\n",
        "                if VERBOSE:\n",
        "                    print(f\"[{i}] fallback keep original (no hottest pixel found)\")\n",
        "            else:\n",
        "                nl, nlo, tmaxk = res\n",
        "                updated_lat.append(nl)\n",
        "                updated_lon.append(nlo)\n",
        "                picked_tempK.append(tmaxk)\n",
        "                updated_mask.append(True)\n",
        "                if VERBOSE:\n",
        "                    print(f\"[{i}] fmi=-1 â†’ ({nl:.5f}, {nlo:.5f}) Tmax(K)={tmaxk:.2f}\")\n",
        "        except Exception as e:\n",
        "            updated_lat.append(lat)\n",
        "            updated_lon.append(lon)\n",
        "            picked_tempK.append(np.nan)\n",
        "            updated_mask.append(False)\n",
        "            fail_rows.append(i)\n",
        "            if VERBOSE:\n",
        "                print(f\"[{i}] error â†’ keep original: {e}\")\n",
        "    else:\n",
        "        # fmi != -1 ì¸ ì¢Œí‘œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€\n",
        "        updated_lat.append(lat)\n",
        "        updated_lon.append(lon)\n",
        "        picked_tempK.append(np.nan)\n",
        "        updated_mask.append(False)\n",
        "\n",
        "df['new_lat'] = updated_lat\n",
        "df['new_lon'] = updated_lon\n",
        "df['hottest_temp_K'] = picked_tempK\n",
        "df['was_updated'] = updated_mask\n",
        "\n",
        "# ì‹¤ì œ ì¢Œí‘œ êµì²´: fmi==-1 ì´ê³  ëŒ€ì²´ì— ì„±ê³µí•œ í–‰ë§Œ lat, lonì„ new_lat/new_lonìœ¼ë¡œ ë³€ê²½\n",
        "mask_replace = (df['fmi'] == -1) & (df['was_updated'])\n",
        "df.loc[mask_replace, 'lat'] = df.loc[mask_replace, 'new_lat']\n",
        "df.loc[mask_replace, 'lon'] = df.loc[mask_replace, 'new_lon']\n",
        "\n",
        "# 5) ê²°ê³¼ ì €ì¥ ë° ìš”ì•½ íŒŒì¼ ìƒì„±\n",
        "# datetime ì»¬ëŸ¼ì€ Excelì—ì„œ ë³´ê¸° ì¢‹ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
        "if has_datetime:\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "OUT_MAIN = '/content/updated_coords.csv'          # utf-8-sig (BOM í¬í•¨)\n",
        "OUT_FAIL = '/content/update_failures.csv'\n",
        "OUT_SUMM = '/content/update_summary.txt'\n",
        "\n",
        "df.to_csv(OUT_MAIN, index=False, encoding='utf-8-sig', float_format='%.6f')\n",
        "\n",
        "# ëŒ€ì²´ ì‹¤íŒ¨ í–‰ë§Œ ë”°ë¡œ ì €ì¥ (ì°¸ê³ ìš©)\n",
        "if fail_rows:\n",
        "    df.iloc[fail_rows].to_csv(OUT_FAIL, index=False, encoding='utf-8-sig', float_format='%.6f')\n",
        "\n",
        "# ìš”ì•½ ë¦¬í¬íŠ¸ ì‘ì„±: ì „ì²´ í–‰ ìˆ˜, fmi==-1 ê°œìˆ˜, ì‹¤ì œë¡œ ê°±ì‹ ëœ í–‰ ìˆ˜, ì‹¤íŒ¨ í–‰ ìˆ˜\n",
        "total = len(df)\n",
        "n_minus1 = int((df['fmi'] == -1).sum())\n",
        "n_updated = int(mask_replace.sum())\n",
        "n_failed  = len(fail_rows)\n",
        "with open(OUT_SUMM, 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"Total rows           : {total}\\n\")\n",
        "    f.write(f\"fmi == -1 rows       : {n_minus1}\\n\")\n",
        "    f.write(f\"Updated (-1â†’hot)     : {n_updated}\\n\")\n",
        "    f.write(f\"Failed (kept original): {n_failed}\\n\")\n",
        "print(open(OUT_SUMM, encoding='utf-8').read())\n",
        "\n",
        "print(f\"ì €ì¥ ì™„ë£Œ: {OUT_MAIN}\")\n",
        "if fail_rows:\n",
        "    print(f\"ì‹¤íŒ¨ ëª©ë¡: {OUT_FAIL}\")\n",
        "\n",
        "# ìµœì¢… ì¶œë ¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì¢Œí‘œ ê²°ê³¼, ì‹¤íŒ¨ ëª©ë¡, ìš”ì•½ í…ìŠ¤íŠ¸)\n",
        "files.download(OUT_MAIN)\n",
        "if fail_rows:\n",
        "    files.download(OUT_FAIL)\n",
        "files.download(OUT_SUMM)\n",
        "\n",
        "### ì‚°ë¶ˆ ë¹„ë°œìƒ ì§€ì—­ ëœë¤ ìƒ˜í”Œë§\n",
        "\n",
        "# ì–‘ì„± í™”ì (pos.csv)ì„ ê¸°ì¤€ìœ¼ë¡œ, ë™ì¼ ë‚ ì§œ(Â±K_DAYS)Â·ë™ì¼ í´ëŸ¬ìŠ¤í„°(ì§€ì—­+ê³„ì ˆ)ì—ì„œ\n",
        "# í™”ì  ì£¼ë³€ BUFFER_M ë°”ê¹¥ì—ì„œ ìŒì„±(ë¹„ë°œìƒ) ì¢Œí‘œë¥¼ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸\n",
        "# ì…ë ¥:\n",
        "#   - pos.csv (date, lon, lat, region, season, label í¬í•¨, label=1)\n",
        "# ì¶œë ¥:\n",
        "#   - sampling_plan_for_extraction.csv (ì–‘ì„±+ìŒì„± í†µí•© ìƒ˜í”Œë§ ê³„íší‘œ)\n",
        "\n",
        "import os, sys, math, io, datetime as dt\n",
        "import numpy as np, pandas as pd\n",
        "from shapely.geometry import Point\n",
        "from shapely.strtree import STRtree\n",
        "from pyproj import Transformer\n",
        "\n",
        "# 0) íŒŒì¼ ì—…ë¡œë“œ: Colab ë˜ëŠ” ë¡œì»¬ í™˜ê²½ì—ì„œ pos.csv ì„ íƒ\n",
        "def pick_file_interactive():\n",
        "    # Colab í™˜ê²½: files.upload ì‚¬ìš©\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        up = files.upload()\n",
        "        fname = next(iter(up.keys()))\n",
        "        with open(fname, 'wb') as f:\n",
        "            f.write(up[fname])\n",
        "        print(\"[Colab] Uploaded:\", fname)\n",
        "        return fname\n",
        "    except Exception:\n",
        "        pass\n",
        "    # ì¼ë°˜ Jupyter/ë¡œì»¬ í™˜ê²½: Tkinter íŒŒì¼ ì„ íƒì°½ ì‚¬ìš©\n",
        "    try:\n",
        "        import tkinter as tk\n",
        "        from tkinter import filedialog\n",
        "        root = tk.Tk(); root.withdraw()\n",
        "        fname = filedialog.askopenfilename(title=\"Select pos.csv\", filetypes=[(\"CSV\",\"*.csv\"),(\"All\",\"*.*\")])\n",
        "        if not fname: raise RuntimeError(\"No file selected\")\n",
        "        print(\"[Local] Selected:\", fname)\n",
        "        return fname\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"íŒŒì¼ ì„ íƒ ì‹¤íŒ¨. Colabì´ë©´ google.colab.files.uploadë¥¼, ë¡œì»¬ì´ë©´ Tkinter ì‚¬ìš© ê°€ëŠ¥ í™˜ê²½ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.\") from e\n",
        "\n",
        "POS_CSV = pick_file_interactive()\n",
        "\n",
        "# 1) ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°: ìŒì„± ë¹„ìœ¨, í™”ì  ë°°ì œ ë²„í¼, ë‚ ì§œ ë²”ìœ„, ë‚œìˆ˜ ì‹œë“œ\n",
        "RATIO_NEG_PER_POS = 1.5    # ìŒì„±/ì–‘ì„± ëª©í‘œ ë¹„ìœ¨\n",
        "BUFFER_M          = 1500   # í™”ì  ì£¼ë³€ ë°°ì œ ê±°ë¦¬(m)\n",
        "K_DAYS            = 0       # ê°™ì€ ë‚ ì§œ(Â±K_DAYS) ë²”ìœ„\n",
        "SEED              = 42\n",
        "\n",
        "# ì¢Œí‘œê³„ ë³€í™˜ê¸°: ê²½ìœ„ë„(WGS84) â†” íˆ¬ì˜ì¢Œí‘œ(EPSG:5179, í•œêµ­ ì¤‘ë¶€ê¶Œ ì˜ˆì‹œ)\n",
        "to_m  = Transformer.from_crs(\"EPSG:4326\", \"EPSG:5179\", always_xy=True).transform\n",
        "to_deg= Transformer.from_crs(\"EPSG:5179\", \"EPSG:4326\", always_xy=True).transform\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "# 2) ì–‘ì„± ë°ì´í„° ë¡œë“œ ë° ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬\n",
        "pos = pd.read_csv(POS_CSV)\n",
        "need_cols = {'date','lon','lat','region','season','label'}\n",
        "missing = need_cols - set(map(str.lower, pos.columns))\n",
        "\n",
        "# ì›ë³¸ ì»¬ëŸ¼ëª… â†’ ì†Œë¬¸ì í‚¤ë¡œ ë§¤í•‘\n",
        "cols_map = {c.lower(): c for c in pos.columns}\n",
        "def col(name): return cols_map[name]\n",
        "\n",
        "if missing:\n",
        "    raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}. í•„ìš”í•œ ì»¬ëŸ¼: {sorted(list(need_cols))}\")\n",
        "\n",
        "pos = pos.rename(columns={col('date'):'date', col('lon'):'lon', col('lat'):'lat',\n",
        "                          col('region'):'region', col('season'):'season', col('label'):'label'})\n",
        "pos['label'] = 1\n",
        "pos['date']  = pd.to_datetime(pos['date']).dt.date\n",
        "\n",
        "# region+season ì¡°í•©ì„ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„° í‚¤ë¡œ ì‚¬ìš©\n",
        "pos['cluster'] = pos['region'].astype(str) + '|' + pos['season'].astype(str)\n",
        "print(f\"[INFO] Loaded positives: {len(pos)} rows, clusters={pos['cluster'].nunique()}\")\n",
        "\n",
        "# 3) ë‚ ì§œë³„ í™”ì  ë²„í¼(íˆ¬ì˜ì¢Œí‘œê³„ì—ì„œ BUFFER_M ë§Œí¼) ìƒì„±\n",
        "def to_point_m(lon, lat):\n",
        "    x, y = to_m(lon, lat)\n",
        "    return Point(x, y)\n",
        "\n",
        "pos['date_dt'] = pd.to_datetime(pos['date'])\n",
        "buf_rows = []\n",
        "for d, df_d in pos.groupby('date_dt'):\n",
        "    # í•´ë‹¹ ë‚ ì§œì˜ ëª¨ë“  í™”ì ì„ BUFFER_M ë§Œí¼ ë²„í¼ë§\n",
        "    geoms_m = [to_point_m(lon, lat).buffer(BUFFER_M) for lon,lat in zip(df_d['lon'], df_d['lat'])]\n",
        "    buf_rows.append((d, geoms_m, STRtree(geoms_m)))\n",
        "\n",
        "def get_buffer_union_for_date(d):\n",
        "    # d ê¸°ì¤€ Â±K_DAYS ë‚ ì§œì˜ ëª¨ë“  í™”ì  ë²„í¼ë¥¼ í•©ì§‘í•©ìœ¼ë¡œ ê³„ì‚°\n",
        "    dates = [d + dt.timedelta(days=k) for k in range(-K_DAYS, K_DAYS+1)]\n",
        "    geoms = []\n",
        "    for dd, geoms_m, _ in buf_rows:\n",
        "        if dd.date() in [x.date() for x in dates]:\n",
        "            geoms.extend(geoms_m)\n",
        "    if not geoms:\n",
        "        return None\n",
        "    u = geoms[0]\n",
        "    for g in geoms[1:]:\n",
        "        u = u.union(g)\n",
        "    return u\n",
        "\n",
        "# 4) í´ëŸ¬ìŠ¤í„°Ã—ë‚ ì§œë³„ ëª©í‘œ ìŒì„± ê°œìˆ˜ ê³„ì‚°\n",
        "targets = (pos.groupby(['cluster','date'])\n",
        "             .size().rename('n_pos').reset_index())\n",
        "targets['n_neg_target'] = np.ceil(RATIO_NEG_PER_POS * targets['n_pos']).astype(int)\n",
        "\n",
        "# 5) ëœë¤ ìƒ˜í”Œë§ ë°•ìŠ¤ ì„¤ì •: ìµœì†Œ/ìµœëŒ€ ê²½ìœ„ë„ ë°•ìŠ¤ ì•ˆì—ì„œ ìŒì„± ìœ„ì¹˜ ìƒì„±\n",
        "#   (í•„ìš”í•˜ë©´ ì´ ë°•ìŠ¤ë¥¼ í–‰ì •ê²½ê³„(GeoJSON ë“±)ë¡œ êµì²´ ê°€ëŠ¥)\n",
        "min_lon, max_lon = pos['lon'].min()-0.5, pos['lon'].max()+0.5\n",
        "min_lat, max_lat = pos['lat'].min()-0.5, pos['lat'].max()+0.5\n",
        "\n",
        "def random_points_in_box(n):\n",
        "    lons = rng.uniform(min_lon, max_lon, n)\n",
        "    lats = rng.uniform(min_lat, max_lat, n)\n",
        "    return list(zip(lons, lats))\n",
        "\n",
        "# 6) ìŒì„± ìƒ˜í”Œë§: í™”ì  ë²„í¼ ë°–ì—ì„œ ëª©í‘œ ê°œìˆ˜ë§Œí¼ ìŒì„± ì¢Œí‘œ ìƒì„±\n",
        "neg_rows = []\n",
        "for (cluster, date), row in targets.set_index(['cluster','date']).iterrows():\n",
        "    need = int(row['n_neg_target'])\n",
        "    bu = get_buffer_union_for_date(pd.to_datetime(date))\n",
        "    got = 0\n",
        "    trials = 0\n",
        "    cap = max(need * 300, 5000)  # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ìƒí•œ\n",
        "\n",
        "    while got < need and trials < cap:\n",
        "        batch_n = min(need - got, 500)\n",
        "        for lon, lat in random_points_in_box(batch_n):\n",
        "            trials += 1\n",
        "            pt_m = to_point_m(lon, lat)\n",
        "            # í™”ì  ë²„í¼ ì•ˆì´ë©´ ìŒì„± í›„ë³´ì—ì„œ ì œì™¸\n",
        "            if bu is not None and bu.contains(pt_m):\n",
        "                continue\n",
        "            neg_rows.append(dict(\n",
        "                date=date, lon=lon, lat=lat,\n",
        "                region=cluster.split('|')[0],\n",
        "                season=cluster.split('|')[1],\n",
        "                label=0\n",
        "            ))\n",
        "            got += 1\n",
        "            if got >= need: break\n",
        "\n",
        "    if got < need:\n",
        "        # ì§€ì •í•œ ë²„í¼/ë‚ ì§œ ë²”ìœ„ì—ì„œ ìŒì„±ì´ ë¶€ì¡±í•  ë•Œ ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥\n",
        "        print(f\"[WARN] ë¶€ì¡±: cluster={cluster}, date={date}, target_neg={need}, got={got}. \"\n",
        "              f\"K_DAYS í™•ëŒ€ ë˜ëŠ” BUFFER_M ì™„í™” ê³ ë ¤.\")\n",
        "\n",
        "neg = pd.DataFrame(neg_rows)\n",
        "\n",
        "# 7) ìµœì¢… ìƒ˜í”Œë§ ê³„íš ë³‘í•© ë° ì €ì¥\n",
        "pos_out = pos[['date','lon','lat','region','season','label']].copy()\n",
        "plan = pd.concat([pos_out, neg], ignore_index=True)\n",
        "\n",
        "out_csv = \"sampling_plan_for_extraction.csv\"\n",
        "plan.to_csv(out_csv, index=False)\n",
        "print(f\"[DONE] saved â†’ {out_csv} (rows={len(plan)})\")\n",
        "\n",
        "# ìš”ì•½ ë¦¬í¬íŠ¸: ì „ì²´ label ë¹„ìœ¨ ë° regionÂ·seasonÂ·labelë³„ ê°œìˆ˜ í…Œì´ë¸”\n",
        "print(\"\\n[Summary]\")\n",
        "print(plan['label'].value_counts())\n",
        "print(plan.groupby(['region','season','label']).size().unstack(fill_value=0))\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"sampling_plan_for_extraction.csv\")\n",
        "\n",
        "\n",
        "### ë¶„ë¥˜\n",
        "\n",
        "# ì‚°ë¶ˆ ë°œìƒ ì—¬ë¶€(label)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ìŠ¤í¬ë¦½íŠ¸\n",
        "# ì…ë ¥:\n",
        "#   - CSV 1ê°œ (í•„ìˆ˜ ì»¬ëŸ¼: label, date, lon, lat, TP_mm, Tmax, Tmin, RH, WSPD, FMI, DEM, Slope)\n",
        "#   - ì„ íƒ ì»¬ëŸ¼: Tmean, TMI, FFDRI, FRI_norm ë“±ì€ ìˆìœ¼ë©´ ì‚¬ìš©\n",
        "# ì²˜ë¦¬:\n",
        "#   - ì¢Œí‘œë³„ë¡œ ì‹œê³„ì—´ ì •ë ¬ í›„ ì´ë™í‰ê· Â·ëˆ„ì ê°•ìˆ˜Â·ê±´ì¡°ì¼ìˆ˜(DrySpell) íŒŒìƒ\n",
        "#   - 2015â€“2022ë…„ì„ í•™ìŠµ, 2023/2024ë…„ì„ í™€ë“œì•„ì›ƒ í‰ê°€\n",
        "#   - RandomForest/ExtraTrees í•™ìŠµ, RF ê¸°ì¤€ OOFë¡œ ì„ê³„ê°’ ìµœì í™”(F2 ìµœëŒ€, P í•˜í•œ 0.35)\n",
        "#   - season_clusterë³„ë¡œ ë‹¤ë¥¸ ì„ê³„ê°’ ì ìš©\n",
        "# ì¶œë ¥:\n",
        "#   - /content/artifacts/thresholds.json      : ê³„ì ˆë³„ ì„ê³„ê°’\n",
        "#   - /content/artifacts/feature_list.json    : ì‚¬ìš© í”¼ì²˜ ë¦¬ìŠ¤íŠ¸\n",
        "#   - /content/artifacts/clf_rf.joblib        : RF ëª¨ë¸ ë¦¬ìŠ¤íŠ¸\n",
        "#   - /content/pred_cls_2023.csv, pred_cls_2024.csv : ì—°ë„ë³„ ì˜ˆì¸¡/ê²½ë³´ ê²°ê³¼\n",
        "\n",
        "# íŒ¨í‚¤ì§€ ì„¤ì¹˜ (Colab ê¸°ì¤€)\n",
        "!pip -q install scikit-learn imbalanced-learn lightgbm joblib pandas\n",
        "\n",
        "# CSV ì—…ë¡œë“œ (Colab ìœ„ì ¯ìœ¼ë¡œ ì§ì ‘ ì„ íƒ)\n",
        "from google.colab import files\n",
        "up = files.upload()  # CSV ì§ì ‘ ì„ íƒ\n",
        "\n",
        "import io, pandas as pd, numpy as np, json, joblib, os, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "CSV_PATH = next(iter(up))  # ì—…ë¡œë“œí•œ ì²« ë²ˆì§¸ íŒŒì¼ëª…\n",
        "print(\"Loaded:\", CSV_PATH)\n",
        "\n",
        "# CSV ë¡œë“œ (ì—¬ëŸ¬ ì¸ì½”ë”©ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„)\n",
        "enc_trials = [\"utf-8\", \"cp949\", \"euc-kr\"]\n",
        "for enc in enc_trials:\n",
        "    try:\n",
        "        df = pd.read_csv(io.BytesIO(up[CSV_PATH]), encoding=enc, parse_dates=[\"date\"])\n",
        "        print(f\"[INFO] Read CSV with encoding={enc}, rows={len(df)}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "else:\n",
        "    raise last_err\n",
        "\n",
        "# í•„ìˆ˜/ì„ íƒ ì»¬ëŸ¼ ì ê²€ ë° Tmin ë³´ì •\n",
        "must_cols = [\"label\",\"date\",\"lon\",\"lat\",\"TP_mm\",\"Tmax\",\"Tmin\",\"RH\",\"WSPD\",\"FMI\",\"DEM\",\"Slope\"]\n",
        "miss = [c for c in must_cols if c not in df.columns]\n",
        "if miss:\n",
        "    # Tminë§Œ ì—†ëŠ” ê²½ìš°: Tmeanì„ ì´ìš©í•´ ê·¼ì‚¬ ìƒì„±\n",
        "    if miss == [\"Tmin\"]:\n",
        "        if \"Tmean\" in df.columns:\n",
        "            df[\"Tmin\"] = 2*df[\"Tmean\"] - df[\"Tmax\"]\n",
        "            print(\"[INFO] 'Tmin' ë¯¸ì¡´ì¬ â†’ Tmean,Tmaxë¡œ ê·¼ì‚¬ ìƒì„±.\")\n",
        "        else:\n",
        "            raise ValueError(f\"í•„ìˆ˜ ì¹¼ëŸ¼ ëˆ„ë½: {miss} (ë˜ëŠ” Tmean ì œê³µ)\")\n",
        "    else:\n",
        "        raise ValueError(f\"í•„ìˆ˜ ì¹¼ëŸ¼ ëˆ„ë½: {miss}\")\n",
        "\n",
        "opt_cols = [\"Tmean\",\"TMI\",\"FFDRI\",\"FRI_norm\",\"FRI_grade\",\"range\",\"RNE\",\"EH\",\"pDWI\",\"DWI_by_month\",\"DWI\",\"season_cluster\"]\n",
        "print(\"[INFO] Optional present:\", [c for c in opt_cols if c in df.columns])\n",
        "\n",
        "# ì—°ë„ íŒŒìƒ ë³€ìˆ˜ ì¶”ê°€\n",
        "df[\"year\"] = df[\"date\"].dt.year\n",
        "\n",
        "# ì¼êµì°¨(DTR) íŒŒìƒ\n",
        "df[\"DTR\"] = df[\"Tmax\"] - df[\"Tmin\"]\n",
        "\n",
        "# season_clusterê°€ ì—†ìœ¼ë©´ ë‹¨ìˆœ ê·œì¹™ìœ¼ë¡œ ë´„/ì—¬ë¦„/ê°€ì„ê²¨ìš¸ ë¶„ë¦¬\n",
        "if \"season_cluster\" not in df.columns:\n",
        "    m = df[\"date\"].dt.month\n",
        "    season = np.where(m.isin([3,4,5]), \"spring\",\n",
        "              np.where(m.isin([6,7,8]), \"summer\", \"fall_winter\"))\n",
        "    df[\"season_cluster\"] = season\n",
        "    print(\"[INFO] season_cluster ìƒì„± ì™„ë£Œ (rule-based: spring/summer/fall_winter)\")\n",
        "\n",
        "# ì¢Œí‘œ ë‹¨ìœ„ ì‹œê³„ì—´ ê·¸ë£¹ í‚¤ (í•„ìš”í•˜ë©´ season_clusterë¥¼ í¬í•¨í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥)\n",
        "group_keys = [\"lat\",\"lon\"]\n",
        "\n",
        "def add_memory_feats(g):\n",
        "    # ê° ì¢Œí‘œë³„ë¡œ ì‹œê°„ìˆœ ì •ë ¬ ì´í›„ ì´ë™íŠ¹ì§•ì„ ê³„ì‚°í•œë‹¤ê³  ê°€ì •\n",
        "    # 3ì¼, 7ì¼ ì´ë™í‰ê·  (ê¸°ì˜¨, ìŠµë„, í’ì†, ê°•ìˆ˜, ì¼êµì°¨)\n",
        "    for c in [\"Tmax\",\"RH\",\"WSPD\",\"TP_mm\",\"DTR\"]:\n",
        "        g[f\"{c}_ma3\"] = g[c].rolling(3, min_periods=1).mean()\n",
        "        g[f\"{c}_ma7\"] = g[c].rolling(7, min_periods=1).mean()\n",
        "    # ìµœê·¼ 3/7ì¼ ëˆ„ì ê°•ìˆ˜ (ê°•ìˆ˜ëŸ‰ ëˆ„ì )\n",
        "    g[\"TP_3obs_sum\"] = g[\"TP_mm\"].rolling(3, min_periods=1).sum()\n",
        "    g[\"TP_7obs_sum\"] = g[\"TP_mm\"].rolling(7, min_periods=1).sum()\n",
        "    # DrySpell: 0.1mm ì´í•˜ ë¬´ê°•ìˆ˜ì¼ ì—°ì† ê¸¸ì´\n",
        "    dry = (g[\"TP_mm\"].fillna(0) <= 0.1).astype(int)\n",
        "    g[\"DrySpell\"] = dry.groupby((dry==0).cumsum()).cumcount() * dry\n",
        "    return g\n",
        "\n",
        "# ì¢Œí‘œë³„Â·ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ í›„ ë©”ëª¨ë¦¬ í”¼ì²˜ ìƒì„±\n",
        "df = df.sort_values([\"lat\",\"lon\",\"date\"]).groupby(group_keys, group_keys=False).apply(add_memory_feats).reset_index(drop=True)\n",
        "\n",
        "# ë¶„ë¥˜ ëª¨ë¸ ì…ë ¥ í”¼ì²˜ êµ¬ì„±\n",
        "FEATURES = [\n",
        "    \"Tmax\",\"RH\",\"WSPD\",\"TP_mm\",\"DTR\",\n",
        "    \"Tmax_ma3\",\"Tmax_ma7\",\"RH_ma3\",\"RH_ma7\",\"WSPD_ma3\",\"WSPD_ma7\",\"DTR_ma3\",\"DTR_ma7\",\n",
        "    \"TP_3obs_sum\",\"TP_7obs_sum\",\"DrySpell\",\n",
        "    \"Slope\",\"FMI\",\"DEM\"\n",
        "]\n",
        "TARGET = \"label\"\n",
        "CLUSTER = \"season_cluster\"\n",
        "\n",
        "# ë¬´í•œëŒ€ ê°’ ì œê±° ë° ìˆ˜ì¹˜ ê²°ì¸¡ì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
        "df[FEATURES] = df[FEATURES].replace([np.inf,-np.inf], np.nan)\n",
        "df[FEATURES] = df[FEATURES].fillna(df[FEATURES].median())\n",
        "\n",
        "# í•™ìŠµ/í‰ê°€ ê¸°ê°„ ë¶„í• \n",
        "train = df[(df[\"year\"]>=2015) & (df[\"year\"]<=2022)].copy()\n",
        "test23 = df[df[\"year\"]==2023].copy()\n",
        "ext24  = df[df[\"year\"]==2024].copy()\n",
        "\n",
        "print(\"Shapes â†’\", \"train:\", train.shape, \"test23:\", test23.shape, \"ext24:\", ext24.shape)\n",
        "\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_fscore_support\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from scipy.optimize import minimize_scalar\n",
        "import numpy as np, os, json, joblib\n",
        "\n",
        "SEED = 123\n",
        "N_SPLITS = 5\n",
        "\n",
        "def get_model(name=\"RF\"):\n",
        "    # RandomForestì™€ ExtraTrees ë‘ ê°€ì§€ íŠ¸ë¦¬ ê¸°ë°˜ ë¶„ë¥˜ê¸° ì •ì˜\n",
        "    if name==\"RF\":\n",
        "        return RandomForestClassifier(\n",
        "            n_estimators=300, max_depth=None, min_samples_leaf=2,\n",
        "            n_jobs=-1, random_state=SEED, class_weight=\"balanced\"\n",
        "        )\n",
        "    if name==\"ET\":\n",
        "        return ExtraTreesClassifier(\n",
        "            n_estimators=500, max_depth=None, min_samples_leaf=2,\n",
        "            n_jobs=-1, random_state=SEED, class_weight=\"balanced\"\n",
        "        )\n",
        "    raise ValueError\n",
        "\n",
        "def cv_fit_predict(train_df, feats, target, groups, model_name=\"RF\"):\n",
        "    # ì—°ë„(year)ë¥¼ ê·¸ë£¹ìœ¼ë¡œ í•˜ëŠ” StratifiedGroupKFold êµì°¨ê²€ì¦ ìˆ˜í–‰\n",
        "    # ë°˜í™˜:\n",
        "    #   - foldë³„ í•™ìŠµëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸\n",
        "    #   - ì „ì²´ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ OOF ì˜ˆì¸¡ í™•ë¥ \n",
        "    #   - PR-AUC, ROC-AUC ì„±ëŠ¥ ì§€í‘œ\n",
        "    X = train_df[feats].values\n",
        "    y = train_df[target].values\n",
        "    g = train_df[groups].values\n",
        "    cv = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
        "    oof = np.zeros(len(train_df))\n",
        "    models = []\n",
        "    for tr, va in cv.split(X, y, groups=g):\n",
        "        clf = get_model(model_name)\n",
        "        clf.fit(X[tr], y[tr])\n",
        "        p  = clf.predict_proba(X[va])[:,1]\n",
        "        oof[va] = p\n",
        "        models.append(clf)\n",
        "    pr, roc = average_precision_score(y, oof), roc_auc_score(y, oof)\n",
        "    return models, oof, {\"PR-AUC\":pr, \"ROC-AUC\":roc}\n",
        "\n",
        "# RF, ET ë‘ ëª¨ë¸ì— ëŒ€í•´ êµì°¨ê²€ì¦ ê¸°ë°˜ ì„±ëŠ¥ ë¹„êµ\n",
        "models_rf, oof_rf, metr_rf = cv_fit_predict(train, FEATURES, TARGET, \"year\", \"RF\")\n",
        "models_et, oof_et, metr_et = cv_fit_predict(train, FEATURES, TARGET, \"year\", \"ET\")\n",
        "print(\"[RF] \", metr_rf)\n",
        "print(\"[ET] \", metr_et)\n",
        "\n",
        "# ì„ê³„ê°’ íƒìƒ‰: F2-score ìµœëŒ€ + Precision í•˜í•œ(â‰¥0.35) ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” threshold ì„ íƒ\n",
        "def find_threshold_f2_with_precision(y_true, y_prob, min_prec=0.35):\n",
        "    # 0~1 êµ¬ê°„ì—ì„œ ì´˜ì´˜í•œ ê²©ì(threshold)ë¥¼ ìŠ¤ìº”í•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ íƒìƒ‰\n",
        "    best_t, best = 0.0, (-1,-1,-1)  # (P,R,F2)\n",
        "    for thr in np.linspace(0,1,2001):\n",
        "        yp = (y_prob>=thr).astype(int)\n",
        "        p,r,f2,_ = precision_recall_fscore_support(y_true, yp, beta=2.0,\n",
        "                                                   average=\"binary\", zero_division=0)\n",
        "        if p>=min_prec and f2>best[2]:\n",
        "            best_t, best = float(thr), (p,r,f2)\n",
        "    if best[2] >= 0:\n",
        "        return best_t, {\"precision\":best[0], \"recall\":best[1], \"f2\":best[2]}\n",
        "    # Precision í•˜í•œì„ ë§Œì¡±í•˜ëŠ” ê°’ì´ ì—†ìœ¼ë©´ F2ë§Œ ìµœëŒ€ê°€ ë˜ëŠ” ì„ê³„ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
        "    best_t2, best2 = 0.0, (-1,-1,-1)\n",
        "    for thr in np.linspace(0,1,2001):\n",
        "        yp = (y_prob>=thr).astype(int)\n",
        "        p,r,f2,_ = precision_recall_fscore_support(y_true, yp, beta=2.0,\n",
        "                                                   average=\"binary\", zero_division=0)\n",
        "        if f2>best2[2]:\n",
        "            best_t2, best2 = float(thr), (p,r,f2)\n",
        "    return best_t2, {\"precision\":best2[0], \"recall\":best2[1], \"f2\":best2[2]}\n",
        "\n",
        "# ì „ì²´ í•™ìŠµê¸°ê°„ OOF í™•ë¥ ì„ ì´ìš©í•´ ì „ì—­ ì„ê³„ê°’ ê³„ì‚°\n",
        "y_true_tr = train[TARGET].values\n",
        "t_star, stats = find_threshold_f2_with_precision(y_true_tr, oof_rf, min_prec=0.35)\n",
        "print(f\"[Global t*] {t_star:.4f}  stats={stats}\")\n",
        "\n",
        "# OOF ê²°ê³¼ë¥¼ Seriesë¡œ ë§Œë“¤ì–´ index ê¸°ë°˜ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì„œë¸Œì…‹ ì„ íƒ\n",
        "oof_series = pd.Series(oof_rf, index=train.index)\n",
        "\n",
        "# season_clusterë³„ë¡œ ê°œë³„ ì„ê³„ê°’ ê³„ì‚° (ë°ì´í„°ê°€ ì „ë¶€ 0 ë˜ëŠ” 1ì´ë©´ ì „ì—­ ì„ê³„ê°’ ì‚¬ìš©)\n",
        "thresholds_by_cluster = {}\n",
        "for cid, gdf in train.groupby(\"season_cluster\"):\n",
        "    idx = gdf.index\n",
        "    y_c = gdf[TARGET].values\n",
        "    p_c = oof_series.loc[idx].values\n",
        "    if (y_c.sum()==0) or (y_c.sum()==len(y_c)):\n",
        "        t, s = t_star, {\"precision\":np.nan,\"recall\":np.nan,\"f2\":np.nan}\n",
        "    else:\n",
        "        t, s = find_threshold_f2_with_precision(y_c, p_c, 0.35)\n",
        "    thresholds_by_cluster[str(cid)] = float(t)\n",
        "\n",
        "thresholds_by_cluster[\"__global__\"] = float(t_star)\n",
        "print(\"thresholds_by_cluster =\", thresholds_by_cluster)\n",
        "\n",
        "# RF ëª¨ë¸ ë° ì„ê³„ê°’, í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì•„í‹°íŒ©íŠ¸ë¡œ ì €ì¥\n",
        "os.makedirs(\"/content/artifacts\", exist_ok=True)\n",
        "with open(\"/content/artifacts/thresholds.json\",\"w\") as f: json.dump(thresholds_by_cluster, f, indent=2)\n",
        "with open(\"/content/artifacts/feature_list.json\",\"w\") as f: json.dump(FEATURES, f, indent=2)\n",
        "joblib.dump(models_rf, \"/content/artifacts/clf_rf.joblib\")\n",
        "print(\"Saved artifacts â†’ /content/artifacts\")\n",
        "\n",
        "def predict_mean_proba(models, X):\n",
        "    # foldë³„ RF í™•ë¥  ì˜ˆì¸¡ì„ í‰ê· ë‚´ì–´ ìµœì¢… í™•ë¥ ë¡œ ì‚¬ìš©\n",
        "    return np.mean([m.predict_proba(X)[:,1] for m in models], axis=0)\n",
        "\n",
        "def evaluate_binary(y, p, thr):\n",
        "    # ì£¼ì–´ì§„ ì„ê³„ê°’(thr)ì— ëŒ€í•´ PR-AUC, ROC-AUC, Precision/Recall/F2 ë“± ì‚°ì¶œ\n",
        "    from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_fscore_support\n",
        "    yhat = (p>=thr).astype(int)\n",
        "    pr = average_precision_score(y, p)\n",
        "    roc = roc_auc_score(y, p)\n",
        "    P,R,F2,_ = precision_recall_fscore_support(y, yhat, beta=2.0, average=\"binary\", zero_division=0)\n",
        "    return {\"PR-AUC\":pr, \"ROC-AUC\":roc, \"P\":P, \"R\":R, \"F2\":F2, \"N_pred_pos\":int(yhat.sum()), \"N\":int(len(yhat))}\n",
        "\n",
        "# ì €ì¥ëœ ì„ê³„ê°’ ë¡œë“œ\n",
        "with open(\"/content/artifacts/thresholds.json\") as f:\n",
        "    THR = json.load(f)\n",
        "\n",
        "# 2023, 2024 ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ ë° ê²½ë³´ CSV ìƒì„±\n",
        "for name, hold in {\"2023\":test23, \"2024\":ext24}.items():\n",
        "    if len(hold)==0:\n",
        "        print(f\"[{name}] no rows.\"); continue\n",
        "    Xh = hold[FEATURES].values\n",
        "    ph = predict_mean_proba(models_rf, Xh)\n",
        "\n",
        "    # ì°¸ê³ ìš©: ê³ ì • ì„ê³„ê°’ 0.5 ê¸°ì¤€ ì„±ëŠ¥ ì¶œë ¥\n",
        "    rep05 = evaluate_binary(hold[TARGET].values, ph, 0.5)\n",
        "    print(f\"[{name}] metrics@0.5 â†’\", rep05)\n",
        "\n",
        "    # ì‹¤ì œ ê²½ë³´ëŠ” season_clusterë³„ ì„ê³„ê°’ ì ìš©\n",
        "    thr_vec = np.array([ THR.get(str(c), THR[\"__global__\"]) for c in hold[\"season_cluster\"].astype(str) ])\n",
        "    yhat = (ph >= thr_vec).astype(int)\n",
        "\n",
        "    out = hold[[\"date\",\"lon\",\"lat\",\"season_cluster\",TARGET]].copy()\n",
        "    out[\"y_prob\"] = ph\n",
        "    out[\"y_pred\"] = yhat\n",
        "    out_path = f\"/content/pred_cls_{name}.csv\"\n",
        "    out.to_csv(out_path, index=False)\n",
        "    print(\"Saved:\", out_path, \"| ê²½ë³´ëŸ‰:\", yhat.sum(), \"/\", len(yhat))\n",
        "\n",
        "    from google.colab import files\n",
        "\n",
        "# ìƒì„±ëœ ì˜ˆì¸¡ ê²°ê³¼ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ (2023, 2024)\n",
        "files.download(\"/content/pred_cls_2023.csv\")\n",
        "files.download(\"/content/pred_cls_2024.csv\")\n",
        "\n",
        "\n",
        "\n",
        "### íšŒê·€\n",
        "\n",
        "# FFDRI ë˜ëŠ” FRI_normê³¼ ê°™ì€ ì—°ì†í˜• ìœ„í—˜ì§€ìˆ˜ë¥¼ íšŒê·€ë¡œ ì˜ˆì¸¡í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸\n",
        "# íŠ¹ì§•:\n",
        "#   - ì—…ë¡œë“œëœ ì›ë³¸ CSVì—ì„œ íƒ€ê¹ƒ(FFDRI/FRI_norm)ì„ ìë™ íƒì§€\n",
        "#   - ì¢Œí‘œÂ·ê³„ì ˆë³„ ì¼ ë‹¨ìœ„ë¡œ ì¡°ë°€í™”(asfreq('D')) í›„ ì§§ì€ ê²°ì¸¡ë§Œ ë³´ê°„\n",
        "#   - ê° ì‹œì ì—ì„œ í–¥í›„ Lì¼(0~7ì¼) ë¦¬ë“œ íƒ€ê¹ƒì„ ìƒì„±(row-lead êµ¬ì¡°)\n",
        "#   - LightGBM ìš°ì„ , ë°ì´í„°ê°€ ì‘ê±°ë‚˜ ê¹Šì€ ë¦¬ë“œì—ì„œëŠ” ElasticNetìœ¼ë¡œ í´ë°±\n",
        "#   - 2015â€“2022 í•™ìŠµ, 2023/2024ì— ëŒ€í•œ ë¦¬ë“œë³„ ì„±ëŠ¥í‘œ ë° ì˜ˆì¸¡ CSV, ë“±ê¸‰ì»· ì €ì¥\n",
        "# ì¶œë ¥:\n",
        "#   - /content/artifacts_reg/reg_L0_7_rowlead.joblib     : ë¦¬ë“œë³„ íšŒê·€ ëª¨ë¸ dict\n",
        "#   - /content/artifacts_reg/reg_features_used.json      : ë¦¬ë“œë³„ ì‹¤ì œ ì‚¬ìš© í”¼ì²˜ ëª©ë¡\n",
        "#   - /content/artifacts_reg/grades_reg.json             : ìœ„í—˜ë„ ë“±ê¸‰ êµ¬ê°„(Low/Moderate/High/VeryHigh)\n",
        "#   - /content/pred_reg_2023_rowlead.csv, _2024_rowlead.csv : ë¦¬ë“œë³„ ì˜ˆì¸¡ê°’/ë“±ê¸‰\n",
        "#   - /content/artifacts_reg_bundle_rowlead.zip          : íšŒê·€ ê´€ë ¨ ì•„í‹°íŒ©íŠ¸ ì••ì¶•ë³¸\n",
        "\n",
        "# íšŒê·€ì— í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸\n",
        "import os, json, shutil, joblib, numpy as np, pandas as pd, re\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Colab/ë¡œì»¬ ê³µìš© ë‹¤ìš´ë¡œë“œ í—¬í¼\n",
        "try:\n",
        "    from google.colab import files\n",
        "except Exception:\n",
        "    class _Files:\n",
        "        def upload(self): raise RuntimeError(\"ë¡œì»¬ í™˜ê²½: files.upload() ë¯¸ì§€ì›. ëŒ€ì‹  ê²½ë¡œ ì§€ì • ë¡œë“œ í•„ìš”.\")\n",
        "        def download(self, path): print(f\"[local] íŒŒì¼ ì €ì¥ë¨: {path}\")\n",
        "    files = _Files()\n",
        "\n",
        "# ì›ë³¸ ë°ì´í„° CSV ì—…ë¡œë“œ (ì˜ˆ: wildfire_dataset.csv)\n",
        "print(\"ì›ë³¸ ë°ì´í„° CSV ì—…ë¡œë“œ (ì˜ˆ: wildfire_dataset.csv)\")\n",
        "up = files.upload()\n",
        "PATH = list(up.keys())[0]\n",
        "\n",
        "# ì¸ì½”ë”© ìë™ ì‹œë„ (utf-8-sig â†’ utf-8 â†’ cp949 â†’ euc-kr ìˆœì„œ)\n",
        "_encs = [\"utf-8-sig\",\"utf-8\",\"cp949\",\"euc-kr\"]\n",
        "for enc in _encs:\n",
        "    try:\n",
        "        df = pd.read_csv(PATH, encoding=enc)\n",
        "        print(f\"ë¡œë“œ ì„±ê³µ: {PATH} (encoding={enc}) shape={df.shape}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "else:\n",
        "    raise RuntimeError(f\"CSV ë¡œë“œ ì‹¤íŒ¨: {last_err}\")\n",
        "\n",
        "# ë‚ ì§œ ì»¬ëŸ¼ í‘œì¤€í™” (dateê°€ ì—†ìœ¼ë©´ dt/ë‚ ì§œ/ymd í›„ë³´ë¥¼ dateë¡œ ë¦¬ë„¤ì„)\n",
        "if \"date\" not in df.columns:\n",
        "    cand = [c for c in df.columns if c.lower() in (\"dt\",\"ë‚ ì§œ\",\"ymd\")]\n",
        "    if cand:\n",
        "        df = df.rename(columns={cand[0]: \"date\"})\n",
        "    else:\n",
        "        raise ValueError(\"date ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤ (YYYY-MM-DD).\")\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "\n",
        "# ì—°ë„ íŒŒìƒ ë³€ìˆ˜ (ì—†ìœ¼ë©´ ìƒì„±)\n",
        "if \"year\" not in df.columns:\n",
        "    df[\"year\"] = df[\"date\"].dt.year\n",
        "\n",
        "# season_clusterê°€ ì—†ìœ¼ë©´ ê°„ë‹¨ ê·œì¹™ìœ¼ë¡œ ìƒì„± (ë´„: 2~4, ê°€ì„Â·ê²¨ìš¸: 10~12, ë‚˜ë¨¸ì§€ëŠ” summer)\n",
        "if \"season_cluster\" not in df.columns:\n",
        "    m = df[\"date\"].dt.month\n",
        "    season = np.select([m.isin([2,3,4]), m.isin([10,11,12])],\n",
        "                       [\"spring\",\"fall_winter\"], default=\"summer\")\n",
        "    df[\"season_cluster\"] = season\n",
        "\n",
        "# ìœ„ê²½ë„ ì´ë¦„ ì •ë¦¬ (ì†Œë¬¸ìí™” + longitude/latitude â†’ lon/lat ì¹˜í™˜)\n",
        "def _scrub_cols(cols):\n",
        "    fixed = {}\n",
        "    for c in cols:\n",
        "        cl = c.strip().lower()\n",
        "        cl = cl.replace(\"longitude\",\"lon\").replace(\"latitude\",\"lat\")\n",
        "        fixed[c] = cl\n",
        "    return fixed\n",
        "df = df.rename(columns=_scrub_cols(df.columns))\n",
        "\n",
        "# lon/lat í•„ìˆ˜: ì—†ìœ¼ë©´ ìœ ì‚¬ ì´ë¦„ì„ ì°¾ì•„ì„œ ë¦¬ë„¤ì„\n",
        "for need in [\"lon\",\"lat\"]:\n",
        "    if need not in df.columns:\n",
        "        cand = [c for c in df.columns if re.fullmatch(rf\".*\\b{need}\\b.*\", c.lower())]\n",
        "        if cand:\n",
        "            df = df.rename(columns={cand[0]: need})\n",
        "        else:\n",
        "            raise ValueError(f\"{need} ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# íƒ€ê¹ƒ ì»¬ëŸ¼ ìë™ íƒì§€ (FFDRI â†’ FRI_norm ìˆœìœ¼ë¡œ ìš°ì„ , ì—†ìœ¼ë©´ íŒ¨í„´/ìˆ«ìí˜• í›„ë³´ ì•ˆë‚´)\n",
        "def pick_target_column(_df):\n",
        "    cols = list(_df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    for key in [\"ffdri\",\"fri_norm\"]:\n",
        "        if key in lowmap: return lowmap[key]\n",
        "    for pat in [r\"ffdr[iy]?\", r\"fri[_\\s-]*norm\", r\"^fri$\"]:\n",
        "        for c in cols:\n",
        "            if re.search(pat, c.lower()):\n",
        "                return c\n",
        "    num_candidates = [c for c in cols if np.issubdtype(_df[c].dropna().dtype, np.number)]\n",
        "    print(\"íƒ€ê¹ƒ(FFDRI/FRI_norm) ìë™íƒì§€ ì‹¤íŒ¨. ìˆ«ìí˜• í›„ë³´:\", num_candidates)\n",
        "    user = input(\"íƒ€ê¹ƒ ì¹¼ëŸ¼ëª… ì…ë ¥: \").strip()\n",
        "    if user in cols: return user\n",
        "    raise ValueError(\"íšŒê·€ íƒ€ê¹ƒ ì¹¼ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "TARGET_CONT = pick_target_column(df)\n",
        "TARGET_CONT = TARGET_CONT.strip()\n",
        "\n",
        "# ê¸°ë³¸ ê¸°ìƒÂ·ì§€í˜• í”¼ì²˜ ìŠ¤í‚¤ë§ˆ (í‘œì¤€ ì´ë¦„)\n",
        "base_feats_std = [\n",
        "    \"tmax\",\"rh\",\"wspd\",\"tp_mm\",\"dtr\",\n",
        "    \"tmax_ma3\",\"tmax_ma7\",\"rh_ma3\",\"rh_ma7\",\n",
        "    \"wspd_ma3\",\"wspd_ma7\",\"dtr_ma3\",\"dtr_ma7\",\n",
        "    \"tp_3obs_sum\",\"tp_7obs_sum\",\"dryspell\",\"slope\",\"fmi\",\"dem\"\n",
        "]\n",
        "\n",
        "# ë°ì´í„°ì…‹ ë‚´ ë‹¤ì–‘í•œ ì´ë¦„ì„ í‘œì¤€ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•œ ë³„ì¹­\n",
        "ALIASES = {\n",
        "    \"tmax\":       [\"tmax\",\"t_max\",\"tasmax\",\"tmax_c\",\"tmax_deg\",\"tmax_â„ƒ\",\"tmax_celsius\"],\n",
        "    \"rh\":         [\"rh\",\"relhum\",\"humidity\",\"relative_humidity\",\"rh_%\"],\n",
        "    \"wspd\":       [\"wspd\",\"ws\",\"wind\",\"wind_speed\",\"wspd_mps\"],\n",
        "    \"tp_mm\":      [\"tp_mm\",\"tp\",\"precip\",\"precip_mm\",\"prcp_mm\",\"rain_mm\",\"ppt_mm\"],\n",
        "    \"dtr\":        [\"dtr\",\"diurnal_range\",\"tmax_tmin_diff\",\"tdiff\"],\n",
        "    \"tmax_ma3\":   [\"tmax_ma3\",\"tmax_3ma\",\"tmax_ma_3\"],\n",
        "    \"tmax_ma7\":   [\"tmax_ma7\",\"tmax_7ma\",\"tmax_ma_7\"],\n",
        "    \"rh_ma3\":     [\"rh_ma3\",\"rh_3ma\",\"rh_ma_3\"],\n",
        "    \"rh_ma7\":     [\"rh_ma7\",\"rh_7ma\",\"rh_ma_7\"],\n",
        "    \"wspd_ma3\":   [\"wspd_ma3\",\"wspd_3ma\",\"ws_3ma\"],\n",
        "    \"wspd_ma7\":   [\"wspd_ma7\",\"wspd_7ma\",\"ws_7ma\"],\n",
        "    \"dtr_ma3\":    [\"dtr_ma3\",\"dtr_3ma\"],\n",
        "    \"dtr_ma7\":    [\"dtr_ma7\",\"dtr_7ma\"],\n",
        "    \"tp_3obs_sum\":[\"tp_3obs_sum\",\"tp_3d_sum\",\"precip_3d_sum\",\"rain_3d_sum\"],\n",
        "    \"tp_7obs_sum\":[\"tp_7obs_sum\",\"tp_7d_sum\",\"precip_7d_sum\",\"rain_7d_sum\"],\n",
        "    \"dryspell\":   [\"dryspell\",\"dry_spell\",\"drydays\",\"dry_days\"],\n",
        "    \"slope\":      [\"slope\",\"dem_slope\",\"srtm_slope\"],\n",
        "    \"fmi\":        [\"fmi\",\"forest_moisture_index\",\"fuel_moisture_index\"],\n",
        "    \"dem\":        [\"dem\",\"elev\",\"elevation\",\"altitude\",\"srtm\"]\n",
        "}\n",
        "\n",
        "present = set(df.columns)\n",
        "for std, aliases in ALIASES.items():\n",
        "    found = None\n",
        "    for a in aliases:\n",
        "        if a in present:\n",
        "            found = a; break\n",
        "    if found and (found != std) and (std not in df.columns):\n",
        "        df = df.rename(columns={found: std})\n",
        "\n",
        "# ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ íšŒê·€ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ (ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ì‚¬ìš©)\n",
        "FEATURES = [c for c in base_feats_std if c in df.columns]\n",
        "REG_FEATURES = FEATURES[:]\n",
        "print(f\"ê¸°ë³¸ í”¼ì²˜ ìˆ˜: {len(REG_FEATURES)} / í›„ë³´ {len(base_feats_std)}\")\n",
        "\n",
        "# íšŒê·€ìš© íŒŒë¼ë¯¸í„° ì •ì˜\n",
        "DATE = \"date\"\n",
        "ID_COLS = [\"season_cluster\",\"lat\",\"lon\"]\n",
        "DAILY_FFILL_LIMIT = 3\n",
        "INTERP_LIMIT      = 3\n",
        "LAG_DAYS = 7\n",
        "NA_RATIO_CUT = 0.7\n",
        "MIN_VAR = 1e-10\n",
        "MIN_FEATS = 5\n",
        "MIN_Y = 8\n",
        "MAX_L = 7\n",
        "\n",
        "# ë°ì¼ë¦¬í™” ë° ì§§ì€ ê²°ì¸¡ ë³´ê°„ì— ì‚¬ìš©í•  ì»¬ëŸ¼\n",
        "interp_feats = [c for c in (REG_FEATURES + [TARGET_CONT]) if c in df.columns]\n",
        "\n",
        "# ì¢Œí‘œÂ·ê³„ì ˆë³„ë¡œ ë‚ ì§œ ì¸ë±ìŠ¤ë¥¼ \"ë§¤ì¼\"ë¡œ í™•ì¥í•˜ê³ \n",
        "# ì§§ì€ ê°„ê²©ì˜ ê²°ì¸¡ë§Œ ì‹œê°„ ë³´ê°„/ì•ë’¤ ì±„ìš°ê¸°ë¡œ ë©”ìš°ëŠ” í•¨ìˆ˜\n",
        "def densify_daily(df_in):\n",
        "    out_list = []\n",
        "    for keys, g in df_in.sort_values(DATE).groupby(ID_COLS, dropna=False):\n",
        "        g = g.set_index(DATE).sort_index()\n",
        "\n",
        "        # ìµœëŒ€ ì˜ˆì¸¡ ë¦¬ë“œ(MAX_L)ì¼ ì´í›„ê¹Œì§€ ì¸ë±ìŠ¤ë¥¼ í™•ì¥\n",
        "        end_ext = g.index.max() + pd.Timedelta(days=MAX_L)\n",
        "        full_idx = pd.date_range(g.index.min(), end_ext, freq=\"D\")\n",
        "        g = g.reindex(full_idx)\n",
        "\n",
        "        # ID ì»¬ëŸ¼ ê°’ ìœ ì§€\n",
        "        for i, col in enumerate(ID_COLS):\n",
        "            g[col] = keys[i]\n",
        "\n",
        "        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•´ ì‹œê°„ ë³´ê°„/ì•ë’¤ ì±„ìš°ê¸°ë¥¼ ì ìš©\n",
        "        num_cols = [c for c in interp_feats if c in g.columns]\n",
        "\n",
        "        # ê¸°ìƒ/ì§€í˜• í”¼ì²˜: ì•ë’¤ ì§§ì€ ê²°ì¸¡ì—ë§Œ ë³´ê°„/ì±„ìš°ê¸° í—ˆìš©\n",
        "        g[num_cols] = g[num_cols].interpolate(method=\"time\", limit=INTERP_LIMIT, limit_direction=\"both\")\n",
        "        g[num_cols] = g[num_cols].ffill(limit=DAILY_FFILL_LIMIT).bfill(limit=DAILY_FFILL_LIMIT)\n",
        "\n",
        "        # íƒ€ê¹ƒì€ ë¯¸ë˜ ë¦¬ë“œ ë¼ë²¨ ìƒì„±ì„ ìœ„í•´ ì˜¤ë¥¸ìª½(ë¯¸ë˜ ë°©í–¥)ìœ¼ë¡œ ì¡°ê¸ˆ ë” í—ˆìš©\n",
        "        if TARGET_CONT in g.columns:\n",
        "            g[TARGET_CONT] = (\n",
        "                g[TARGET_CONT]\n",
        "                  .interpolate(method=\"time\", limit=INTERP_LIMIT, limit_direction=\"forward\")\n",
        "                  .ffill(limit=MAX_L)\n",
        "            )\n",
        "\n",
        "        g = g.reset_index().rename(columns={\"index\": DATE})\n",
        "        out_list.append(g)\n",
        "    return pd.concat(out_list, ignore_index=True)\n",
        "\n",
        "# ì¢Œí‘œÂ·ê³„ì ˆë³„ ë°ì¼ë¦¬ ì‹œê³„ì—´ ìƒì„±\n",
        "df_dense = densify_daily(df)\n",
        "print(\"Dense shape:\", df_dense.shape)\n",
        "\n",
        "# íƒ€ê¹ƒì˜ ê³¼ê±°ê°’(ë˜ê·¸) í”¼ì²˜ ì¶”ê°€ (0ì¼ì „, 1ì¼ì „, ..., LAG_DAYS-1ì¼ì „)\n",
        "grp = df_dense.sort_values(ID_COLS + [DATE]).groupby(ID_COLS, group_keys=False)\n",
        "for k in range(0, LAG_DAYS):\n",
        "    lag_col = f\"{TARGET_CONT}_lag{k}\"\n",
        "    df_dense[lag_col] = grp[TARGET_CONT].shift(k)\n",
        "\n",
        "lag_feats = [f\"{TARGET_CONT}_lag{k}\" for k in range(0, LAG_DAYS)]\n",
        "REG_FEATURES = REG_FEATURES + [c for c in lag_feats if c in df_dense.columns]\n",
        "print(f\"ë˜ê·¸ í¬í•¨ í”¼ì²˜ ìˆ˜: {len(REG_FEATURES)}\")\n",
        "\n",
        "# date+L ë§¤ì¹­ ëŒ€ì‹ , ê·¸ë£¹ ë‚´ì—ì„œ ë‹¨ìˆœ ì‹œí”„íŠ¸ë¡œ ë¦¬ë“œ íƒ€ê¹ƒ ìƒì„±\n",
        "# ê° Lì— ëŒ€í•´ í˜„ì¬ ì‹œì ì—ì„œ Lì¼ ë’¤ì˜ ê°’ì„ íƒ€ê¹ƒìœ¼ë¡œ ì‚¬ìš©\n",
        "def make_leads_by_shift(df_in, id_cols, date_col, target_col, max_l=7):\n",
        "    out = df_in.sort_values(id_cols + [date_col]).copy()\n",
        "    g = out.groupby(id_cols, group_keys=False)\n",
        "    for L in range(0, max_l+1):\n",
        "        out[f\"{target_col}_L{L}\"] = g[target_col].shift(-L)\n",
        "    return out\n",
        "\n",
        "df_reg = make_leads_by_shift(df_dense, ID_COLS, DATE, TARGET_CONT, max_l=MAX_L)\n",
        "\n",
        "# íšŒê·€ìš© í•™ìŠµ/í‰ê°€ ê¸°ê°„ ë¶„í• \n",
        "train_r = df_reg[(df_reg[\"year\"]>=2015) & (df_reg[\"year\"]<=2022)].copy()\n",
        "test23_r = df_reg[df_reg[\"year\"]==2023].copy()\n",
        "ext24_r  = df_reg[df_reg[\"year\"]==2024].copy()\n",
        "print(\"Shapes (reg):\", \"train:\", train_r.shape, \"2023:\", test23_r.shape, \"2024:\", ext24_r.shape)\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import ElasticNet, Ridge\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# ì†Œê·œëª¨ ë°ì´í„°ì— ì í•©í•œ LightGBM íšŒê·€ê¸° ìƒì„± í•¨ìˆ˜\n",
        "def make_lgbm_smalldata():\n",
        "    params = dict(\n",
        "        n_estimators=1200, learning_rate=0.02,\n",
        "        num_leaves=31, max_depth=6,\n",
        "        subsample=0.9, colsample_bytree=0.9,\n",
        "        min_child_samples=5, random_state=123, n_jobs=-1\n",
        "    )\n",
        "    try:\n",
        "        params.update(dict(min_data_in_bin=1, feature_pre_filter=False, force_col_wise=True))\n",
        "        _ = LGBMRegressor(**params)\n",
        "    except TypeError:\n",
        "        for k in [\"min_data_in_bin\",\"feature_pre_filter\",\"force_col_wise\"]:\n",
        "            params.pop(k, None)\n",
        "    return LGBMRegressor(**params)\n",
        "\n",
        "# ë¦¬ë“œ Lì— ëŒ€í•´ í•™ìŠµ/í‰ê°€ìš© X, yë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜\n",
        "def prepare_xy(sub, L, feats, target_col, na_ratio_cut=NA_RATIO_CUT):\n",
        "    y = sub[f\"{target_col}_L{L}\"].values\n",
        "    X = sub[feats].replace([np.inf,-np.inf], np.nan)\n",
        "    ok = ~np.isnan(y)\n",
        "    X, y = X.loc[ok], y[ok]\n",
        "    # ì»¬ëŸ¼ë³„ ê²°ì¸¡ ë¹„ìœ¨ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ì œê±°\n",
        "    keep = X.isna().mean() <= na_ratio_cut\n",
        "    X = X.loc[:, keep.values]\n",
        "    # ì¤‘ì•™ê°’ ëŒ€ì²´ í›„ ë¶„ì‚°ì´ ê±°ì˜ ì—†ëŠ” ì»¬ëŸ¼ ì œê±°\n",
        "    X = X.fillna(X.median())\n",
        "    var = X.var(ddof=0)\n",
        "    X = X.loc[:, var > MIN_VAR]\n",
        "    # í”¼ì²˜ ìˆ˜ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê¸°ë³¸ ê¸°ìƒ ì½”ì–´ í”¼ì²˜ë¥¼ ì¶”ê°€ë¡œ ë³´ì¥\n",
        "    if X.shape[1] < MIN_FEATS:\n",
        "        core = [c for c in [\"tmax\",\"rh\",\"wspd\",\"tp_mm\",\"dtr\"] if c in sub.columns]\n",
        "        add = sub.loc[ok, core].replace([np.inf,-np.inf], np.nan).fillna(sub[core].median())\n",
        "        X = pd.concat([X, add], axis=1)\n",
        "        var = X.var(ddof=0)\n",
        "        X = X.loc[:, var > MIN_VAR]\n",
        "    return X, y, list(X.columns)\n",
        "\n",
        "# ì†Œê·œëª¨ ë°ì´í„°ìš© íšŒê·€ ëª¨ë¸ í•™ìŠµ (LGBM ìš°ì„ , í•„ìš” ì‹œ ElasticNetìœ¼ë¡œ í´ë°±)\n",
        "def train_regressor_small_data(X, y, L=None):\n",
        "    # ë¦¬ë“œê°€ í¬ê±°ë‚˜ í‘œë³¸ì´ ì ìœ¼ë©´ ENetì„ ìš°ì„  ì‚¬ìš©\n",
        "    if L is None or len(y) < 80 or (isinstance(L, int) and L >= 5):\n",
        "        model = Pipeline([(\"scaler\", StandardScaler()),\n",
        "                          (\"enet\", ElasticNet(alpha=0.05, l1_ratio=0.2, max_iter=10000, random_state=123))])\n",
        "        model.fit(X, y);  return model, \"ENet\"\n",
        "    lgb = make_lgbm_smalldata(); lgb.fit(X, y)\n",
        "    # LightGBMì´ ì‚¬ì‹¤ìƒ í•™ìŠµë˜ì§€ ì•Šì€ ê²½ìš° ENetìœ¼ë¡œ ëŒ€ì²´\n",
        "    if getattr(lgb, \"best_iteration_\", 1) <= 1 or (np.asarray(getattr(lgb, \"feature_importances_\", []))==0).all():\n",
        "        model = Pipeline([(\"scaler\", StandardScaler()),\n",
        "                          (\"enet\", ElasticNet(alpha=0.03, l1_ratio=0.15, max_iter=10000, random_state=123))])\n",
        "        model.fit(X, y);  return model, \"ENet(fallback)\"\n",
        "    return lgb, \"LGBM\"\n",
        "\n",
        "# íšŒê·€ í‰ê°€ í•¨ìˆ˜ (MAE, RMSE, ìƒê´€ê³„ìˆ˜ r, ìƒëŒ€ RMSE ë“± ê³„ì‚°)\n",
        "def eval_reg(hold, L, model, feats):\n",
        "    y = hold[f\"{TARGET_CONT}_L{L}\"].values\n",
        "    X = hold[feats].replace([np.inf,-np.inf], np.nan)\n",
        "    ok = ~np.isnan(y)\n",
        "    y, X = y[ok], X[ok]\n",
        "    if len(y)==0 or X.shape[1]==0:\n",
        "        return {\"MAE\":np.nan,\"RMSE\":np.nan,\"r\":np.nan,\"rRMSE\":np.nan}, np.array([]), np.array([])\n",
        "    ph = model.predict(X.fillna(X.median()))\n",
        "    mae  = mean_absolute_error(y, ph)\n",
        "    rmse = np.sqrt(mean_squared_error(y, ph))\n",
        "    r    = pearsonr(y, ph)[0] if (len(y)>2 and np.std(ph)>0 and np.std(y)>0) else np.nan\n",
        "    rrmse = rmse / (np.nanmean(y)+1e-9)\n",
        "    return {\"MAE\":mae,\"RMSE\":rmse,\"r\":r,\"rRMSE\":rrmse}, ph, y\n",
        "\n",
        "# ë°ì´í„°ê°€ ê±°ì˜ ì—†ê±°ë‚˜ íƒ€ê¹ƒì´ ëª¨ë‘ NaNì¸ ê²½ìš° ì‚¬ìš©í•  ì•ˆì „í•œ í‰ê· ê°’ ê³„ì‚°\n",
        "def safe_mean_constant(sub_df, target_col, L):\n",
        "    mu = np.nanmean(sub_df[f\"{target_col}_L{L}\"].values)\n",
        "    if not np.isfinite(mu): mu = np.nanmean(sub_df[target_col].values)\n",
        "    if not np.isfinite(mu): mu = 0.0\n",
        "    return float(mu)\n",
        "\n",
        "# íšŒê·€ ëª¨ë¸ ë° ì‚¬ìš© í”¼ì²˜ ì €ì¥ ê²½ë¡œ ìƒì„±\n",
        "os.makedirs(\"/content/artifacts_reg\", exist_ok=True)\n",
        "reg_models = {}; feature_used_by_L = {}\n",
        "\n",
        "# ë¦¬ë“œ L=0~MAX_Lê¹Œì§€ ìˆœíšŒí•˜ë©° ê°œë³„ íšŒê·€ ëª¨ë¸ í•™ìŠµ\n",
        "for L in range(0, MAX_L+1):\n",
        "    Xtr, ytr, used_feats = prepare_xy(train_r, L, REG_FEATURES, TARGET_CONT)\n",
        "    if len(ytr) < MIN_Y or len(used_feats)==0:\n",
        "        mu = safe_mean_constant(train_r, TARGET_CONT, L)\n",
        "        mdl, tag = DummyRegressor(strategy=\"constant\", constant=mu), \"Mean\"\n",
        "        mdl.fit(pd.DataFrame({\"c\":[0.0]}), [mu]); used_feats=[]\n",
        "    else:\n",
        "        mdl, tag = train_regressor_small_data(Xtr, ytr)\n",
        "    reg_models[L] = mdl; feature_used_by_L[L] = used_feats\n",
        "    print(f\"[L={L}] model={tag}, n={len(ytr)}, used_feats={len(used_feats)}\")\n",
        "\n",
        "# ë¦¬ë“œë³„ ëª¨ë¸ dictì™€ ì‚¬ìš© í”¼ì²˜ dict ì €ì¥\n",
        "joblib.dump(reg_models, \"/content/artifacts_reg/reg_L0_7_rowlead.joblib\")\n",
        "with open(\"/content/artifacts_reg/reg_features_used.json\",\"w\") as f: json.dump(feature_used_by_L, f, indent=2)\n",
        "print(\"Saved: /content/artifacts_reg/reg_L0_7_rowlead.joblib, reg_features_used.json\")\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„°ì˜ í”¼ì²˜ë³„ ì¤‘ì•™ê°’ (ê²°ì¸¡ ëŒ€ì²´ì— ì¼ê´€ë˜ê²Œ ì‚¬ìš©)\n",
        "train_medians = train_r[REG_FEATURES].replace([np.inf,-np.inf], np.nan).median()\n",
        "\n",
        "# 2023, 2024ì— ëŒ€í•´ ë¦¬ë“œë³„ íšŒê·€ ì„±ëŠ¥í‘œ ì¶œë ¥\n",
        "for name, hold in {\"2023\":test23_r, \"2024\":ext24_r}.items():\n",
        "    rows=[]\n",
        "    for L in range(0, MAX_L+1):\n",
        "        featsL = feature_used_by_L[L]\n",
        "        if len(featsL)==0:\n",
        "            metr = {\"MAE\":np.nan,\"RMSE\":np.nan,\"r\":np.nan,\"rRMSE\":np.nan}\n",
        "        else:\n",
        "            y = hold[f\"{TARGET_CONT}_L{L}\"].values\n",
        "            X = hold[featsL].replace([np.inf,-np.inf], np.nan)\n",
        "            ok = ~np.isnan(y); y, X = y[ok], X[ok]\n",
        "            if len(y)==0 or X.shape[1]==0:\n",
        "                metr = {\"MAE\":np.nan,\"RMSE\":np.nan,\"r\":np.nan,\"rRMSE\":np.nan}\n",
        "            else:\n",
        "                X = X.fillna(train_medians.reindex(featsL))\n",
        "                ph = reg_models[L].predict(X)\n",
        "                mae  = mean_absolute_error(y, ph)\n",
        "                rmse = np.sqrt(mean_squared_error(y, ph))\n",
        "                r    = pearsonr(y, ph)[0] if (len(y)>2 and np.std(ph)>0 and np.std(y)>0) else np.nan\n",
        "                rrmse = rmse / (np.nanmean(y)+1e-9)\n",
        "                metr = {\"MAE\":mae,\"RMSE\":rmse,\"r\":r,\"rRMSE\":rrmse}\n",
        "        rows.append({\"L\":L, **metr})\n",
        "    print(f\"[{name}]\")\n",
        "    try:\n",
        "        from IPython.display import display\n",
        "        display(pd.DataFrame(rows))\n",
        "    except:\n",
        "        print(pd.DataFrame(rows).to_string(index=False))\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„° ë¶„í¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ FFDRI/FRI_norm ë“±ê¸‰ êµ¬ê°„ ê³„ì‚°\n",
        "score = train_r[TARGET_CONT].values\n",
        "valid_n = np.isfinite(score).sum()\n",
        "q = np.nanquantile(score, [0.5, 0.65, 0.85]) if valid_n>10 else [np.nanquantile(score,p) for p in [0.25,0.5,0.75]]\n",
        "grades = {\"Low\":[-1e18,float(q[0])],\"Moderate\":[float(q[0]),float(q[1])],\n",
        "          \"High\":[float(q[1]),float(q[2])],\"VeryHigh\":[float(q[2]),1e18]}\n",
        "with open(\"/content/artifacts_reg/grades_reg.json\",\"w\") as f: json.dump(grades, f, indent=2)\n",
        "print(\"Saved: /content/artifacts_reg/grades_reg.json\")\n",
        "\n",
        "# ì—°ì† ì ìˆ˜ë¥¼ ë“±ê¸‰ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼\n",
        "def score_to_grade(x):\n",
        "    for k,(a,b) in grades.items():\n",
        "        if a<=x<b: return k\n",
        "    return \"VeryHigh\"\n",
        "\n",
        "# 2023, 2024ì— ëŒ€í•´ ê° ë¦¬ë“œë³„ ì˜ˆì¸¡ê°’ê³¼ L0 ë“±ê¸‰ì„ í¬í•¨í•œ CSV ì €ì¥\n",
        "for name, hold in {\"2023\":test23_r, \"2024\":ext24_r}.items():\n",
        "    if len(hold)==0:\n",
        "        print(f\"[{name}] no rows.\")\n",
        "        continue\n",
        "    out = hold[[\"date\",\"lon\",\"lat\",\"season_cluster\"]].copy()\n",
        "    for L in range(0, MAX_L+1):\n",
        "        featsL = feature_used_by_L[L]\n",
        "        if len(featsL)==0:\n",
        "            mu = safe_mean_constant(train_r, TARGET_CONT, L)\n",
        "            yhat = np.full(len(out), mu, dtype=float)\n",
        "        else:\n",
        "            Xh = hold[featsL].replace([np.inf,-np.inf], np.nan)\n",
        "            Xh = Xh.fillna(train_medians.reindex(featsL))\n",
        "            yhat = reg_models[L].predict(Xh)\n",
        "        out[f\"{TARGET_CONT}_hat_L{L}\"] = yhat\n",
        "    out[\"grade_hat_L0\"] = [score_to_grade(x) for x in out[f\"{TARGET_CONT}_hat_L0\"].values]\n",
        "    p = f\"/content/pred_reg_{name}_rowlead.csv\"; out.to_csv(p, index=False)\n",
        "    print(\"Saved:\", p)\n",
        "    try: files.download(p)\n",
        "    except: pass\n",
        "\n",
        "# íšŒê·€ ì•„í‹°íŒ©íŠ¸ í´ë”ë¥¼ ZIPìœ¼ë¡œ ë¬¶ì–´ ì €ì¥\n",
        "shutil.make_archive(\"/content/artifacts_reg_bundle_rowlead\", \"zip\", \"/content/artifacts_reg\")\n",
        "try: files.download(\"/content/artifacts_reg_bundle_rowlead.zip\")\n",
        "except: print(\"[local] ZIP ì €ì¥ ì™„ë£Œ: /content/artifacts_reg_bundle_rowlead.zip\")\n",
        "\n",
        "# ë””ë²„ê·¸ìš©: ë¦¬ë“œë³„ ì‹¤ì œ ì‚¬ìš© í”¼ì²˜ ëª©ë¡ ì¼ë¶€ ì¶œë ¥\n",
        "for L in range(0, MAX_L+1):\n",
        "    feats = feature_used_by_L.get(L, [])\n",
        "    print(f\"[DEBUG] L={L} used_feats={len(feats)} -> {feats[:10]}\")\n",
        "\n",
        "\n",
        "\n",
        "### í´ëŸ¬ìŠ¤í„°ë§\n",
        "\n",
        "# NDVI, ëˆ„ì  ì¼ì‚¬ëŸ‰(DSR_total_MJm^2), FFDRIë¥¼ ì´ìš©í•´ ì‚°ë¦¼ ìƒíƒœ/ìœ„í—˜ ìœ í˜•ì„ êµ°ì§‘í™”í•˜ëŠ” ì½”ë“œ\n",
        "# ì…ë ¥:\n",
        "#   - df_out_removed (ì „ì²˜ë¦¬ í›„ ì´ìƒì¹˜ ì œê±°ëœ ë°ì´í„°í”„ë ˆì„, NDVI/DSR/FFDRI í¬í•¨)\n",
        "# ì²˜ë¦¬:\n",
        "#   - RobustScalerë¡œ ì´ìƒì¹˜ì— ê°•ê±´í•œ ìŠ¤ì¼€ì¼ë§\n",
        "#   - KMeans(k=2)ë¡œ ë‘ ê°œì˜ í´ëŸ¬ìŠ¤í„°ë¡œ ë‚˜ëˆ”\n",
        "#   - PCA 2ì°¨ì›ìœ¼ë¡œ ì°¨ì› ì¶•ì†Œ í›„ êµ°ì§‘ì„ ì‹œê°í™”\n",
        "# ì¶œë ¥:\n",
        "#   - df[\"cluster_k2\"]ì— êµ°ì§‘ ë ˆì´ë¸” ì¶”ê°€\n",
        "#   - PCA ì¢Œí‘œ(_PCA1, _PCA2) ë° ì‚°ì ë„ í”Œë¡¯\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "FEATURES = ['NDVI', 'DSR_total_MJm^2','FFDRI']\n",
        "\n",
        "# ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ df_out_removedë¥¼ ë³µì‚¬í•´ì„œ ì‚¬ìš©\n",
        "df = df_out_removed.copy()\n",
        "\n",
        "# ì›ë³¸ í”¼ì²˜ë§Œ ì¶”ì¶œ\n",
        "X_raw = df[FEATURES].copy()\n",
        "\n",
        "# ì´ìƒì¹˜ì— ëœ ë¯¼ê°í•œ RobustScalerë¡œ ìŠ¤ì¼€ì¼ë§\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X_raw)\n",
        "\n",
        "# ìŠ¤ì¼€ì¼ë§ëœ ê°’ì„ ë³„ë„ DataFrameìœ¼ë¡œ ë³´ê´€ (í•„ìš” ì‹œ ì¶”ê°€ ë¶„ì„ìš©)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=[f\"scaled_{c}\" for c in FEATURES], index=df.index)\n",
        "\n",
        "# KMeans(k=2) êµ°ì§‘í™” ìˆ˜í–‰\n",
        "kmeans = KMeans(n_clusters=2, random_state=RANDOM_STATE, n_init=10)\n",
        "labels = kmeans.fit_predict(X_scaled)\n",
        "df[\"cluster_k2\"] = labels\n",
        "\n",
        "# PCAë¥¼ ì´ìš©í•´ 2ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜ (ì‹œê°í™”ìš©)\n",
        "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "df[\"_PCA1\"], df[\"_PCA2\"] = X_pca[:, 0], X_pca[:, 1]\n",
        "\n",
        "# PCA ê³µê°„ì—ì„œ êµ°ì§‘ë³„ ì‚°ì ë„ ì‹œê°í™”\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.scatterplot(data=df, x=\"_PCA1\", y=\"_PCA2\", hue=\"cluster_k2\",\n",
        "                s=50, alpha=0.85, edgecolor=\"none\")\n",
        "plt.title(\"K-Means (k=2) with Robust Scaling\")\n",
        "plt.xlabel(f\"PCA1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)\")\n",
        "plt.ylabel(f\"PCA2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)\")\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "### ì¢Œí‘œ ì¶”ì¶œ ë° ìœ„í—˜ í›„ë³´ ì§€ì—­ ì„ ì •\n",
        "\n",
        "# ì¢Œí‘œë³„(2019-01-01 ~ 2024-12-31)ë¡œ ERA5 ê¸°ìƒ ì¼í‰ê· ì„ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸\n",
        "# ì…ë ¥:\n",
        "#   - /content/sites_by_type.csv  (í•„ìˆ˜ ì»¬ëŸ¼: lat, lon)\n",
        "# ì¶œë ¥:\n",
        "#   - /content/fri_inputs_by_row.csv\n",
        "# í¬í•¨ ë³€ìˆ˜:\n",
        "#   - Tmean  (ì¼ í‰ê·  ê¸°ì˜¨, â„ƒ)\n",
        "#   - RH     (ìƒëŒ€ìŠµë„, %)\n",
        "#   - WSPD   (í’ì†, m/s)\n",
        "#   - TP_mm  (ì¼ê°•ìˆ˜ëŸ‰, mm/day)\n",
        "# íŠ¹ì§•:\n",
        "#   - ERA5-Land ìš°ì„  ì‚¬ìš©, ê²°ì¸¡ ì‹œ ERA5-Globalë¡œ ë³´ì™„\n",
        "#   - íƒ€ì„ì•„ì›ƒ ì™„í™” ì˜µì…˜(tileScale) ë° ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì´ìš©í•œ ì¬ì‹œë„\n",
        "#   - ì¼ì • ê°„ê²©ë§ˆë‹¤ CSVë¥¼ ì €ì¥í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ë°©ì‹ìœ¼ë¡œ ì´ì–´ì„œ ì‹¤í–‰ ê°€ëŠ¥\n",
        "\n",
        "!pip -q install earthengine-api pandas tqdm\n",
        "\n",
        "import ee, pandas as pd, datetime as dt, time\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "\n",
        "# GEE í”„ë¡œì íŠ¸ ë° ê²½ë¡œ, ìƒ˜í”Œë§ ê´€ë ¨ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "PROJECT_ID = \"solid-time-472606-u0\"     # ë³¸ì¸ GEE í”„ë¡œì íŠ¸ ID\n",
        "CSV_PATH   = \"/content/sites_by_type.csv\"\n",
        "OUT_CSV    = \"/content/fri_inputs_by_row.csv\"\n",
        "\n",
        "# ERA5 í•´ìƒë„(~9km) ê¸°ì¤€ ê¶Œì¥ ë°˜ê²½/ìŠ¤ì¼€ì¼\n",
        "SAMPLE_RADIUS_M = 9000      # í¬ì¸íŠ¸ ì£¼ë³€ í‰ê· ì„ ë‚¼ ë°˜ê²½\n",
        "SAMPLE_SCALE_M  = 9000      # reduceRegions ìŠ¤ì¼€ì¼\n",
        "TILESCALE       = 4         # ì„œë²„ ë©”ëª¨ë¦¬/íƒ€ì„ì•„ì›ƒ ì¡°ì ˆìš© (í•„ìš” ì‹œ 8ê¹Œì§€ ì¡°ì • ê°€ëŠ¥)\n",
        "\n",
        "# ì¶”ì¶œ ë‚ ì§œ ë²”ìœ„\n",
        "DATE_START = dt.date(2019, 1, 1)\n",
        "DATE_END   = dt.date(2024,12,31)\n",
        "\n",
        "# í•„ìš” ì‹œ ë‚ ì§œë¥¼ í†µì§¸ë¡œ ì´ë™ì‹œí‚¬ ë³´ì • ê°’ (ì¼ ë‹¨ìœ„, ë³´í†µ 0 ìœ ì§€)\n",
        "DATE_SHIFT_DAYS = 0\n",
        "\n",
        "# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°(ì¼ ë‹¨ìœ„)\n",
        "CHECKPOINT_EVERY = 50\n",
        "\n",
        "# getInfo ì¬ì‹œë„ íšŸìˆ˜ì™€ ë°±ì˜¤í”„ ê³„ìˆ˜\n",
        "MAX_RETRY   = 6\n",
        "BACKOFF_BASE= 1.5\n",
        "\n",
        "# GEE ì´ˆê¸°í™”\n",
        "try:\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "except Exception:\n",
        "    ee.Authenticate(project=PROJECT_ID)\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "print(\"[GEE] initialized\")\n",
        "ee.data.setDeadline(120000)  # ìš”ì²­ íƒ€ì„ë¦¬ë°‹(ms)\n",
        "\n",
        "# ì…ë ¥ CSV ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ í•„ìš” ì‹œ ì—…ë¡œë“œ\n",
        "try:\n",
        "    _ = open(CSV_PATH, \"r\")\n",
        "except FileNotFoundError:\n",
        "    print(\"[UPLOAD] sites_by_type.csv íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”\")\n",
        "    uploaded = files.upload()\n",
        "    if \"sites_by_type.csv\" not in uploaded:\n",
        "        name = next(iter(uploaded))\n",
        "        with open(CSV_PATH, \"wb\") as f:\n",
        "            f.write(uploaded[name])\n",
        "        print(f\"[INFO] ì—…ë¡œë“œ íŒŒì¼ì„ sites_by_type.csvë¡œ ì €ì¥: {name} â†’ sites_by_type.csv\")\n",
        "    else:\n",
        "        print(\"[INFO] sites_by_type.csv ì—…ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "# ì¢Œí‘œ ëª©ë¡ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "df_sites = pd.read_csv(CSV_PATH, encoding=\"utf-8-sig\", engine=\"python\")\n",
        "df_sites.columns = [c.strip().lower().replace(\"\\ufeff\",\"\") for c in df_sites.columns]\n",
        "\n",
        "need = {\"lat\",\"lon\"}\n",
        "if not (need <= set(df_sites.columns)):\n",
        "    raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½. í˜„ì¬ í—¤ë”: {list(df_sites.columns)} (í•„ìš”: {sorted(need)})\")\n",
        "\n",
        "df_sites[\"lon\"] = pd.to_numeric(df_sites[\"lon\"], errors=\"coerce\")\n",
        "df_sites[\"lat\"] = pd.to_numeric(df_sites[\"lat\"], errors=\"coerce\")\n",
        "df_sites = df_sites.dropna(subset=[\"lon\",\"lat\"]).drop_duplicates(subset=[\"lon\",\"lat\"]).reset_index(drop=True)\n",
        "df_sites[\"pid\"] = df_sites.index.astype(int)\n",
        "\n",
        "print(f\"[INFO] sites: {len(df_sites)} rows (ì˜ˆìƒ: 15)\")\n",
        "print(df_sites.head())\n",
        "\n",
        "# ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
        "def daterange(d0, d1):\n",
        "    cur = d0\n",
        "    while cur <= d1:\n",
        "        yield cur\n",
        "        cur = cur + dt.timedelta(days=1)\n",
        "\n",
        "unique_dates = [d for d in daterange(DATE_START, DATE_END)]\n",
        "if DATE_SHIFT_DAYS != 0:\n",
        "    unique_dates = [d + dt.timedelta(days=DATE_SHIFT_DAYS) for d in unique_dates]\n",
        "unique_dates_str = [d.isoformat() for d in unique_dates]\n",
        "print(f\"[INFO] date range: {unique_dates_str[0]} ~ {unique_dates_str[-1]}  (days={len(unique_dates_str)})  # ì˜ˆìƒ=2192\")\n",
        "\n",
        "# ERA5-Land, ERA5-Global ì»¬ë ‰ì…˜ ì •ì˜\n",
        "ERA5_LAND   = ee.ImageCollection(\"ECMWF/ERA5_LAND/HOURLY\")\n",
        "ERA5_GLOBAL = ee.ImageCollection(\"ECMWF/ERA5/HOURLY\")\n",
        "\n",
        "# ì‹œê°„ë‹¹ ì´ë¯¸ì§€ë¥¼ ì¼í‰ê· /ì¼í•©ì‚° ë³€ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
        "def _per_hour_to_vars(im):\n",
        "    # ì˜¨ë„(K)ë¥¼ â„ƒë¡œ ë³€í™˜\n",
        "    T  = im.select(\"temperature_2m\").subtract(273.15).rename(\"Tmean\")\n",
        "    Td = im.select(\"dewpoint_temperature_2m\").subtract(273.15)\n",
        "    # Magnus ê³µì‹ì„ ì´ìš©í•œ ìƒëŒ€ìŠµë„ ê³„ì‚°\n",
        "    a, b = 17.625, 243.04\n",
        "    RH = Td.expression(\n",
        "        \"100*exp(a*Td/(b+Td) - a*T/(b+T))\",\n",
        "        {\"a\": a, \"b\": b, \"Td\": Td, \"T\": T}\n",
        "    ).rename(\"RH\")\n",
        "    # 10m ë°”ëŒì„±ë¶„ì—ì„œ í’ì†(m/s) ê³„ì‚°\n",
        "    U = im.select(\"u_component_of_wind_10m\")\n",
        "    V = im.select(\"v_component_of_wind_10m\")\n",
        "    WSPD = U.pow(2).add(V.pow(2)).sqrt().rename(\"WSPD\")\n",
        "    # ê°•ìˆ˜ëŸ‰(m)ì„ mmë¡œ ë³€í™˜\n",
        "    TP = im.select(\"total_precipitation\").multiply(1000).rename(\"TP_mm\")\n",
        "    return T.addBands([RH, WSPD, TP])\n",
        "\n",
        "# íŠ¹ì • ë‚ ì§œì˜ ì¼í‰ê· /ì¼í•©ì‚° ì´ë¯¸ì§€ ìƒì„±\n",
        "def _daily_from(ic, date_str):\n",
        "    d0 = ee.Date(date_str); d1 = d0.advance(1, \"day\")\n",
        "    hourly = ic.filterDate(d0, d1).map(_per_hour_to_vars)\n",
        "    Tmean = hourly.select(\"Tmean\").mean()\n",
        "    RH    = hourly.select(\"RH\").mean()\n",
        "    WSPD  = hourly.select(\"WSPD\").mean()\n",
        "    TP    = hourly.select(\"TP_mm\").sum()\n",
        "    return Tmean.addBands([RH, WSPD, TP])\n",
        "\n",
        "# ERA5-Landì™€ ERA5-Globalë¥¼ ê²°í•©í•œ ì¼ ë‹¨ìœ„ ì´ë¯¸ì§€ ìƒì„±\n",
        "def daily_era5(date_str):\n",
        "    land  = _daily_from(ERA5_LAND,   date_str)\n",
        "    globe = _daily_from(ERA5_GLOBAL, date_str)\n",
        "    fused = land.unmask(globe)  # Landì—ì„œ ëˆ„ë½ëœ í”½ì…€ì€ Globalë¡œ ëŒ€ì²´\n",
        "    return fused.set({\"date\": date_str})\n",
        "\n",
        "# getInfoì— ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ì ìš©\n",
        "def safe_getInfo(ee_obj):\n",
        "    for k in range(MAX_RETRY):\n",
        "        try:\n",
        "            return ee_obj.getInfo()\n",
        "        except Exception as e:\n",
        "            wait = BACKOFF_BASE**k\n",
        "            print(f\"[WARN] getInfo retry {k+1}/{MAX_RETRY} in {wait:.2f}s -> {e}\")\n",
        "            time.sleep(wait)\n",
        "    return ee_obj.getInfo()\n",
        "\n",
        "# ì´ì–´ë‹¬ë¦¬ê¸°ë¥¼ ìœ„í•œ ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ (ìˆë‹¤ë©´ ì¬ì‚¬ìš©)\n",
        "try:\n",
        "    df_out = pd.read_csv(OUT_CSV)\n",
        "    done_keys = set(zip(df_out['date'].astype(str), df_out['pid'].astype(int)))\n",
        "    rows_out = df_out.to_dict('records')\n",
        "    print(f\"[RESUME] existing rows: {len(rows_out)} (ì´ì–´ë‹¬ë¦¬ê¸°)\")\n",
        "except Exception:\n",
        "    rows_out = []\n",
        "    done_keys = set()\n",
        "\n",
        "# ë‚ ì§œ Ã— ì¢Œí‘œ ì¡°í•©ì— ëŒ€í•´ ìƒ˜í”Œë§ ì‹¤í–‰\n",
        "total_dates = len(unique_dates_str)\n",
        "print(f\"[INFO] sampling by date Ã— {len(df_sites)} sites  (dates={total_dates})\")\n",
        "\n",
        "processed_since_cp = 0\n",
        "for idx, d in enumerate(tqdm(unique_dates_str, desc=\"[sampling by date]\"), 1):\n",
        "    # ì´ ë‚ ì§œì™€ pid ì¡°í•©ì´ ëª¨ë‘ ì™„ë£Œë˜ì–´ ìˆìœ¼ë©´ ê±´ë„ˆëœ€\n",
        "    if all((d, int(pid)) in done_keys for pid in df_sites['pid'].tolist()):\n",
        "        continue\n",
        "\n",
        "    # ë‚ ì§œë³„ ì¢Œí‘œë¥¼ FeatureCollectionìœ¼ë¡œ êµ¬ì„±, ê° í¬ì¸íŠ¸ëŠ” ë²„í¼ ì˜ì—­ìœ¼ë¡œ í™•ì¥\n",
        "    fc = ee.FeatureCollection([\n",
        "        ee.Feature(\n",
        "            ee.Geometry.Point([float(r[\"lon\"]), float(r[\"lat\"])]).buffer(SAMPLE_RADIUS_M),\n",
        "            {\"pid\": int(r[\"pid\"]), \"lon\": float(r[\"lon\"]), \"lat\": float(r[\"lat\"])}\n",
        "        )\n",
        "        for _, r in df_sites.iterrows()\n",
        "    ])\n",
        "\n",
        "    # í•´ë‹¹ ë‚ ì§œì˜ ERA5 ì¼ë³„ ì´ë¯¸ì§€ êµ¬ì„±\n",
        "    img = daily_era5(d)\n",
        "\n",
        "    # reduceRegionsë¡œ ëª¨ë“  í¬ì¸íŠ¸ì— ëŒ€í•´ í‰ê· ê°’ ì¶”ì¶œ\n",
        "    reduced = img.reduceRegions(\n",
        "        collection=fc,\n",
        "        reducer=ee.Reducer.mean(),\n",
        "        scale=SAMPLE_SCALE_M,\n",
        "        tileScale=TILESCALE\n",
        "    ).map(lambda f: f.set({\"date\": d}))\n",
        "\n",
        "    data  = safe_getInfo(reduced)\n",
        "    feats = data.get(\"features\", []) if data else []\n",
        "    for f in feats:\n",
        "        p = f.get(\"properties\", {})\n",
        "        key = (str(p.get(\"date\")), int(p.get(\"pid\")))\n",
        "        if key in done_keys:\n",
        "            continue\n",
        "        rows_out.append({\n",
        "            \"date\":  p.get(\"date\"),\n",
        "            \"pid\":   p.get(\"pid\"),\n",
        "            \"lon\":   p.get(\"lon\"),\n",
        "            \"lat\":   p.get(\"lat\"),\n",
        "            \"Tmean\": p.get(\"Tmean_mean\", p.get(\"Tmean\")),\n",
        "            \"RH\":    p.get(\"RH_mean\",    p.get(\"RH\")),\n",
        "            \"WSPD\":  p.get(\"WSPD_mean\",  p.get(\"WSPD\")),\n",
        "            \"TP_mm\": p.get(\"TP_mm_mean\", p.get(\"TP_mm\")),\n",
        "        })\n",
        "        done_keys.add(key)\n",
        "\n",
        "    processed_since_cp += 1\n",
        "    if processed_since_cp >= CHECKPOINT_EVERY or idx == total_dates:\n",
        "        tmp = pd.DataFrame(rows_out).sort_values([\"date\",\"pid\"]).reset_index(drop=True)\n",
        "        tmp.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"[CP] saved {len(tmp)} rows at {d}  ({idx}/{total_dates})\")\n",
        "        processed_since_cp = 0\n",
        "\n",
        "# ìµœì¢… ê²°ê³¼ ì €ì¥ ë° í’ˆì§ˆ ì ê²€\n",
        "df_out = pd.DataFrame(rows_out).sort_values([\"date\",\"pid\"]).reset_index(drop=True)\n",
        "df_out.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "num_total   = len(df_out)\n",
        "num_dates   = df_out[\"date\"].nunique()\n",
        "num_sites   = df_out[\"pid\"].nunique()\n",
        "num_all_nan = df_out[[\"Tmean\",\"RH\",\"WSPD\",\"TP_mm\"]].isna().all(axis=1).sum()\n",
        "num_any_nan = df_out[[\"Tmean\",\"RH\",\"WSPD\",\"TP_mm\"]].isna().any(axis=1).sum()\n",
        "\n",
        "print(f\"[SAVED] {OUT_CSV}  rows={num_total}  (sites={num_sites}, dates={num_dates})\")\n",
        "print(f\"[QC] all-NaN rows = {num_all_nan} / any-NaN rows = {num_any_nan}\")\n",
        "\n",
        "# ì¢Œí‘œê°€ í•œêµ­ ëŒ€ëµ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê²½ìš° ê²½ê³ \n",
        "bad = df_sites[(df_sites[\"lat\"]<33)|(df_sites[\"lat\"]>39)|(df_sites[\"lon\"]<124)|(df_sites[\"lon\"]>132)]\n",
        "if len(bad):\n",
        "    print(\"[WARN] KR bounds outliers (first 5):\")\n",
        "    print(bad[[\"pid\",\"lat\",\"lon\"]].head())\n",
        "\n",
        "# CSV ë‹¤ìš´ë¡œë“œ (Colab í™˜ê²½)\n",
        "files.download(OUT_CSV)\n",
        "\n",
        "\n",
        "\n",
        "### í•˜ë£¨ ì¼ì¡°ëŸ‰ ì¶”ì¶œ\n",
        "\n",
        "# MODIS/061/MCD18A1ì—ì„œ 3ì‹œê°„ ë‹¨ìœ„ DSR ë°´ë“œë¥¼ í•©ì‚°í•´\n",
        "# í•˜ë£¨ ëˆ„ì  ì¼ì‚¬ëŸ‰(DSR_total_MJm^2)ì„ êµ¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸\n",
        "# ì…ë ¥:\n",
        "#   - TOTAL_DWI_with_NDVI11111_filled.xlsx  (date, lon, lat í¬í•¨)\n",
        "# ì¶œë ¥:\n",
        "#   - TOTAL_DWI_with_NDVI11111_filled_with_DSR.xlsx\n",
        "# ì²˜ë¦¬:\n",
        "#   - ê° í–‰ì˜ ë‚ ì§œ/ì¢Œí‘œì— ëŒ€í•´ DSR_total_MJm^2 ê³„ì‚°\n",
        "#   - ì—ëŸ¬ ì‹œ NaN ì²˜ë¦¬, ì „ì²´ë¥¼ ì—‘ì…€ë¡œ ì €ì¥\n",
        "\n",
        "import ee, pandas as pd, numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "\n",
        "# GEE ì´ˆê¸°í™” (project ì§€ì •, ì‹¤íŒ¨ ì‹œ ì¸ì¦)\n",
        "try:\n",
        "    ee.Initialize(project='matprocject11')\n",
        "    print(\"GEE initialized (project=matprocject11).\")\n",
        "except Exception:\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize()\n",
        "    print(\"GEE authenticated & initialized.\")\n",
        "\n",
        "# íŒŒì¼ ê²½ë¡œ ë° ì»¬ëŸ¼ ì´ë¦„\n",
        "INPUT_XLSX = \"/content/TOTAL_DWI_with_NDVI11111_filled.xlsx\"\n",
        "OUTPUT_XLSX = \"/content/TOTAL_DWI_with_NDVI11111_filled_with_DSR.xlsx\"\n",
        "\n",
        "DATE_COL = \"date\"\n",
        "LON_COL  = \"lon\"\n",
        "LAT_COL  = \"lat\"\n",
        "\n",
        "# ì‚¬ìš©í•  MODIS DSR ë°´ë“œ ëª©ë¡\n",
        "MCD18A1_DSR_BANDS = [\n",
        "    'GMT_0000_DSR','GMT_0300_DSR','GMT_0600_DSR','GMT_0900_DSR',\n",
        "    'GMT_1200_DSR','GMT_1500_DSR','GMT_1800_DSR','GMT_2100_DSR'\n",
        "]\n",
        "\n",
        "def daily_dsr_total_MJm2(lon: float, lat: float, date_str: str, scale: int = 1000):\n",
        "    \"\"\"\n",
        "    íŠ¹ì • ë‚ ì§œ(UTC)ì™€ ì¢Œí‘œì—ì„œ MODIS/061/MCD18A1ì˜ 3ì‹œê°„ DSR ë°´ë“œë¥¼ í•©ì‚°í•´\n",
        "    í•˜ë£¨ ëˆ„ì  ë³µì‚¬ëŸ‰(MJ/m^2)ì„ ë°˜í™˜.\n",
        "    ì´ë¯¸ì§€ ë¶€ì¬ ë˜ëŠ” ì˜¤ë¥˜ ì‹œ np.nan ë°˜í™˜.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        d0 = ee.Date(date_str)\n",
        "        d1 = d0.advance(1, 'day')\n",
        "        pt = ee.Geometry.Point([float(lon), float(lat)])\n",
        "\n",
        "        ic = ee.ImageCollection('MODIS/061/MCD18A1').filterDate(d0, d1)\n",
        "        img = ic.first()\n",
        "        if img.getInfo() is None:\n",
        "            return np.nan\n",
        "\n",
        "        band_names = img.bandNames()\n",
        "        actual_bands = ee.List(MCD18A1_DSR_BANDS).filter(ee.Filter.inList('item', band_names))\n",
        "\n",
        "        # 3ì‹œê°„ ë‹¨ìœ„ W/m^2 ê°’ì„ í•©ì‚° í›„ J/m^2, MJ/m^2ë¡œ ë³€í™˜\n",
        "        dsr_sum_wm2 = img.select(actual_bands).reduce(ee.Reducer.sum())\n",
        "        dsr_total_Jm2 = dsr_sum_wm2.multiply(10800)\n",
        "        dsr_total_MJm2 = dsr_total_Jm2.divide(1e6)\n",
        "\n",
        "        val = dsr_total_MJm2.reduceRegion(\n",
        "            reducer=ee.Reducer.first(),\n",
        "            geometry=pt,\n",
        "            scale=scale,\n",
        "            bestEffort=True\n",
        "        ).get('sum')\n",
        "\n",
        "        # ë°´ë“œ í‚¤ ì´ë¦„ì´ ë‹¬ë¼ì§„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë³´ì™„ ë¡œì§\n",
        "        if val is None:\n",
        "            keys = dsr_total_MJm2.bandNames().getInfo()\n",
        "            if keys:\n",
        "                val = dsr_total_MJm2.reduceRegion(\n",
        "                    reducer=ee.Reducer.first(),\n",
        "                    geometry=pt,\n",
        "                    scale=scale,\n",
        "                    bestEffort=True\n",
        "                ).get(keys[0])\n",
        "\n",
        "        result = ee.Number(val).getInfo() if val is not None else np.nan\n",
        "        return float(result) if result is not None else np.nan\n",
        "\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "# ì…ë ¥ ì—‘ì…€ ì½ê¸°\n",
        "df = pd.read_excel(INPUT_XLSX)\n",
        "\n",
        "# ë‚ ì§œë¥¼ ë¬¸ìì—´(YYYY-MM-DD)ë¡œ í†µì¼\n",
        "def to_date_str(x):\n",
        "    if pd.isna(x):\n",
        "        return None\n",
        "    if isinstance(x, (datetime, pd.Timestamp)):\n",
        "        return x.strftime(\"%Y-%m-%d\")\n",
        "    s = str(x).strip()\n",
        "    return s[:10]\n",
        "\n",
        "if DATE_COL not in df.columns or LON_COL not in df.columns or LAT_COL not in df.columns:\n",
        "    raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë¨: '{DATE_COL}', '{LON_COL}', '{LAT_COL}' í•„ìš”\")\n",
        "\n",
        "df[\"_date_str\"] = df[DATE_COL].apply(to_date_str)\n",
        "\n",
        "# ê° í–‰ì— ëŒ€í•´ DSR_total_MJm^2 ê³„ì‚°\n",
        "vals = []\n",
        "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Computing DSR_total_MJm^2\"):\n",
        "    date_str = row[\"_date_str\"]\n",
        "    lon = row[LON_COL]\n",
        "    lat = row[LAT_COL]\n",
        "\n",
        "    if pd.isna(date_str) or pd.isna(lon) or pd.isna(lat):\n",
        "        vals.append(np.nan)\n",
        "        continue\n",
        "\n",
        "    v = daily_dsr_total_MJm2(float(lon), float(lat), date_str)\n",
        "    vals.append(v)\n",
        "\n",
        "df[\"DSR_total_MJm^2\"] = vals\n",
        "\n",
        "df.drop(columns=[\"_date_str\"], inplace=True)\n",
        "df.to_excel(OUTPUT_XLSX, index=False)\n",
        "print(\"Saved:\", OUTPUT_XLSX)\n",
        "\n",
        "\n",
        "\n",
        "### NDVI ì¶”ì¶œ 1ì°¨ (DWI ê¸°ë°˜ ì „ì²´ ë°ì´í„°ì…‹)\n",
        "\n",
        "# LANDSAT NDVIì™€ MODIS NDVIë¥¼ ìš°ì„ ìˆœìœ„ë¡œ ì‚¬ìš©í•´\n",
        "# ê° ë‚ ì§œÂ·ì¢Œí‘œì— ëŒ€í•´ NDVIë¥¼ ì±„ìš°ëŠ” ìŠ¤í¬ë¦½íŠ¸\n",
        "# ì…ë ¥:\n",
        "#   - TOTAL_DWI_CatForest_cleaned.xlsx\n",
        "# ì¶œë ¥:\n",
        "#   - TOTAL_DWI_with_NDVI11111.xlsx\n",
        "# íŠ¹ì§•:\n",
        "#   - LANDSAT ìš°ì„ , ì‹¤íŒ¨ ì‹œ MODIS\n",
        "#   - ì‹œê°„ ì°½ ë° ê³µê°„ ë²„í¼ í¬ê¸°ë¥¼ ì ì§„ì ìœ¼ë¡œ í™•ì¥\n",
        "#   - NDVI, ì‚¬ìš© ë‚ ì§œ, ì‚¬ìš© ì„¼ì„œ, ìœˆë„ìš° ì •ë³´, ë²„í¼ ì •ë³´ê¹Œì§€ ê¸°ë¡\n",
        "#   - ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì—‘ì…€ ì €ì¥ ê¸°ëŠ¥ í¬í•¨\n",
        "\n",
        "import ee, pandas as pd, numpy as np, os, math, traceback, time\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "\n",
        "# GEE ì´ˆê¸°í™”\n",
        "try:\n",
        "    ee.Initialize(project='matprocject11')\n",
        "    print(\"GEE initialized\")\n",
        "except Exception as e:\n",
        "    print(\"Init failed, authenticating...\", repr(e))\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize()\n",
        "    print(\"GEE authenticated & initialized\")\n",
        "\n",
        "# ê²½ë¡œ ë° ì²´í¬í¬ì¸íŠ¸ ì„¤ì •\n",
        "INPUT_PATH   = \"/content/TOTAL_DWI_CatForest_cleaned.xlsx\"\n",
        "OUTPUT_PATH  = \"/content/TOTAL_DWI_with_NDVI11111.xlsx\"\n",
        "CHECKPOINT   = \"/content/_ndvi_checkpoint.xlsx\"\n",
        "LOG_PATH     = \"/content/_ndvi_log.txt\"\n",
        "\n",
        "DATE_COL, LAT_COL, LON_COL = \"date\", \"lat\", \"lon\"\n",
        "\n",
        "# ì‚¬ìš©í•  GEE NDVI ì»¬ë ‰ì…˜\n",
        "LANDSAT_ID = \"LANDSAT/COMPOSITES/C02/T1_L2_8DAY_NDVI\"\n",
        "MODIS_ID   = \"MODIS/061/MOD13Q1\"\n",
        "BAND       = \"NDVI\"\n",
        "\n",
        "# ì‹œê°„ ì°½(ì¼ ë‹¨ìœ„)ê³¼ ê³µê°„ ë²„í¼(ë¯¸í„°) ë‹¨ê³„\n",
        "WINDOW_STEPS = [16, 24, 32, 45, 60]\n",
        "BUFFER_STEPS = [120, 300, 600, 1000]\n",
        "\n",
        "SAVE_EVERY   = 100\n",
        "PRINT_EVERY  = 50\n",
        "\n",
        "# íƒ€ê¹ƒ ë‚ ì§œì™€ ê°€ì¥ ê°€ê¹Œìš´ ì˜ìƒ í•˜ë‚˜ë¥¼ ì„ íƒ\n",
        "def _closest_image(col, target_date_str: str) -> ee.Image:\n",
        "    d0 = ee.Date(target_date_str)\n",
        "    def add_diff(img):\n",
        "        t = ee.Date(img.get('system:time_start'))\n",
        "        return img.set({\n",
        "            'date_diff': t.difference(d0, 'day').abs(),\n",
        "            'img_date': t.format('YYYY-MM-dd')\n",
        "        })\n",
        "    return ee.Image(col.map(add_diff).sort('date_diff').first())\n",
        "\n",
        "# í•˜ë‚˜ì˜ GEE NDVI ë°ì´í„°ì…‹(LANDSAT ë˜ëŠ” MODIS)ì— ëŒ€í•´\n",
        "# ì‹œê°„ ì°½ ë° ë²„í¼ë¥¼ ì ì§„ì ìœ¼ë¡œ ëŠ˜ë ¤ê°€ë©° NDVIë¥¼ ì¶”ì¶œ\n",
        "def _try_dataset(lat: float, lon: float, date_str: str,\n",
        "                 dataset_id: str, band: str, is_modis: bool):\n",
        "\n",
        "    point = ee.Geometry.Point([float(lon), float(lat)])\n",
        "    scale = 250 if dataset_id == MODIS_ID else 30\n",
        "    sfac  = 0.0001 if is_modis else 1.0\n",
        "\n",
        "    for win in WINDOW_STEPS:\n",
        "        start = ee.Date((datetime.strptime(date_str, \"%Y-%m-%d\") - timedelta(days=win)).strftime(\"%Y-%m-%d\"))\n",
        "        end   = ee.Date((datetime.strptime(date_str, \"%Y-%m-%d\") + timedelta(days=win+1)).strftime(\"%Y-%m-%d\"))\n",
        "        col = ee.ImageCollection(dataset_id).filterDate(start, end).select(band)\n",
        "        if col.size().getInfo() == 0:\n",
        "            continue\n",
        "\n",
        "        img = _closest_image(col, date_str)\n",
        "        used_date = img.get('img_date').getInfo()\n",
        "\n",
        "        # 1ì°¨: ë²„í¼ ì˜ì—­ì—ì„œ median NDVI ê³„ì‚°\n",
        "        for buf in BUFFER_STEPS:\n",
        "            geom = point.buffer(buf)\n",
        "            try:\n",
        "                d = img.reduceRegion(\n",
        "                    reducer=ee.Reducer.median(),\n",
        "                    geometry=geom, scale=scale,\n",
        "                    bestEffort=True, maxPixels=1e8\n",
        "                ).get(band).getInfo()\n",
        "            except Exception:\n",
        "                d = None\n",
        "            if d is not None:\n",
        "                v = max(-1.0, min(1.0, float(d) * sfac))\n",
        "                return v, used_date, win, buf, dataset_id\n",
        "\n",
        "        # 2ì°¨: sampleì„ ì´ìš©í•œ í¬ì¸íŠ¸ ìƒ˜í”Œë§\n",
        "        try:\n",
        "            fc = img.sample({\n",
        "                'region': point, 'scale': scale,\n",
        "                'numPixels': 1, 'geometries': False, 'tileScale': 2\n",
        "            })\n",
        "            d2 = None if fc.size().getInfo() == 0 else ee.Feature(fc.first()).get(band).getInfo()\n",
        "        except Exception:\n",
        "            d2 = None\n",
        "        if d2 is not None:\n",
        "            v2 = max(-1.0, min(1.0, float(d2) * sfac))\n",
        "            return v2, used_date, win, 0, dataset_id\n",
        "\n",
        "        # 3ì°¨: unmask + focal_meanìœ¼ë¡œ ë¹ˆ í”½ì…€ ë³´ì™„ í›„ ë‹¤ì‹œ ë²„í¼ median\n",
        "        filled = img.select(band).unmask().focal_mean(radius=150, units='meters', iterations=1)\n",
        "        for buf2 in BUFFER_STEPS:\n",
        "            geom2 = point.buffer(buf2)\n",
        "            try:\n",
        "                d3 = filled.reduceRegion(\n",
        "                    reducer=ee.Reducer.median(),\n",
        "                    geometry=geom2, scale=scale,\n",
        "                    bestEffort=True, maxPixels=1e8\n",
        "                ).get(band).getInfo()\n",
        "            except Exception:\n",
        "                d3 = None\n",
        "            if d3 is not None:\n",
        "                v3 = max(-1.0, min(1.0, float(d3) * sfac))\n",
        "                return v3, used_date, win, buf2, dataset_id\n",
        "\n",
        "    return None, None, None, None, None\n",
        "\n",
        "# LANDSAT â†’ MODIS ìˆœìœ¼ë¡œ NDVIë¥¼ ì‹œë„ í›„ ê°’ ë°˜í™˜\n",
        "def get_ndvi(lat: float, lon: float, date_str: str):\n",
        "    v, used, win, buf, src = _try_dataset(lat, lon, date_str, LANDSAT_ID, BAND, False)\n",
        "    if v is not None:\n",
        "        return v, used, os.path.basename(src), win, buf\n",
        "\n",
        "    v, used, win, buf, src = _try_dataset(lat, lon, date_str, MODIS_ID, BAND, True)\n",
        "    if v is not None:\n",
        "        return v, used, os.path.basename(src), win, buf\n",
        "    return np.nan, None, None, None, None\n",
        "\n",
        "# ì…ë ¥ ì—‘ì…€ ë¡œë“œ ë° ë‚ ì§œ ì •ê·œí™”\n",
        "df = pd.read_excel(INPUT_PATH)\n",
        "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# NDVI ê´€ë ¨ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìƒì„±\n",
        "for c in [\"NDVI\", \"NDVI_used_date\", \"NDVI_source\", \"NDVI_window\", \"NDVI_buffer\"]:\n",
        "    if c not in df.columns:\n",
        "        df[c] = np.nan if c == \"NDVI\" else None\n",
        "\n",
        "errors = []\n",
        "filled = 0\n",
        "\n",
        "# ê° í–‰ì— ëŒ€í•´ GEEì—ì„œ NDVI ì¶”ì¶œ\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    lat, lon, d = row[LAT_COL], row[LON_COL], row[DATE_COL]\n",
        "\n",
        "    if pd.isna(lat) or pd.isna(lon) or pd.isna(d):\n",
        "        df.at[i, \"NDVI\"]            = np.nan\n",
        "        df.at[i, \"NDVI_used_date\"]  = None\n",
        "        df.at[i, \"NDVI_source\"]     = None\n",
        "        df.at[i, \"NDVI_window\"]     = None\n",
        "        df.at[i, \"NDVI_buffer\"]     = None\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        ndvi, used_date, src, win, buf = get_ndvi(float(lat), float(lon), str(d))\n",
        "        df.at[i, \"NDVI\"]            = ndvi\n",
        "        df.at[i, \"NDVI_used_date\"]  = used_date\n",
        "        df.at[i, \"NDVI_source\"]     = src\n",
        "        df.at[i, \"NDVI_window\"]     = win\n",
        "        df.at[i, \"NDVI_buffer\"]     = buf\n",
        "        if not (pd.isna(ndvi) or (isinstance(ndvi, float) and math.isnan(ndvi))):\n",
        "            filled += 1\n",
        "    except Exception as e:\n",
        "        errors.append(f\"[row {i}] {type(e).__name__}: {repr(e)}\")\n",
        "        df.at[i, \"NDVI\"]            = np.nan\n",
        "        df.at[i, \"NDVI_used_date\"]  = None\n",
        "        df.at[i, \"NDVI_source\"]     = None\n",
        "        df.at[i, \"NDVI_window\"]     = None\n",
        "        df.at[i, \"NDVI_buffer\"]     = None\n",
        "\n",
        "    # ì§„í–‰ ìƒí™© ì¶œë ¥ ë° ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "    if (i + 1) % PRINT_EVERY == 0:\n",
        "        print(f\"[{i+1}/{len(df)}] filled so far: {filled}\")\n",
        "    if (i + 1) % SAVE_EVERY == 0:\n",
        "        df.to_excel(CHECKPOINT, index=False)\n",
        "        print(\"Checkpoint saved ->\", CHECKPOINT)\n",
        "\n",
        "# ì¢Œí‘œë³„ ê·¸ë£¹ ë‚´ì—ì„œ NDVI ì„ í˜• ë³´ê°„ìœ¼ë¡œ ë¹ˆ êµ¬ê°„ ì±„ìš°ê¸°\n",
        "df = df.sort_values([LAT_COL, LON_COL, DATE_COL])\n",
        "def _fill_group(g):\n",
        "    g[\"NDVI\"] = g[\"NDVI\"].astype(float)\n",
        "    g[\"NDVI\"] = g[\"NDVI\"].interpolate(method=\"linear\", limit_direction=\"both\")\n",
        "    return g\n",
        "df = df.groupby([LAT_COL, LON_COL], group_keys=False).apply(_fill_group)\n",
        "\n",
        "# ìµœì¢… ì €ì¥ ë° ì—ëŸ¬ ë¡œê·¸ ê¸°ë¡\n",
        "df.to_excel(OUTPUT_PATH, index=False)\n",
        "print(\"Saved ->\", OUTPUT_PATH, \"| rows:\", len(df), \"| filled:\", int((~pd.isna(df['NDVI'])).sum()))\n",
        "\n",
        "if errors:\n",
        "    with open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(errors))\n",
        "    print(f\"Logged {len(errors)} exceptions -> {LOG_PATH}\")\n",
        "else:\n",
        "    print(\"No exceptions.\")\n",
        "\n",
        "\n",
        "\n",
        "### NDVI ì¶”ì¶œ 2ì°¨ (0ê°’ ë³´ì™„ìš©)\n",
        "\n",
        "# NDVIê°€ 0ìœ¼ë¡œ ì±„ì›Œì§„ í–‰ì— ëŒ€í•´\n",
        "# Landsat8, Sentinel-2, MODIS, ì›”ë³„ ê¸°í›„ê°’ ìˆœìœ¼ë¡œ ì‹œë„í•´ NDVIë¥¼ ë‹¤ì‹œ ì±„ìš°ëŠ” ìŠ¤í¬ë¦½íŠ¸\n",
        "# ì…ë ¥:\n",
        "#   - TOTAL_DWI_with_NDVI11111.xlsx (ê¸°ì¡´ NDVI í¬í•¨)\n",
        "# ì¶œë ¥:\n",
        "#   - TOTAL_DWI_with_NDVI11111_filled.xlsx\n",
        "\n",
        "try:\n",
        "    ee.Initialize(project='matprocject11')\n",
        "    print(\"GEE initialized\")\n",
        "except Exception:\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize()\n",
        "    print(\"GEE authenticated & initialized\")\n",
        "\n",
        "INPUT_XLSX  = \"/content/TOTAL_DWI_with_NDVI11111.xlsx\"\n",
        "OUTPUT_XLSX = \"/content/TOTAL_DWI_with_NDVI11111_filled.xlsx\"\n",
        "\n",
        "DATE_COL = None\n",
        "LON_COL  = None\n",
        "LAT_COL  = None\n",
        "NDVI_COL = None\n",
        "\n",
        "BUF_METERS   = 120\n",
        "LS_WIN_DAYS  = 32\n",
        "S2_WIN_DAYS  = 20\n",
        "CLIM_YEARS   = 5\n",
        "S2_CLOUD_TH  = 40\n",
        "REDUCER      = ee.Reducer.mean()\n",
        "\n",
        "def _safe_date(s):\n",
        "    if pd.isna(s): return None\n",
        "    if isinstance(s, (pd.Timestamp, datetime)):\n",
        "        return pd.to_datetime(s).strftime(\"%Y-%m-%d\")\n",
        "    try:\n",
        "        return pd.to_datetime(str(s)).strftime(\"%Y-%m-%d\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _value_at(img, band, geom, scale, reducer=REDUCER):\n",
        "    d = img.select(band).reduceRegion(\n",
        "        reducer=reducer, geometry=geom, scale=scale,\n",
        "        maxPixels=1e8, bestEffort=True\n",
        "    )\n",
        "    return d.get(band)\n",
        "\n",
        "def _closest(col, target_date):\n",
        "    t = ee.Date(target_date)\n",
        "    col2 = col.map(lambda im: im.set('d', ee.Date(im.get('system:time_start')).difference(t,'day').abs()))\n",
        "    return ee.Image(col2.sort('d').first())\n",
        "\n",
        "# Landsat L2 ê¸°ë°˜ NDVI\n",
        "def _landsat_ndvi(point, date_str):\n",
        "    t  = ee.Date(date_str)\n",
        "    st = t.advance(-LS_WIN_DAYS,'day')\n",
        "    en = t.advance( LS_WIN_DAYS,'day').advance(1,'day')\n",
        "\n",
        "    col = (ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n",
        "           .merge(ee.ImageCollection('LANDSAT/LC09/C02/T1_L2'))\n",
        "           .filterDate(st, en)\n",
        "           .filterBounds(point))\n",
        "\n",
        "    def _mask_map(img):\n",
        "        qa = img.select('QA_PIXEL')\n",
        "        cloud  = qa.bitwiseAnd(1<<3).neq(0)\n",
        "        shadow = qa.bitwiseAnd(1<<4).neq(0)\n",
        "        snow   = qa.bitwiseAnd(1<<5).neq(0)\n",
        "        mask = cloud.Or(shadow).Or(snow).Not()\n",
        "        sr = img.select(['SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7']).multiply(0.0000275).add(-0.2)\n",
        "        nir = sr.select('SR_B5')\n",
        "        red = sr.select('SR_B4')\n",
        "        ndvi = nir.subtract(red).divide(nir.add(red)).rename('NDVI').updateMask(mask)\n",
        "        return ndvi.copyProperties(img, img.propertyNames())\n",
        "\n",
        "    ndvi_col = col.map(_mask_map)\n",
        "    if ndvi_col.size().getInfo() == 0:\n",
        "        return None, None\n",
        "    best = _closest(ndvi_col, t)\n",
        "    geom = point if BUF_METERS==0 else point.buffer(BUF_METERS)\n",
        "    val  = _value_at(best, 'NDVI', geom, 30)\n",
        "    return val, ee.Date(best.get('system:time_start')).format('YYYY-MM-dd')\n",
        "\n",
        "# Sentinel-2 ê¸°ë°˜ NDVI\n",
        "def _s2_ndvi(point, date_str):\n",
        "    t  = ee.Date(date_str)\n",
        "    st = t.advance(-S2_WIN_DAYS,'day')\n",
        "    en = t.advance( S2_WIN_DAYS,'day').advance(1,'day')\n",
        "\n",
        "    s2  = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "           .filterDate(st, en).filterBounds(point))\n",
        "\n",
        "    prob = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY').filterDate(st, en).filterBounds(point)\n",
        "\n",
        "    joined = ee.Join.saveFirst('cloud_prob').apply(\n",
        "        primary=s2,\n",
        "        secondary=prob,\n",
        "        condition=ee.Filter.equals(leftField='system:index', rightField='system:index')\n",
        "    )\n",
        "\n",
        "    def _map(img):\n",
        "        img = ee.Image(img)\n",
        "        cp  = ee.Image(img.get('cloud_prob'))\n",
        "        cp  = ee.Algorithms.If(cp, ee.Image(cp).select('probability'), ee.Image(0).rename('probability'))\n",
        "        cp  = ee.Image(cp)\n",
        "\n",
        "        scl = img.select('SCL')\n",
        "        cloudMask = cp.gt(S2_CLOUD_TH) \\\n",
        "            .Or(scl.eq(3)).Or(scl.eq(8)).Or(scl.eq(9)).Or(scl.eq(10)).Or(scl.eq(11)).Not()\n",
        "\n",
        "        b4 = img.select('B4').multiply(0.0001)\n",
        "        b8 = img.select('B8').multiply(0.0001)\n",
        "        ndvi = b8.subtract(b4).divide(b8.add(b4)).rename('NDVI').updateMask(cloudMask)\n",
        "        return ndvi.copyProperties(img, img.propertyNames())\n",
        "\n",
        "    ndvi_col = ee.ImageCollection(joined).map(_map)\n",
        "    if ndvi_col.size().getInfo() == 0:\n",
        "        return None, None\n",
        "    best = _closest(ndvi_col, t)\n",
        "    geom = point if BUF_METERS==0 else point.buffer(BUF_METERS)\n",
        "    val  = _value_at(best, 'NDVI', geom, 10)\n",
        "    return val, ee.Date(best.get('system:time_start')).format('YYYY-MM-dd')\n",
        "\n",
        "# MODIS ê¸°ë°˜ NDVI\n",
        "def _modis_ndvi(point, date_str):\n",
        "    t = ee.Date(date_str)\n",
        "    col = (ee.ImageCollection('MODIS/061/MOD13Q1')\n",
        "           .filterDate(t.advance(-40,'day'), t.advance(40,'day'))\n",
        "           .filterBounds(point)\n",
        "           .select('NDVI'))\n",
        "\n",
        "    if col.size().getInfo() == 0:\n",
        "        return None, None\n",
        "    best = _closest(col, t)\n",
        "    ndvi = ee.Image(best).multiply(0.0001).rename('NDVI')\n",
        "    geom = point if BUF_METERS==0 else point.buffer(BUF_METERS)\n",
        "    val  = _value_at(ndvi, 'NDVI', geom, 250)\n",
        "    return val, ee.Date(best.get('system:time_start')).format('YYYY-MM-dd')\n",
        "\n",
        "# ìµœê·¼ 5ë…„ ì›”ë³„ MODIS í´ë¼ì´ë§ˆí† ë¡œ NDVI ê·¼ì‚¬\n",
        "def _modis_monthly_clim(point, date_str):\n",
        "    t = ee.Date(date_str)\n",
        "    month = t.get('month')\n",
        "    start = t.advance(-CLIM_YEARS, 'year')\n",
        "    col = (ee.ImageCollection('MODIS/061/MOD13Q1')\n",
        "           .filterDate(start, t)\n",
        "           .filterBounds(point)\n",
        "           .filter(ee.Filter.calendarRange(month, month, 'month'))\n",
        "           .select('NDVI')\n",
        "           .map(lambda im: ee.Image(im).multiply(0.0001).rename('NDVI')))\n",
        "    if col.size().getInfo() == 0:\n",
        "        return None, None\n",
        "    clim = col.median()\n",
        "    geom = point if BUF_METERS==0 else point.buffer(BUF_METERS)\n",
        "    val  = _value_at(clim, 'NDVI', geom, 250)\n",
        "    return val, ee.String('climatology(m=').cat(ee.Number(month).format()).cat(')')\n",
        "\n",
        "# ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•˜ë©° NDVIë¥¼ ì–»ëŠ” í•¨ìˆ˜\n",
        "def get_ndvi_with_fallback(lat, lon, date_str):\n",
        "    pt = ee.Geometry.Point([float(lon), float(lat)])\n",
        "    funcs = [_landsat_ndvi, _s2_ndvi, _modis_ndvi, _modis_monthly_clim]\n",
        "    for f in funcs:\n",
        "        try:\n",
        "            val, when = f(pt, date_str)\n",
        "            if val is None:\n",
        "                continue\n",
        "            is_null = ee.Algorithms.IsEqual(val, None).getInfo()\n",
        "            if not is_null:\n",
        "                return float(ee.Number(val).getInfo())\n",
        "        except ee.EEException:\n",
        "            time.sleep(0.5)\n",
        "            continue\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "# ì…ë ¥ ì—‘ì…€ ë¡œë“œ ë° ì¹¼ëŸ¼ ìë™ íƒì§€\n",
        "df = pd.read_excel(INPUT_XLSX)\n",
        "cols_lower = {c: c.lower() for c in df.columns}\n",
        "\n",
        "def _guess(colnames, keys):\n",
        "    for c in colnames:\n",
        "        cl = c.lower()\n",
        "        if any(k in cl for k in keys):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "ndvi_col = NDVI_COL or _guess(df.columns, ['ndvi'])\n",
        "date_col = DATE_COL or _guess(df.columns, ['date','ë‚ ì§œ','datetime'])\n",
        "lon_col  = LON_COL  or _guess(df.columns, ['lon','ê²½ë„','x'])\n",
        "lat_col  = LAT_COL  or _guess(df.columns, ['lat','ìœ„ë„','y'])\n",
        "\n",
        "assert ndvi_col is not None, \"NDVI ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. NDVI_COLì— ëª…ì‹œí•˜ì„¸ìš”.\"\n",
        "assert date_col is not None, \"date ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. DATE_COLì— ëª…ì‹œí•˜ì„¸ìš”.\"\n",
        "assert lon_col  is not None and lat_col is not None, \"lon/lat ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "print(\"Detected columns ->\",\n",
        "      f\"NDVI:{ndvi_col}, date:{date_col}, lon:{lon_col}, lat:{lat_col}\")\n",
        "\n",
        "# NDVIê°€ 0ì¸ í–‰ë§Œ ëŒ€ìƒìœ¼ë¡œ GEEì—ì„œ ë‹¤ì‹œ ì¶”ì¶œ\n",
        "mask = (df[ndvi_col].fillna(0) == 0) & df[lon_col].notna() & df[lat_col].notna() & df[date_col].notna()\n",
        "rows_to_fill = df[mask].copy()\n",
        "print(f\"Rows to fill (NDVI==0): {len(rows_to_fill)} / {len(df)}\")\n",
        "\n",
        "filled_values = []\n",
        "idx_list = rows_to_fill.index.tolist()\n",
        "\n",
        "for i in tqdm(idx_list, desc=\"Filling NDVI\"):\n",
        "    lat = df.at[i, lat_col]\n",
        "    lon = df.at[i, lon_col]\n",
        "    date_str = _safe_date(df.at[i, date_col])\n",
        "    if (lat is None) or (lon is None) or (date_str is None):\n",
        "        filled_values.append((i, None))\n",
        "        continue\n",
        "    try:\n",
        "        val = get_ndvi_with_fallback(lat, lon, date_str)\n",
        "    except Exception:\n",
        "        val = None\n",
        "    filled_values.append((i, val))\n",
        "\n",
        "# NDVI ê°’ì„ ë®ì–´ì“°ê¸° (Noneì€ NaN ì²˜ë¦¬)\n",
        "for i, val in filled_values:\n",
        "    if val is not None:\n",
        "        df.at[i, ndvi_col] = val\n",
        "    else:\n",
        "        df.at[i, ndvi_col] = np.nan\n",
        "\n",
        "df.to_excel(OUTPUT_XLSX, index=False)\n",
        "print(\"Saved:\", OUTPUT_XLSX)\n",
        "\n",
        "\n",
        "\n",
        "### ê³ ì • ì¢Œí‘œ(15ê°œ) ê¸°ì¤€ NDVI ì¶”ì¶œ\n",
        "\n",
        "# 15ê°œ ê³ ì • ì¢Œí‘œì™€ ë‚ ì§œì— ëŒ€í•´\n",
        "# LANDSAT8 8ì¼ í•©ì„± NDVI â†’ MODIS MOD13Q1 ìˆœìœ¼ë¡œ NDVIë¥¼ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸\n",
        "# ì…ë ¥:\n",
        "#   - wildfire_dataset.csv (lat, lon, date í¬í•¨)\n",
        "# ì¶œë ¥:\n",
        "#   - wildfire_dataset_with_NDVI.csv\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "import ee\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# GEE ì´ˆê¸°í™”\n",
        "try:\n",
        "    ee.Initialize(project='matprocject11')\n",
        "except:\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize()\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "INPUT_CSV  = \"/content/wildfire_dataset.csv\"\n",
        "OUTPUT_CSV = \"/content/wildfire_dataset_with_NDVI.csv\"\n",
        "\n",
        "# ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "df = df.rename(columns=str.lower)\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "if \"lat\" not in df.columns or \"lon\" not in df.columns:\n",
        "    raise ValueError(\"lat, lon, date ì»¬ëŸ¼ í•„ìš”\")\n",
        "\n",
        "# GEE NDVI ì»¬ë ‰ì…˜ ì°¸ì¡°\n",
        "LAND8_ID = \"LANDSAT/COMPOSITES/C02/T1_L2_8DAY_NDVI\"\n",
        "MODIS_ID = \"MODIS/061/MOD13Q1\"\n",
        "\n",
        "LAND8_COL = ee.ImageCollection(LAND8_ID)\n",
        "MODIS_COL = ee.ImageCollection(MODIS_ID)\n",
        "\n",
        "LAND8_SCALE = 30\n",
        "MODIS_SCALE = 250\n",
        "SPATIAL_LAND8 = [120, 250]\n",
        "SPATIAL_MODIS = [250, 500]\n",
        "\n",
        "# ì§€ì •í•œ ë‚ ì§œì™€ ê°€ì¥ ê°€ê¹Œìš´ ì´ë¯¸ì§€ë¥¼ ì„ íƒ\n",
        "def closest_image(col, date_str):\n",
        "    d0 = ee.Date(date_str)\n",
        "    def add_diff(im):\n",
        "        t = ee.Date(im.get(\"system:time_start\"))\n",
        "        return im.set(\"d\", t.difference(d0, \"day\").abs())\n",
        "    return ee.Image(col.map(add_diff).sort(\"d\").first())\n",
        "\n",
        "# ë‹¨ì¼ ì¢Œí‘œì— ëŒ€í•´ NDVIë¥¼ ì¶”ì¶œ (Landsat8 â†’ MODIS ìˆœìœ¼ë¡œ ì‹œë„)\n",
        "def extract_ndvi_for_one(lat, lon, date_str):\n",
        "    point = ee.Geometry.Point([lon, lat])\n",
        "\n",
        "    # Landsat8 8ì¼ í•©ì„± NDVI\n",
        "    try:\n",
        "        img_l8 = closest_image(LAND8_COL, date_str).select(\"NDVI\")\n",
        "        for buf in SPATIAL_LAND8:\n",
        "            v = img_l8.reduceRegion(\n",
        "                reducer=ee.Reducer.median(),\n",
        "                geometry=point.buffer(buf),\n",
        "                scale=LAND8_SCALE,\n",
        "                bestEffort=True\n",
        "            ).get(\"NDVI\").getInfo()\n",
        "            if v is not None:\n",
        "                return float(v)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # MODIS NDVI\n",
        "    try:\n",
        "        img_m = closest_image(MODIS_COL, date_str).select(\"NDVI\").multiply(0.0001)\n",
        "        for buf in SPATIAL_MODIS:\n",
        "            v = img_m.reduceRegion(\n",
        "                reducer=ee.Reducer.median(),\n",
        "                geometry=point.buffer(buf),\n",
        "                scale=MODIS_SCALE,\n",
        "                bestEffort=True\n",
        "            ).get(\"NDVI\").getInfo()\n",
        "            if v is not None:\n",
        "                return float(v)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return None\n",
        "\n",
        "# ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì›Œì»¤ í•¨ìˆ˜\n",
        "def worker(row):\n",
        "    try:\n",
        "        return extract_ndvi_for_one(row[\"lat\"], row[\"lon\"], row[\"date\"])\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "rows = df.to_dict(\"records\")\n",
        "results = []\n",
        "\n",
        "print(\"Extracting NDVI ...\")\n",
        "\n",
        "# ThreadPoolExecutorë¡œ NDVI ë³‘ë ¬ ê³„ì‚°\n",
        "with ThreadPoolExecutor(max_workers=6) as ex:\n",
        "    futures = {ex.submit(worker, r): i for i, r in enumerate(rows)}\n",
        "    for f in tqdm(as_completed(futures), total=len(futures)):\n",
        "        results.append(f.result())\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥\n",
        "df[\"NDVI\"] = results\n",
        "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Saved:\", OUTPUT_CSV)\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"NDVI non-null:\", df[\"NDVI\"].notnull().sum())\n",
        "\n",
        "\n",
        "\n",
        "### ìµœì¢… ì„ ë³„ ì¢Œí‘œìš© DEM/TMI ë“± ì§€í˜• ì§€í‘œ ì¶”ì¶œ\n",
        "\n",
        "# ì…ë ¥ ì¢Œí‘œ(lat, lon)ì— ëŒ€í•´\n",
        "# SRTM DEMìœ¼ë¡œë¶€í„° ê³ ë„, ì‚¬ë©´ ë°©í–¥, ìƒëŒ€ê³ , slope_pos, TMI ë“±ì„ ê³„ì‚°í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸\n",
        "# ì…ë ¥:\n",
        "#   - CSV (lat, lon í•„ìˆ˜, ì¶”ê°€ ì»¬ëŸ¼ì€ ëª¨ë‘ ìœ ì§€)\n",
        "# ì¶œë ¥:\n",
        "#   - input_with_TMI.csv (ê¸°ì¡´ ì»¬ëŸ¼ + elev_gee, aspect_deg, rel_h, slope_pos, TMI ë“±)\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "import ee, pandas as pd, np\n",
        "\n",
        "# Colab í™˜ê²½ì—ì„œ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•œ ì¤€ë¹„\n",
        "try:\n",
        "    from google.colab import files\n",
        "except ImportError:\n",
        "    files = None\n",
        "\n",
        "# GEE ì´ˆê¸°í™”\n",
        "PROJECT_ID = \"solid-time-472606-u0\"\n",
        "\n",
        "try:\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "except Exception:\n",
        "    ee.Authenticate(project=PROJECT_ID)\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "\n",
        "print(\"[GEE] initialized\")\n",
        "\n",
        "# ì…ë ¥ CSV ë¡œë“œ (colabì´ë©´ ì—…ë¡œë“œ, ë¡œì»¬ì´ë©´ ê²½ë¡œ ì´ìš©)\n",
        "if files is not None:\n",
        "    uploaded = files.upload()\n",
        "    fname = list(uploaded.keys())[0]\n",
        "    print(f\"[INFO] ì—…ë¡œë“œëœ CSV: {fname}\")\n",
        "    df = pd.read_csv(fname)\n",
        "else:\n",
        "    df = pd.read_csv(r\"C:\\path\\to\\your\\input.csv\")\n",
        "\n",
        "# ê¸°ë³¸ í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸\n",
        "if \"lat\" not in df.columns or \"lon\" not in df.columns:\n",
        "    raise ValueError(\"ì…ë ¥ CSVì— 'lat', 'lon' ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# ë³‘í•©ìš© id(pid) ìƒì„±\n",
        "df = df.reset_index(drop=True)\n",
        "df[\"pid\"] = df.index\n",
        "\n",
        "print(\"[INPUT COLUMNS]\", df.columns.tolist())\n",
        "print(df.head())\n",
        "\n",
        "# pandas DataFrame â†’ GEE FeatureCollection(pid + geometryë§Œ ì‚¬ìš©)\n",
        "def df_to_fc_with_pid(df_in, lat_col=\"lat\", lon_col=\"lon\"):\n",
        "    feats = []\n",
        "    for _, row in df_in.iterrows():\n",
        "        lat = float(row[lat_col])\n",
        "        lon = float(row[lon_col])\n",
        "        geom = ee.Geometry.Point([lon, lat])\n",
        "        props = {\"pid\": int(row[\"pid\"])}\n",
        "        feats.append(ee.Feature(geom, props))\n",
        "    return ee.FeatureCollection(feats)\n",
        "\n",
        "fc_points = df_to_fc_with_pid(df, \"lat\", \"lon\")\n",
        "\n",
        "# DEM, Aspect, ì£¼ë³€ min/max ê³ ë„, ìƒëŒ€ê³ (rel_h) ê³„ì‚°ìš© ì´ë¯¸ì§€ ì¤€ë¹„\n",
        "dem = ee.Image(\"USGS/SRTMGL1_003\").select(\"elevation\")\n",
        "aspect = ee.Terrain.aspect(dem).rename(\"aspect_deg\")\n",
        "\n",
        "kernel = ee.Kernel.circle(radius=500, units=\"meters\")\n",
        "dem_min = dem.reduceNeighborhood(reducer=ee.Reducer.min(), kernel=kernel).rename(\"dem_min\")\n",
        "dem_max = dem.reduceNeighborhood(reducer=ee.Reducer.max(), kernel=kernel).rename(\"dem_max\")\n",
        "\n",
        "# ìƒëŒ€ê³ : (í˜„ì¬ ê³ ë„ - ì£¼ë³€ ìµœì†Œ) / (ì£¼ë³€ ìµœëŒ€ - ì£¼ë³€ ìµœì†Œ)\n",
        "rel_h = dem.subtract(dem_min).divide(dem_max.subtract(dem_min)).rename(\"rel_h\")\n",
        "\n",
        "terrain_img = dem.addBands([aspect, dem_min, dem_max, rel_h])\n",
        "\n",
        "# ì¢Œí‘œë³„ë¡œ DEM ë° ì§€í˜• ì§€í‘œë¥¼ ìƒ˜í”Œë§\n",
        "sampled = terrain_img.sampleRegions(\n",
        "    collection=fc_points,\n",
        "    scale=30,\n",
        "    geometries=False\n",
        ")\n",
        "\n",
        "sampled_dict = sampled.getInfo()\n",
        "\n",
        "rows = []\n",
        "for f in sampled_dict[\"features\"]:\n",
        "    props = f[\"properties\"]\n",
        "    rows.append(props)\n",
        "\n",
        "terrain_df = pd.DataFrame(rows)\n",
        "print(\"[TERRAIN DF COLUMNS]\", terrain_df.columns.tolist())\n",
        "print(terrain_df.head())\n",
        "\n",
        "terrain_df = terrain_df.rename(columns={\"elevation\": \"elev_gee\"})\n",
        "\n",
        "# ìƒëŒ€ê³ (rel_h)ë¥¼ ë°”íƒ•ìœ¼ë¡œ slope_pos ë²”ì£¼ ë¶„ë¥˜\n",
        "def classify_slope_pos(rel_h_val):\n",
        "    if pd.isna(rel_h_val):\n",
        "        return np.nan\n",
        "    if rel_h_val < 0.2:\n",
        "        return \"ì‚°ë¡í•˜ë¶€\"\n",
        "    elif rel_h_val < 0.4:\n",
        "        return \"ì‚°ë¡ìƒë¶€\"\n",
        "    elif rel_h_val < 0.6:\n",
        "        return \"ì‚°ë³µí•˜ë¶€\"\n",
        "    elif rel_h_val < 0.8:\n",
        "        return \"ì‚°ë³µìƒë¶€\"\n",
        "    else:\n",
        "        return \"ì‚°ì •í•˜ë¶€\"\n",
        "\n",
        "terrain_df[\"slope_pos\"] = terrain_df[\"rel_h\"].apply(classify_slope_pos)\n",
        "\n",
        "print(\"[AFTER slope_pos]\")\n",
        "print(terrain_df[[\"pid\", \"elev_gee\", \"aspect_deg\", \"rel_h\", \"slope_pos\"]].head())\n",
        "\n",
        "# ì›ë³¸ dfì™€ pid ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©\n",
        "merged = df.merge(\n",
        "    terrain_df[[\"pid\", \"elev_gee\", \"aspect_deg\", \"rel_h\", \"slope_pos\"]],\n",
        "    on=\"pid\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "print(\"[MERGED COLUMNS]\", merged.columns.tolist())\n",
        "print(merged.head())\n",
        "\n",
        "# TMI ê³„ìˆ˜ í•¨ìˆ˜ë“¤ (ì‚¬ë©´ ë°©í–¥, ìƒëŒ€ê³  ìœ„ì¹˜, ê³ ë„ êµ¬ê°„)\n",
        "def tmi_aspect(aspect_deg):\n",
        "    a = aspect_deg % 360\n",
        "    if 67.5 <= a < 112.5:\n",
        "        return 1.5\n",
        "    elif (292.5 <= a < 360) or (0 <= a < 22.5) or (247.5 <= a < 292.5):\n",
        "        return 2.5\n",
        "    elif 112.5 <= a < 202.5:\n",
        "        return 4\n",
        "    elif (22.5 <= a < 67.5) or (292.5 <= a < 337.5):\n",
        "        return 4.5\n",
        "    elif 202.5 <= a < 247.5:\n",
        "        return 5\n",
        "    return np.nan\n",
        "\n",
        "def tmi_position(pos):\n",
        "    mapping = {\n",
        "        \"ì‚°ì •í•˜ë¶€\": 0.5,\n",
        "        \"ì‚°ë³µìƒë¶€\": 0.5,\n",
        "        \"ì‚°ë³µí•˜ë¶€\": 1,\n",
        "        \"ì‚°ë¡ìƒë¶€\": 1.5,\n",
        "        \"ì‚°ë¡í•˜ë¶€\": 5\n",
        "    }\n",
        "    return mapping.get(pos, np.nan)\n",
        "\n",
        "def tmi_elevation(elev):\n",
        "    if elev >= 876:\n",
        "        return 1\n",
        "    elif 628 <= elev < 876:\n",
        "        return 2\n",
        "    elif 380 <= elev < 628:\n",
        "        return 3\n",
        "    elif 132 <= elev < 380:\n",
        "        return 4\n",
        "    elif elev < 132:\n",
        "        return 5\n",
        "    return np.nan\n",
        "\n",
        "def calc_tmi(aspect_deg, pos, elev):\n",
        "    return tmi_aspect(aspect_deg) + tmi_position(pos) + tmi_elevation(elev)\n",
        "\n",
        "# TMI êµ¬ì„± ìš”ì†Œ ë° ìµœì¢… TMI ê³„ì‚°\n",
        "merged[\"TMI_aspect\"] = merged[\"aspect_deg\"].apply(tmi_aspect)\n",
        "merged[\"TMI_pos\"]    = merged[\"slope_pos\"].apply(tmi_position)\n",
        "merged[\"TMI_elev\"]   = merged[\"elev_gee\"].apply(tmi_elevation)\n",
        "merged[\"TMI\"] = merged[\"TMI_aspect\"] + merged[\"TMI_pos\"] + merged[\"TMI_elev\"]\n",
        "\n",
        "print(merged[[\"lat\",\"lon\",\"elev_gee\",\"aspect_deg\",\"slope_pos\",\"TMI\"]].head())\n",
        "\n",
        "# ìµœì¢… CSV ì €ì¥\n",
        "OUT_PATH = \"input_with_TMI.csv\"\n",
        "merged.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"[SAVED] {OUT_PATH}\")\n",
        "\n",
        "if files is not None:\n",
        "    files.download(OUT_PATH)\n",
        "\n",
        "\n",
        "\n",
        "### ìµœì¢… ë°ì´í„°ì…‹ ê³„ì ˆë³„ FFDRI ë¶„í¬ í™•ì¸ ë° LSTM 1ì°¨ ëª¨ë¸\n",
        "\n",
        "# ìµœì¢… LSTM í•™ìŠµìš© ë°ì´í„°ì…‹ì—ì„œ\n",
        "# FFDRIì˜ ì›”ë³„ í†µê³„ì™€ ê³„ì ˆëŒ€ë³„ ë¶„í¬ë¥¼ í™•ì¸í•˜ê³ \n",
        "# ë´„/ê°€ì„ê²¨ìš¸ ê³„ì ˆì— ëŒ€í•´ LSTM 1ì°¨ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ì½”ë“œ\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# LSTMìš© ì‹œê³„ì—´ ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´ ì¶œë ¥\n",
        "df = pd.read_csv(\"/content/lstm_dataset.csv\")\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "df = df.sort_values([\"pid\", \"date\"]).reset_index(drop=True)\n",
        "\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "print(df[\"pid\"].nunique(), df[\"date\"].min(), df[\"date\"].max())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv(\"/content/lstm_dataset.csv\")\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "df[\"Month\"] = df[\"date\"].dt.month\n",
        "\n",
        "# ì›”ë³„ FFDRI ê¸°ì´ˆ í†µê³„ëŸ‰ ê³„ì‚°\n",
        "monthly_stats = df.groupby(\"Month\")[\"FFDRI\"].agg([\n",
        "    (\"mean_FFDRI\", \"mean\"),\n",
        "    (\"std_FFDRI\", \"std\"),\n",
        "    (\"min_FFDRI\", \"min\"),\n",
        "    (\"max_FFDRI\", \"max\"),\n",
        "    (\"iqr_FFDRI\", lambda x: x.quantile(0.75) - x.quantile(0.25)),\n",
        "])\n",
        "\n",
        "# Range(ìµœëŒ€-ìµœì†Œ) ì¶”ê°€\n",
        "monthly_stats[\"range_FFDRI\"] = monthly_stats[\"max_FFDRI\"] - monthly_stats[\"min_FFDRI\"]\n",
        "\n",
        "monthly_stats\n",
        "\n",
        "# ê³„ì ˆëŒ€ë³„ FFDRI KDE ë¹„êµ í”Œë¡¯\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.kdeplot(df[df[\"Month\"].isin([2,3,4])][\"FFDRI\"], label=\"ë´„(2â€“4ì›”)\", linewidth=2)\n",
        "sns.kdeplot(df[df[\"Month\"].isin([10,11,12])][\"FFDRI\"], label=\"10â€“12ì›”\", linewidth=2)\n",
        "sns.kdeplot(df[df[\"Month\"].isin([5,6,7,8,9])][\"FFDRI\"], label=\"5â€“9ì›”\", linewidth=2)\n",
        "plt.title(\"ê³„ì ˆëŒ€ë³„ FFDRI KDE ë¹„êµ\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# LSTM í•™ìŠµì„ ìœ„í•œ ê³„ì ˆ êµ¬ë¶„ ê°’ ìƒì„±\n",
        "df[\"Month\"] = df[\"date\"].dt.month\n",
        "\n",
        "def season_category(m):\n",
        "    if m in [2, 3, 4]:\n",
        "        return 1  # ë´„\n",
        "    elif m in [10, 11, 12]:\n",
        "        return 2  # ê°€ì„Â·ê²¨ìš¸\n",
        "    else:\n",
        "        return 0  # ê¸°íƒ€ ê³„ì ˆ\n",
        "\n",
        "df[\"season_cat\"] = df[\"Month\"].apply(season_category)\n",
        "df[\"season_cat\"].value_counts()\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# pidë³„ë¡œ ì—°ì†ëœ ë‚ ì§œ(seq_lenì¼)ë¥¼ ë§Œì¡±í•˜ëŠ” ì‹œê³„ì—´ ìœˆë„ìš° ìƒì„±\n",
        "def make_sequences_by_pid(df_sub, feature_cols, target_col, seq_len=14):\n",
        "    X_list, y_list = [], []\n",
        "\n",
        "    for pid, g in df_sub.groupby(\"pid\"):\n",
        "        g = g.sort_values(\"date\")\n",
        "        values = g[feature_cols + [target_col]].values\n",
        "        dates = g[\"date\"].values\n",
        "\n",
        "        if len(g) <= seq_len:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(g) - seq_len):\n",
        "            window_dates = dates[i:i+seq_len]\n",
        "            diffs = (window_dates[1:] - window_dates[:-1]).astype(\"timedelta64[D]\").astype(int)\n",
        "            if not np.all(diffs == 1):\n",
        "                continue\n",
        "\n",
        "            X_list.append(values[i:i+seq_len, :-1])\n",
        "            y_list.append(values[i+seq_len, -1])\n",
        "\n",
        "    if len(X_list) == 0:\n",
        "        return None, None\n",
        "    return np.array(X_list), np.array(y_list)\n",
        "\n",
        "# ê°€ë²¼ìš´ LSTM ëª¨ë¸ ì •ì˜\n",
        "def build_light_lstm(input_shape):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.LSTM(32),\n",
        "        layers.Dense(8, activation=\"relu\"),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(\n",
        "        loss=\"mse\",\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        metrics=[\"mae\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ì‚¬ìš©í•  í”¼ì²˜/íƒ€ê¹ƒ/ì‹œí€€ìŠ¤ ê¸¸ì´ ë° í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬ ê¸°ì¤€ì¼\n",
        "feature_cols = [\"Tmean\", \"RH\", \"WSPD\", \"TP_mm\", \"sunlight_era5\", \"NDVI\"]\n",
        "target_col = \"FFDRI\"\n",
        "seq_len = 14\n",
        "split_date = pd.Timestamp(\"2024-01-01\")\n",
        "season_list = [1, 2]\n",
        "\n",
        "quick_results = []\n",
        "\n",
        "# ë´„(1), ê°€ì„Â·ê²¨ìš¸(2) ê³„ì ˆë³„ë¡œ LSTM í•™ìŠµ ë° í‰ê°€\n",
        "for target_season in season_list:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"SEASON\", target_season)\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    df_season = df[df[\"season_cat\"] == target_season].copy()\n",
        "    df_season[\"date\"] = pd.to_datetime(df_season[\"date\"])\n",
        "    df_season = df_season.sort_values([\"pid\", \"date\"])\n",
        "\n",
        "    train_df = df_season[df_season[\"date\"] < split_date].copy()\n",
        "    test_df = df_season[df_season[\"date\"] >= split_date].copy()\n",
        "\n",
        "    if len(train_df) < seq_len + 30 or len(test_df) < seq_len + 10:\n",
        "        print(\"skip season\", target_season)\n",
        "        continue\n",
        "\n",
        "    # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§\n",
        "    scaler = StandardScaler()\n",
        "    train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
        "    test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
        "\n",
        "    # LSTM ì…ë ¥ ì‹œí€€ìŠ¤ ìƒì„±\n",
        "    X_train, y_train = make_sequences_by_pid(train_df, feature_cols, target_col, seq_len)\n",
        "    X_test, y_test = make_sequences_by_pid(test_df, feature_cols, target_col, seq_len)\n",
        "\n",
        "    if X_train is None or X_test is None:\n",
        "        print(\"sequence insufficient\")\n",
        "        continue\n",
        "\n",
        "    print(\"train:\", X_train.shape, \"test:\", X_test.shape)\n",
        "\n",
        "    model = build_light_lstm((X_train.shape[1], X_train.shape[2]))\n",
        "    es = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=12,\n",
        "        batch_size=64,\n",
        "        callbacks=[es],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    y_pred = model.predict(X_test).ravel()\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(\"MAE:\", round(mae, 3), \"R2:\", round(r2, 3))\n",
        "\n",
        "    quick_results.append({\n",
        "        \"season\": target_season,\n",
        "        \"train_seq\": len(X_train),\n",
        "        \"test_seq\": len(X_test),\n",
        "        \"MAE\": mae,\n",
        "        \"R2\": r2\n",
        "    })\n",
        "\n",
        "quick_results_df = pd.DataFrame(quick_results)\n",
        "quick_results_df\n",
        "\n",
        "\n",
        "### LSTM 2ì°¨ëª¨ë¸(DWI ê¸°ë°˜ ì˜ˆì¸¡)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬\n",
        "df = pd.read_csv(\"/content/lstm_dataset_with_type.csv\")\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "\n",
        "print(df.columns)\n",
        "print(df.head())\n",
        "\n",
        "# 2) ì›”, ê³„ì ˆ ë²”ì£¼ ìƒì„± (ë´„=1, ê°€ì„Â·ê²¨ìš¸=2, ë‚˜ë¨¸ì§€=0)\n",
        "df[\"Month\"] = df[\"date\"].dt.month\n",
        "\n",
        "def season_category(m):\n",
        "    if m in [2, 3, 4]:\n",
        "        return 1\n",
        "    elif m in [10, 11, 12]:\n",
        "        return 2\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df[\"season_cat\"] = df[\"Month\"].apply(season_category)\n",
        "\n",
        "# 3) LSTM ì…ë ¥ í”¼ì²˜ ë° íƒ€ê¹ƒ, ë³´ì¡° ë³€ìˆ˜ ì •ì˜\n",
        "#    - feature_cols: ê¸°ìƒ + DWI + ê³„ì ˆ\n",
        "#    - target_col: ë‹¤ìŒë‚  DWI\n",
        "#    - aux_cols: ì´í›„ FFDRI ê³„ì‚°ìš© FMI, TMI, day_weight\n",
        "feature_cols = [\n",
        "    \"Tmean\", \"RH\", \"WSPD\", \"TP_mm\", \"sunlight_era5\",\n",
        "    \"DWI\",\n",
        "    \"season_cat\"\n",
        "]\n",
        "\n",
        "target_col = \"DWI\"\n",
        "aux_cols   = [\"FMI\", \"TMI\", \"day_weight\"]\n",
        "\n",
        "def make_sequences(df_sub, feature_cols, target_col, aux_cols, seq_len=60):\n",
        "    \"\"\"\n",
        "    ì—¬ëŸ¬ pidë¥¼ í¬í•¨í•œ DataFrameì„ ë°›ì•„\n",
        "      X: (N, seq_len, num_features)\n",
        "      y: (N,)\n",
        "      aux: (N, len(aux_cols))\n",
        "    í˜•íƒœë¡œ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±.\n",
        "    ë‚ ì§œê°€ í•˜ë£¨ì”© ì—°ì†ì¸ êµ¬ê°„ë§Œ ì‚¬ìš©.\n",
        "    \"\"\"\n",
        "    X_list, y_list, aux_list = [], [], []\n",
        "\n",
        "    for pid, g in df_sub.groupby(\"pid\"):\n",
        "        g = g.sort_values(\"date\")\n",
        "\n",
        "        feat_vals   = g[feature_cols].values\n",
        "        target_vals = g[target_col].values\n",
        "        aux_vals    = g[aux_cols].values\n",
        "        dates       = g[\"date\"].values\n",
        "\n",
        "        if len(g) <= seq_len:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(g) - seq_len):\n",
        "            window_dates = dates[i:i+seq_len+1]\n",
        "            diffs = (window_dates[1:] - window_dates[:-1]).astype(\"timedelta64[D]\").astype(int)\n",
        "\n",
        "            # ë‚ ì§œê°€ 1ì¼ ë‹¨ìœ„ë¡œ ì—°ì†ì¸ êµ¬ê°„ë§Œ ì‚¬ìš©\n",
        "            if not np.all(diffs == 1):\n",
        "                continue\n",
        "\n",
        "            X_list.append(feat_vals[i:i+seq_len])\n",
        "            y_list.append(target_vals[i+seq_len])\n",
        "            aux_list.append(aux_vals[i+seq_len])\n",
        "\n",
        "    if len(X_list) == 0:\n",
        "        return None, None, None\n",
        "\n",
        "    return np.array(X_list), np.array(y_list), np.array(aux_list)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 4) í•™ìŠµ/í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§\n",
        "seq_len    = 60\n",
        "split_date = pd.Timestamp(\"2023-01-01\")\n",
        "\n",
        "train_df = df[df[\"date\"] < split_date].copy()\n",
        "test_df  = df[df[\"date\"] >= split_date].copy()\n",
        "\n",
        "x_scaler = StandardScaler()\n",
        "train_df[feature_cols] = x_scaler.fit_transform(train_df[feature_cols])\n",
        "test_df[feature_cols]  = x_scaler.transform(test_df[feature_cols])\n",
        "\n",
        "# 5) ì‹œí€€ìŠ¤ ìƒì„±\n",
        "X_train, y_train, aux_train = make_sequences(train_df, feature_cols, target_col, aux_cols, seq_len)\n",
        "X_test,  y_test,  aux_test  = make_sequences(test_df,  feature_cols, target_col, aux_cols, seq_len)\n",
        "\n",
        "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "def build_lstm_model(input_shape):\n",
        "    \"\"\"\n",
        "    2ì¸µ LSTM ê¸°ë°˜ DWI ì˜ˆì¸¡ ëª¨ë¸\n",
        "      ì…ë ¥: (seq_len, num_features)\n",
        "      ì¶œë ¥: ë‹¤ìŒë‚  DWI ìŠ¤ì¹¼ë¼\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.LSTM(64, return_sequences=True),\n",
        "        layers.LSTM(32),\n",
        "        layers.Dense(16, activation=\"relu\"),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"mse\",\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        metrics=[\"mae\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 6) LSTM í•™ìŠµ\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = build_lstm_model(input_shape)\n",
        "\n",
        "es = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=40,\n",
        "    batch_size=32,\n",
        "    callbacks=[es],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# 7) DWI ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€\n",
        "y_pred_dwi = model.predict(X_test).ravel()\n",
        "\n",
        "y_true_dwi      = y_test.copy()\n",
        "y_pred_dwi_true = y_pred_dwi.copy()\n",
        "\n",
        "assert len(y_true_dwi) == len(y_pred_dwi_true) == aux_test.shape[0], \\\n",
        "    f\"length mismatch: y_true={len(y_true_dwi)}, y_pred={len(y_pred_dwi_true)}, aux={aux_test.shape[0]}\"\n",
        "\n",
        "print(\"=== DWI ì˜ˆì¸¡ ì„±ëŠ¥ ===\")\n",
        "print(\"MAE (DWI):\", mean_absolute_error(y_true_dwi, y_pred_dwi_true))\n",
        "print(\"R2  (DWI):\", r2_score(y_true_dwi, y_pred_dwi_true))\n",
        "\n",
        "# 8) ì˜ˆì¸¡ DWI ê¸°ë°˜ FFDRI ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€\n",
        "FMI_test        = aux_test[:, 0]\n",
        "TMI_test        = aux_test[:, 1]\n",
        "day_weight_test = aux_test[:, 2]\n",
        "\n",
        "FFDRI_true = (y_true_dwi      + FMI_test + TMI_test) * day_weight_test\n",
        "FFDRI_pred = (y_pred_dwi_true + FMI_test + TMI_test) * day_weight_test\n",
        "\n",
        "print(\"\\n=== FFDRI ì˜ˆì¸¡ ì„±ëŠ¥ (DWI ì˜ˆì¸¡ ê¸°ë°˜) ===\")\n",
        "print(\"MAE (FFDRI):\", mean_absolute_error(FFDRI_true, FFDRI_pred))\n",
        "print(\"R2  (FFDRI):\", r2_score(FFDRI_true, FFDRI_pred))\n",
        "\n",
        "\n",
        "### ë”¥ëŸ¬ë‹ ë°ì´í„°ì…‹ í™•ë³´ (ERA5 ê¸°ìƒë³€ìˆ˜ ì¶”ì¶œ)\n",
        "\n",
        "!pip -q install earthengine-api pandas tqdm\n",
        "\n",
        "import ee, pandas as pd, datetime as dt, time\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1) GEE í”„ë¡œì íŠ¸ ë° ì…ì¶œë ¥ ê²½ë¡œ ì„¤ì •\n",
        "PROJECT_ID = \"solid-time-472606-u0\"\n",
        "CSV_PATH   = \"/content/sites_by_type.csv\"\n",
        "OUT_CSV    = \"/content/fri_inputs_by_row.csv\"\n",
        "\n",
        "# ERA5 í•´ìƒë„ ê¸°ì¤€ ì¶”ì¶œ ì„¤ì •\n",
        "SAMPLE_RADIUS_M = 9000\n",
        "SAMPLE_SCALE_M  = 9000\n",
        "TILESCALE       = 4\n",
        "\n",
        "# ë‚ ì§œ êµ¬ê°„ ì„¤ì •\n",
        "DATE_START = dt.date(2019, 1, 1)\n",
        "DATE_END   = dt.date(2024, 12, 31)\n",
        "\n",
        "# íƒ€ì„ì¡´ ê²½ê³„ ë³´ì •(ì¼ ë‹¨ìœ„)\n",
        "DATE_SHIFT_DAYS = 0\n",
        "\n",
        "# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°(ì¼ ë‹¨ìœ„)\n",
        "CHECKPOINT_EVERY = 50\n",
        "\n",
        "# getInfo ì¬ì‹œë„ ì„¤ì •\n",
        "MAX_RETRY   = 6\n",
        "BACKOFF_BASE= 1.5\n",
        "\n",
        "# 2) GEE ì´ˆê¸°í™”\n",
        "try:\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "except Exception:\n",
        "    ee.Authenticate(project=PROJECT_ID)\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "print(\"[GEE] initialized\")\n",
        "ee.data.setDeadline(120000)\n",
        "\n",
        "# 3) ì¢Œí‘œ CSV ë¡œë“œ ë˜ëŠ” ì—…ë¡œë“œ\n",
        "try:\n",
        "    _ = open(CSV_PATH, \"r\")\n",
        "except FileNotFoundError:\n",
        "    print(\"[UPLOAD] ì„ íƒì°½ì—ì„œ sites_by_type.csv ì—…ë¡œë“œ\")\n",
        "    uploaded = files.upload()\n",
        "    if \"sites_by_type.csv\" not in uploaded:\n",
        "        name = next(iter(uploaded))\n",
        "        with open(CSV_PATH, \"wb\") as f:\n",
        "            f.write(uploaded[name])\n",
        "        print(f\"[INFO] ì—…ë¡œë“œ íŒŒì¼ì„ sites_by_type.csvë¡œ ì €ì¥: {name} â†’ sites_by_type.csv\")\n",
        "    else:\n",
        "        print(\"[INFO] sites_by_type.csv ì—…ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "df_sites = pd.read_csv(CSV_PATH, encoding=\"utf-8-sig\", engine=\"python\")\n",
        "df_sites.columns = [c.strip().lower().replace(\"\\ufeff\", \"\") for c in df_sites.columns]\n",
        "\n",
        "need = {\"lat\", \"lon\"}\n",
        "if not (need <= set(df_sites.columns)):\n",
        "    raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½. í˜„ì¬ í—¤ë”: {list(df_sites.columns)} (í•„ìš”: {sorted(need)})\")\n",
        "\n",
        "df_sites[\"lon\"] = pd.to_numeric(df_sites[\"lon\"], errors=\"coerce\")\n",
        "df_sites[\"lat\"] = pd.to_numeric(df_sites[\"lat\"], errors=\"coerce\")\n",
        "df_sites = df_sites.dropna(subset=[\"lon\", \"lat\"]).drop_duplicates(subset=[\"lon\", \"lat\"]).reset_index(drop=True)\n",
        "df_sites[\"pid\"] = df_sites.index.astype(int)\n",
        "\n",
        "print(f\"[INFO] sites: {len(df_sites)} rows\")\n",
        "print(df_sites.head())\n",
        "\n",
        "# 4) ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
        "def daterange(d0, d1):\n",
        "    cur = d0\n",
        "    while cur <= d1:\n",
        "        yield cur\n",
        "        cur = cur + dt.timedelta(days=1)\n",
        "\n",
        "unique_dates = [d for d in daterange(DATE_START, DATE_END)]\n",
        "if DATE_SHIFT_DAYS != 0:\n",
        "    unique_dates = [d + dt.timedelta(days=DATE_SHIFT_DAYS) for d in unique_dates]\n",
        "unique_dates_str = [d.isoformat() for d in unique_dates]\n",
        "print(f\"[INFO] date range: {unique_dates_str[0]} ~ {unique_dates_str[-1]}  (days={len(unique_dates_str)})\")\n",
        "\n",
        "# 5) ERA5 ImageCollection ë° ì¼ë‹¨ìœ„ ë³€ìˆ˜ ë³€í™˜ í•¨ìˆ˜\n",
        "ERA5_LAND   = ee.ImageCollection(\"ECMWF/ERA5_LAND/HOURLY\")\n",
        "ERA5_GLOBAL = ee.ImageCollection(\"ECMWF/ERA5/HOURLY\")\n",
        "\n",
        "def _per_hour_to_vars(im):\n",
        "    # ì‹œê°„ë³„ ì˜¨ë„, ì´ìŠ¬ì , í’ì†, ê°•ìˆ˜ëŸ‰ì„ Tmean, RH, WSPD, TP_mmë¡œ ë³€í™˜\n",
        "    T  = im.select(\"temperature_2m\").subtract(273.15).rename(\"Tmean\")\n",
        "    Td = im.select(\"dewpoint_temperature_2m\").subtract(273.15)\n",
        "    a, b = 17.625, 243.04\n",
        "    RH = Td.expression(\n",
        "        \"100*exp(a*Td/(b+Td) - a*T/(b+T))\",\n",
        "        {\"a\": a, \"b\": b, \"Td\": Td, \"T\": T}\n",
        "    ).rename(\"RH\")\n",
        "    U = im.select(\"u_component_of_wind_10m\")\n",
        "    V = im.select(\"v_component_of_wind_10m\")\n",
        "    WSPD = U.pow(2).add(V.pow(2)).sqrt().rename(\"WSPD\")\n",
        "    TP = im.select(\"total_precipitation\").multiply(1000).rename(\"TP_mm\")\n",
        "    return T.addBands([RH, WSPD, TP])\n",
        "\n",
        "def _daily_from(ic, date_str):\n",
        "    d0 = ee.Date(date_str)\n",
        "    d1 = d0.advance(1, \"day\")\n",
        "    hourly = ic.filterDate(d0, d1).map(_per_hour_to_vars)\n",
        "    Tmean = hourly.select(\"Tmean\").mean()\n",
        "    RH    = hourly.select(\"RH\").mean()\n",
        "    WSPD  = hourly.select(\"WSPD\").mean()\n",
        "    TP    = hourly.select(\"TP_mm\").sum()\n",
        "    return Tmean.addBands([RH, WSPD, TP])\n",
        "\n",
        "def daily_era5(date_str):\n",
        "    # land ìš°ì„ , ëˆ„ë½ ì‹œ globalë¡œ ë³´ì™„\n",
        "    land  = _daily_from(ERA5_LAND,   date_str)\n",
        "    globe = _daily_from(ERA5_GLOBAL, date_str)\n",
        "    fused = land.unmask(globe)\n",
        "    return fused.set({\"date\": date_str})\n",
        "\n",
        "def safe_getInfo(ee_obj):\n",
        "    # getInfo í˜¸ì¶œ ì‹œ ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„\n",
        "    for k in range(MAX_RETRY):\n",
        "        try:\n",
        "            return ee_obj.getInfo()\n",
        "        except Exception as e:\n",
        "            wait = BACKOFF_BASE**k\n",
        "            print(f\"[WARN] getInfo retry {k+1}/{MAX_RETRY} in {wait:.2f}s -> {e}\")\n",
        "            time.sleep(wait)\n",
        "    return ee_obj.getInfo()\n",
        "\n",
        "# 6) ì´ì–´ë‹¬ë¦¬ê¸°ìš© ì²´í¬í¬ì¸íŠ¸ ë³µì›\n",
        "try:\n",
        "    df_out = pd.read_csv(OUT_CSV)\n",
        "    done_keys = set(zip(df_out[\"date\"].astype(str), df_out[\"pid\"].astype(int)))\n",
        "    rows_out = df_out.to_dict(\"records\")\n",
        "    print(f\"[RESUME] existing rows: {len(rows_out)}\")\n",
        "except Exception:\n",
        "    rows_out = []\n",
        "    done_keys = set()\n",
        "\n",
        "# 7) ë‚ ì§œ Ã— ì¢Œí‘œ ë£¨í”„ë¥¼ ëŒë©° ì¼ í‰ê·  ë³€ìˆ˜ ì¶”ì¶œ\n",
        "total_dates = len(unique_dates_str)\n",
        "print(f\"[INFO] sampling by date Ã— {len(df_sites)} sites  (dates={total_dates})\")\n",
        "\n",
        "processed_since_cp = 0\n",
        "\n",
        "for idx, d in enumerate(tqdm(unique_dates_str, desc=\"[sampling by date]\"), 1):\n",
        "    if all((d, int(pid)) in done_keys for pid in df_sites[\"pid\"].tolist()):\n",
        "        continue\n",
        "\n",
        "    fc = ee.FeatureCollection([\n",
        "        ee.Feature(\n",
        "            ee.Geometry.Point([float(r[\"lon\"]), float(r[\"lat\"])]).buffer(SAMPLE_RADIUS_M),\n",
        "            {\"pid\": int(r[\"pid\"]), \"lon\": float(r[\"lon\"]), \"lat\": float(r[\"lat\"])}\n",
        "        )\n",
        "        for _, r in df_sites.iterrows()\n",
        "    ])\n",
        "\n",
        "    img = daily_era5(d)\n",
        "\n",
        "    reduced = img.reduceRegions(\n",
        "        collection=fc,\n",
        "        reducer=ee.Reducer.mean(),\n",
        "        scale=SAMPLE_SCALE_M,\n",
        "        tileScale=TILESCALE\n",
        "    ).map(lambda f: f.set({\"date\": d}))\n",
        "\n",
        "    data  = safe_getInfo(reduced)\n",
        "    feats = data.get(\"features\", []) if data else []\n",
        "\n",
        "    for f in feats:\n",
        "        p = f.get(\"properties\", {})\n",
        "        key = (str(p.get(\"date\")), int(p.get(\"pid\")))\n",
        "        if key in done_keys:\n",
        "            continue\n",
        "        rows_out.append({\n",
        "            \"date\":  p.get(\"date\"),\n",
        "            \"pid\":   p.get(\"pid\"),\n",
        "            \"lon\":   p.get(\"lon\"),\n",
        "            \"lat\":   p.get(\"lat\"),\n",
        "            \"Tmean\": p.get(\"Tmean_mean\", p.get(\"Tmean\")),\n",
        "            \"RH\":    p.get(\"RH_mean\",    p.get(\"RH\")),\n",
        "            \"WSPD\":  p.get(\"WSPD_mean\",  p.get(\"WSPD\")),\n",
        "            \"TP_mm\": p.get(\"TP_mm_mean\", p.get(\"TP_mm\")),\n",
        "        })\n",
        "        done_keys.add(key)\n",
        "\n",
        "    processed_since_cp += 1\n",
        "    if processed_since_cp >= CHECKPOINT_EVERY or idx == total_dates:\n",
        "        tmp = pd.DataFrame(rows_out).sort_values([\"date\", \"pid\"]).reset_index(drop=True)\n",
        "        tmp.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"[CP] saved {len(tmp)} rows at {d}  ({idx}/{total_dates})\")\n",
        "        processed_since_cp = 0\n",
        "\n",
        "# 8) ìµœì¢… ì €ì¥ ë° í’ˆì§ˆ ì ê²€\n",
        "df_out = pd.DataFrame(rows_out).sort_values([\"date\", \"pid\"]).reset_index(drop=True)\n",
        "df_out.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "num_total   = len(df_out)\n",
        "num_dates   = df_out[\"date\"].nunique()\n",
        "num_sites   = df_out[\"pid\"].nunique()\n",
        "num_all_nan = df_out[[\"Tmean\", \"RH\", \"WSPD\", \"TP_mm\"]].isna().all(axis=1).sum()\n",
        "num_any_nan = df_out[[\"Tmean\", \"RH\", \"WSPD\", \"TP_mm\"]].isna().any(axis=1).sum()\n",
        "\n",
        "print(f\"[SAVED] {OUT_CSV}  rows={num_total}  (sites={num_sites}, dates={num_dates})\")\n",
        "print(f\"[QC] all-NaN rows = {num_all_nan} / any-NaN rows = {num_any_nan}\")\n",
        "\n",
        "bad = df_sites[\n",
        "    (df_sites[\"lat\"] < 33) | (df_sites[\"lat\"] > 39) |\n",
        "    (df_sites[\"lon\"] < 124) | (df_sites[\"lon\"] > 132)\n",
        "]\n",
        "if len(bad):\n",
        "    print(\"[WARN] KR bounds outliers (first 5):\")\n",
        "    print(bad[[\"pid\", \"lat\", \"lon\"]].head())\n",
        "\n",
        "files.download(OUT_CSV)\n",
        "\n",
        "\n",
        "### ì‚°ë¶ˆ í”¼í•´ê·œëª¨ ê¸°ë°˜ FFDRI_new ì˜ˆì¸¡ ì‹œë„ (RandomForest, ElasticNet)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) ì‚°ë¶ˆ í”¼í•´ê·œëª¨ ë°ì´í„° ë¡œë“œ\n",
        "DATA_PATH = \"/content/wildfire_dataset_GBDG_FFDRI.csv\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "df.head()\n",
        "\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "df = df.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "# 2) ë¡œê·¸ ìŠ¤ì¼€ì¼ íƒ€ê¹ƒ ìƒì„±\n",
        "target_col = \"í”¼í•´ë©´ì _í•©ê³„\"\n",
        "df[\"target_log\"] = np.log(df[target_col] + 0.01)\n",
        "\n",
        "# 3) ì…ë ¥ í”¼ì²˜ ì„¤ì •\n",
        "feature_cols = [\n",
        "    \"tmean\", \"rh\", \"eh\", \"wspd\", \"tp_mm\", \"rne\",\n",
        "    \"sunlight_era5\", \"NDVI\", \"pdwi\", \"dwi\",\n",
        "    \"fmi\", \"tmi\", \"ffdri\"\n",
        "]\n",
        "\n",
        "X = df[feature_cols]\n",
        "y = df[\"target_log\"]\n",
        "\n",
        "print(\"ì…ë ¥ë³€ìˆ˜:\", feature_cols)\n",
        "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 4) í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.15, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1765, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Valid:\", X_valid.shape)\n",
        "print(\"Test:\", X_test.shape)\n",
        "\n",
        "# 5) RandomForest íšŒê·€ ëª¨ë¸ í•™ìŠµ\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "def evaluate(model, X, y_true, name=\"\"):\n",
        "    \"\"\"\n",
        "    í”¼íŒ…ëœ ëª¨ë¸ê³¼ X, y_trueë¥¼ ë°›ì•„\n",
        "    ë¡œê·¸ ìŠ¤ì¼€ì¼ê³¼ ì› ìŠ¤ì¼€ì¼ì—ì„œ ì„±ëŠ¥ í‰ê°€.\n",
        "    \"\"\"\n",
        "    y_pred_log = model.predict(X)\n",
        "\n",
        "    mae_log = mean_absolute_error(y_true, y_pred_log)\n",
        "    rmse_log = np.sqrt(mean_squared_error(y_true, y_pred_log))\n",
        "    r2_log   = r2_score(y_true, y_pred_log)\n",
        "    spear_log, _ = spearmanr(y_true, y_pred_log)\n",
        "\n",
        "    y_true_raw = np.exp(y_true) - 0.01\n",
        "    y_pred_raw = np.exp(y_pred_log) - 0.01\n",
        "\n",
        "    mae = mean_absolute_error(y_true_raw, y_pred_raw)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_raw, y_pred_raw))\n",
        "    r2   = r2_score(y_true_raw, y_pred_raw)\n",
        "    spear, _ = spearmanr(y_true_raw, y_pred_raw)\n",
        "\n",
        "    print(f\"\\n=== {name} í‰ê°€ ===\")\n",
        "    print(f\"[Log]  MAE={mae_log:.4f}, RMSE={rmse_log:.4f}, R2={r2_log:.4f}, Spearman={spear_log:.4f}\")\n",
        "    print(f\"[Raw]  MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}, Spearman={spear:.4f}\")\n",
        "\n",
        "    return y_pred_raw, y_pred_log\n",
        "\n",
        "# 6) RandomForest ì„±ëŠ¥ í‰ê°€\n",
        "train_pred, _ = evaluate(rf, X_train, y_train, \"Train\")\n",
        "valid_pred, _ = evaluate(rf, X_valid, y_valid, \"Valid\")\n",
        "test_pred,  _ = evaluate(rf, X_test,  y_test,  \"Test\")\n",
        "\n",
        "# 7) ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ìœ„í—˜ë„ ì¸ë±ìŠ¤ ìƒì„±\n",
        "all_pred_log = rf.predict(X)\n",
        "all_pred_raw = np.exp(all_pred_log) - 0.01\n",
        "\n",
        "df[\"pred_area\"] = all_pred_raw\n",
        "\n",
        "min_val, max_val = df[\"pred_area\"].min(), df[\"pred_area\"].max()\n",
        "df[\"danger_index\"] = 100 * (df[\"pred_area\"] - min_val) / (max_val - min_val + 1e-9)\n",
        "\n",
        "print(df[[\"date\", target_col, \"pred_area\", \"danger_index\"]].head())\n",
        "\n",
        "plt.hist(df[\"danger_index\"], bins=30)\n",
        "plt.title(\"Danger Index Distribution (0~100)\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# 8) ElasticNet ê¸°ë°˜ ëŒ€ì•ˆ ëª¨ë¸ í•™ìŠµ\n",
        "DATA_PATH = \"/content/wildfire_dataset_GBDG_FFDRI.csv\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "df.head()\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.15, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1765, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Valid:\", X_valid.shape)\n",
        "print(\"Test:\", X_test.shape)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "X_all_scaled   = scaler.transform(X)\n",
        "\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "enet = ElasticNetCV(\n",
        "    l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "    alphas=np.logspace(-4, 2, 50),\n",
        "    cv=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "enet.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Best alpha:\", enet.alpha_)\n",
        "print(\"Best l1_ratio:\", enet.l1_ratio_)\n",
        "\n",
        "def eval_split(model, Xs, y_true, name=\"\"):\n",
        "    \"\"\"\n",
        "    ElasticNet ëª¨ë¸ì˜ ë¡œê·¸/ì› ìŠ¤ì¼€ì¼ í‰ê°€.\n",
        "    \"\"\"\n",
        "    y_pred_log = model.predict(Xs)\n",
        "\n",
        "    mae_log = mean_absolute_error(y_true, y_pred_log)\n",
        "    rmse_log = np.sqrt(mean_squared_error(y_true, y_pred_log))\n",
        "    r2_log   = r2_score(y_true, y_pred_log)\n",
        "    spear_log, _ = spearmanr(y_true, y_pred_log)\n",
        "\n",
        "    y_true_raw = np.exp(y_true) - 0.01\n",
        "    y_pred_raw = np.exp(y_pred_log) - 0.01\n",
        "\n",
        "    mae = mean_absolute_error(y_true_raw, y_pred_raw)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_raw, y_pred_raw))\n",
        "    r2   = r2_score(y_true_raw, y_pred_raw)\n",
        "    spear, _ = spearmanr(y_true_raw, y_pred_raw)\n",
        "\n",
        "    print(f\"\\n=== {name} í‰ê°€ ===\")\n",
        "    print(f\"[Log] MAE={mae_log:.4f}, RMSE={rmse_log:.4f}, R2={r2_log:.4f}, Spearman={spear_log:.4f}\")\n",
        "    print(f\"[Raw] MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}, Spearman={spear:.4f}\")\n",
        "\n",
        "eval_split(enet, X_train_scaled, y_train, \"Train\")\n",
        "eval_split(enet, X_valid_scaled, y_valid, \"Valid\")\n",
        "eval_split(enet, X_test_scaled,  y_test,  \"Test\")\n",
        "\n",
        "df[\"pred_log\"]  = enet.predict(X_all_scaled)\n",
        "df[\"pred_area\"] = np.exp(df[\"pred_log\"]) - 0.01\n",
        "\n",
        "min_v = df[\"pred_area\"].min()\n",
        "max_v = df[\"pred_area\"].max()\n",
        "df[\"danger_index\"] = 100 * (df[\"pred_area\"] - min_v) / (max_v - min_v + 1e-9)\n",
        "\n",
        "df[[\"date\", target_col, \"pred_area\", \"danger_index\"]].head()\n",
        "\n",
        "\n",
        "### FFDRI_new ì‹ ì¶”ì¶œ (íšŒê·€ê³„ìˆ˜ ê¸°ë°˜ ìƒˆ ì§€ìˆ˜ ì •ì˜)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# 1) CSV ì—…ë¡œë“œ ë° ë¡œë“œ\n",
        "CSV_PATH = None\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    CSV_PATH = list(uploaded.keys())[0]\n",
        "    print(\"ì—…ë¡œë“œí•œ íŒŒì¼ëª…:\", CSV_PATH)\n",
        "except Exception:\n",
        "    print(\"Colabì´ ì•„ë‹ˆë©´ CSV_PATHë¥¼ ì§ì ‘ ì§€ì •í•˜ì„¸ìš”.\")\n",
        "    CSV_PATH = \"lstm_dataset.csv\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "print(\"\\nCSV ì»¬ëŸ¼ ëª©ë¡:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# 2) FFDRI_new êµ¬ì„±ì— ì‚¬ìš©í•  í”¼ì²˜ì™€ íƒ€ê¹ƒ ì§€ì •\n",
        "feature_cols = [\n",
        "    \"DWI\",\n",
        "    \"FMI\",\n",
        "    \"TMI\",\n",
        "    \"sunlight_era5\",\n",
        "    \"NDVI\"\n",
        "]\n",
        "\n",
        "target_col = \"FFDRI\"\n",
        "\n",
        "missing = [c for c in feature_cols + [target_col] if c not in df.columns]\n",
        "if len(missing) > 0:\n",
        "    raise ValueError(f\"CSVì— ì—†ëŠ” ì»¬ëŸ¼ì´ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì´ë¦„ì„ CSVì— ë§ê²Œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤: {missing}\")\n",
        "\n",
        "# 3) ê²°ì¸¡ ì œê±° ë° í•™ìŠµ ë°ì´í„° êµ¬ì„±\n",
        "df_clean = df.dropna(subset=feature_cols + [target_col]).copy()\n",
        "\n",
        "X = df_clean[feature_cols].copy()\n",
        "y = df_clean[target_col].values\n",
        "\n",
        "print(f\"\\nì‚¬ìš© ë°ì´í„° ê°œìˆ˜: {len(df_clean)} í–‰\")\n",
        "\n",
        "# 4) í‘œì¤€í™” í›„ Ridge íšŒê·€ ëª¨ë¸ í•™ìŠµ\n",
        "scaler = StandardScaler()\n",
        "X_std = scaler.fit_transform(X)\n",
        "\n",
        "reg = Ridge(alpha=1.0, random_state=42)\n",
        "reg.fit(X_std, y)\n",
        "\n",
        "y_pred = reg.predict(X_std)\n",
        "r2   = r2_score(y, y_pred)\n",
        "mse  = mean_squared_error(y, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"\\nëª¨ë¸ ì„±ëŠ¥ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)\")\n",
        "print(f\"RÂ²  : {r2:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "\n",
        "# 5) í‘œì¤€í™” ê³µê°„ ê³„ìˆ˜ë¥¼ ì› ë‹¨ìœ„ ê³„ìˆ˜ë¡œ ë³€í™˜\n",
        "beta0      = reg.intercept_\n",
        "betas_std  = reg.coef_\n",
        "means      = scaler.mean_\n",
        "scales     = scaler.scale_\n",
        "\n",
        "coeff_orig     = betas_std / scales\n",
        "intercept_orig = beta0 - np.sum(betas_std * means / scales)\n",
        "\n",
        "# 6) ìµœì¢… FFDRI_new ì„ í˜•ì‹ ì¶œë ¥\n",
        "print(\"\\nì§€ì—­íŠ¹í™” FFDRI_new ê³µì‹ (ì› ë‹¨ìœ„)\")\n",
        "formula = f\"FFDRI_new = {intercept_orig:.4f}\"\n",
        "for name, c in zip(feature_cols, coeff_orig):\n",
        "    sign = \" + \" if c >= 0 else \" - \"\n",
        "    formula += f\"{sign}{abs(c):.4f} * {name}\"\n",
        "print(formula)\n",
        "\n",
        "coef_table = pd.DataFrame({\n",
        "    \"feature\": feature_cols,\n",
        "    \"coef_original_space\": coeff_orig,\n",
        "    \"coef_standard_space\": betas_std\n",
        "})\n",
        "print(\"\\nê³„ìˆ˜ ìƒì„¸í‘œ\")\n",
        "print(coef_table)\n",
        "\n",
        "print(\"\\nê°„ë‹¨ í•´ì„\")\n",
        "for name, c in zip(feature_cols, coeff_orig):\n",
        "    print(f\"{name}: ê³„ìˆ˜ {c:.4f}\")\n",
        "\n",
        "# 7) ìƒˆ FFDRI_new ê°’ ê³„ì‚° í›„ ì›ë³¸ dfì— ë³‘í•©\n",
        "ffdri_new = intercept_orig\n",
        "for name, c in zip(feature_cols, coeff_orig):\n",
        "    ffdri_new += c * df_clean[name].values\n",
        "\n",
        "df_clean[\"FFDRI_new\"] = ffdri_new\n",
        "\n",
        "df_out = df.copy()\n",
        "df_out = df_out.merge(\n",
        "    df_clean[[\"FFDRI_new\"]],\n",
        "    left_index=True,\n",
        "    right_index=True,\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "output_name = \"ffdri_with_new_index.csv\"\n",
        "df_out.to_csv(output_name, index=False)\n",
        "print(\"\\nìƒˆ ì§€ìˆ˜(FFDRI_new)ë¥¼ í¬í•¨í•œ CSV ì €ì¥ ì™„ë£Œ:\", output_name)\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_name)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "### LSTM vs GRU ëª¨ë¸ í•™ìŠµ ë° ê³„ì ˆë³„ ë¹„êµ (FFDRI_new ì˜ˆì¸¡)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# 1) FFDRI_newê°€ í¬í•¨ëœ CSV ë¡œë“œ\n",
        "if IN_COLAB:\n",
        "    print(\"FFDRI_newê°€ í¬í•¨ëœ lstm_dataset.csvë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.\")\n",
        "    uploaded = files.upload()\n",
        "    CSV_PATH = list(uploaded.keys())[0]\n",
        "else:\n",
        "    CSV_PATH = \"lstm_dataset.csv\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"ì›ë³¸ ë°ì´í„° shape:\", df.shape)\n",
        "\n",
        "if \"date\" not in df.columns:\n",
        "    raise ValueError(\"CSVì— 'date' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "\n",
        "# ì¢Œí‘œ ID ì»¬ëŸ¼ ê²°ì •\n",
        "if \"pid\" in df.columns:\n",
        "    id_col = \"pid\"\n",
        "elif \"site_id\" in df.columns:\n",
        "    id_col = \"site_id\"\n",
        "else:\n",
        "    id_col = \"pid\"\n",
        "    df[id_col] = 0\n",
        "\n",
        "# season ì»¬ëŸ¼ ì—†ìœ¼ë©´ monthì—ì„œ ìƒì„±\n",
        "if \"season\" not in df.columns:\n",
        "    df[\"month\"] = df[\"date\"].dt.month\n",
        "\n",
        "    def season_from_month(m):\n",
        "        if m in [2, 3, 4]:\n",
        "            return \"spring\"\n",
        "        elif m in [10, 11, 12]:\n",
        "            return \"fall\"\n",
        "        else:\n",
        "            return \"other\"\n",
        "\n",
        "    df[\"season\"] = df[\"month\"].apply(season_from_month)\n",
        "\n",
        "print(\"season ë¶„í¬:\")\n",
        "print(df[\"season\"].value_counts())\n",
        "\n",
        "# FFDRI_new íƒ€ê¹ƒ í™•ì¸\n",
        "TARGET_COL = \"FFDRI_new\"\n",
        "if TARGET_COL not in df.columns:\n",
        "    raise ValueError(\"CSVì— 'FFDRI_new' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# 2) ë™ì  ì…ë ¥ feature í›„ë³´ ì„¤ì • (íƒ€ê¹ƒì€ ì œì™¸)\n",
        "candidate_cols = [\n",
        "    \"Tmean\", \"RH\", \"WSPD\", \"TP_mm\",\n",
        "    \"sunlight_era5\", \"DWI\", \"FMI\", \"TMI\",\n",
        "    \"NDVI\", \"day_weight\", \"FFDRI\"\n",
        "]\n",
        "\n",
        "dynamic_features = []\n",
        "for c in candidate_cols:\n",
        "    if c in df.columns and c != TARGET_COL:\n",
        "        dynamic_features.append(c)\n",
        "\n",
        "print(\"ì‚¬ìš©í•  ì…ë ¥ feature:\", dynamic_features)\n",
        "n_features = len(dynamic_features)\n",
        "if n_features == 0:\n",
        "    raise ValueError(\"dynamic_featuresê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# 3) ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„± í•¨ìˆ˜\n",
        "LOOKBACK = 14\n",
        "HORIZON  = 7\n",
        "\n",
        "def create_sequences(sub_df, lookback, horizon,\n",
        "                     feature_cols, id_col, target_col=TARGET_COL):\n",
        "    \"\"\"\n",
        "    idë³„ë¡œ ì •ë ¬ëœ ì‹œê³„ì—´ì—ì„œ\n",
        "      X: (N, lookback, n_features)\n",
        "      y: (N, horizon)\n",
        "      meta: pid, base_date, lat, lon\n",
        "    ìƒì„±.\n",
        "    \"\"\"\n",
        "    X_list, y_list, meta_list = [], [], []\n",
        "\n",
        "    for pid, g in sub_df.groupby(id_col):\n",
        "        g = g.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "        lat_val = g[\"lat\"].iloc[0] if \"lat\" in g.columns else np.nan\n",
        "        lon_val = g[\"lon\"].iloc[0] if \"lon\" in g.columns else np.nan\n",
        "\n",
        "        feats  = g[feature_cols].values\n",
        "        target = g[target_col].values\n",
        "\n",
        "        if len(g) < lookback + horizon:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(g) - lookback - horizon + 1):\n",
        "            X_list.append(feats[i:i+lookback])\n",
        "            y_list.append(target[i+lookback:i+lookback+horizon])\n",
        "\n",
        "            base_idx  = i + lookback - 1\n",
        "            base_date = g.loc[base_idx, \"date\"]\n",
        "\n",
        "            meta_list.append({\n",
        "                \"pid\": pid,\n",
        "                \"base_date\": base_date,\n",
        "                \"lat\": lat_val,\n",
        "                \"lon\": lon_val\n",
        "            })\n",
        "\n",
        "    if not X_list:\n",
        "        X = np.empty((0, lookback, len(feature_cols)))\n",
        "        y = np.empty((0, horizon))\n",
        "        meta_df = pd.DataFrame(columns=[\"pid\", \"base_date\", \"lat\", \"lon\"])\n",
        "    else:\n",
        "        X = np.array(X_list, dtype=\"float32\")\n",
        "        y = np.array(y_list, dtype=\"float32\")\n",
        "        meta_df = pd.DataFrame(meta_list)\n",
        "\n",
        "    return X, y, meta_df\n",
        "\n",
        "# 4) ê³„ì ˆë³„ ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§\n",
        "def build_splits_for_season(df_all, season_name,\n",
        "                            feature_cols, id_col):\n",
        "    \"\"\"\n",
        "    íŠ¹ì • seasonì— ëŒ€í•´ train/val/test ë¶„í• ,\n",
        "    ì‹œí€€ìŠ¤ ìƒì„± ë° ìŠ¤ì¼€ì¼ë§ê¹Œì§€ ìˆ˜í–‰.\n",
        "    \"\"\"\n",
        "    sub = df_all[df_all[\"season\"] == season_name].copy()\n",
        "    if sub.empty:\n",
        "        print(f\"[{season_name}] season ë°ì´í„° ì—†ìŒ\")\n",
        "        return None\n",
        "\n",
        "    sub = sub.sort_values([\"date\", id_col])\n",
        "\n",
        "    train_df = sub[sub[\"date\"] < pd.Timestamp(\"2023-01-01\")]\n",
        "    val_df   = sub[\n",
        "        (sub[\"date\"] >= pd.Timestamp(\"2023-01-01\")) &\n",
        "        (sub[\"date\"] <  pd.Timestamp(\"2024-01-01\"))\n",
        "    ]\n",
        "    test_df  = sub[sub[\"date\"] >= pd.Timestamp(\"2024-01-01\")]\n",
        "\n",
        "    X_train, y_train, meta_train = create_sequences(\n",
        "        train_df, LOOKBACK, HORIZON, feature_cols, id_col\n",
        "    )\n",
        "    X_val,   y_val,   meta_val   = create_sequences(\n",
        "        val_df,   LOOKBACK, HORIZON, feature_cols, id_col\n",
        "    )\n",
        "    X_test,  y_test,  meta_test  = create_sequences(\n",
        "        test_df,  LOOKBACK, HORIZON, feature_cols, id_col\n",
        "    )\n",
        "\n",
        "    print(f\"[{season_name}] samples: train {X_train.shape[0]} / val {X_val.shape[0]} / test {X_test.shape[0]}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    n_feat = len(feature_cols)\n",
        "\n",
        "    if X_train.shape[0] > 0:\n",
        "        Xtr_2d = X_train.reshape(-1, n_feat)\n",
        "        X_train_scaled = scaler.fit_transform(Xtr_2d).reshape(X_train.shape)\n",
        "    else:\n",
        "        X_train_scaled = X_train\n",
        "\n",
        "    def transform_X(X):\n",
        "        if X.shape[0] == 0:\n",
        "            return X\n",
        "        X2d = X.reshape(-1, n_feat)\n",
        "        return scaler.transform(X2d).reshape(X.shape)\n",
        "\n",
        "    X_val_scaled  = transform_X(X_val)\n",
        "    X_test_scaled = transform_X(X_test)\n",
        "\n",
        "    return {\n",
        "        \"X_train\": X_train_scaled, \"y_train\": y_train, \"meta_train\": meta_train,\n",
        "        \"X_val\":   X_val_scaled,   \"y_val\":   y_val,   \"meta_val\":   meta_val,\n",
        "        \"X_test\":  X_test_scaled,  \"y_test\":  y_test,  \"meta_test\":  meta_test,\n",
        "        \"scaler\":  scaler\n",
        "    }\n",
        "\n",
        "# 5) LSTM/GRU ëª¨ë¸ ì •ì˜\n",
        "def build_lstm_model(timesteps, n_features, horizon):\n",
        "    model = Sequential([\n",
        "        LSTM(64, return_sequences=True, input_shape=(timesteps, n_features)),\n",
        "        LSTM(32),\n",
        "        Dropout(0.2),\n",
        "        Dense(horizon)\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss=\"mse\",\n",
        "        metrics=[\"mae\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def build_gru_model(timesteps, n_features, horizon):\n",
        "    model = Sequential([\n",
        "        GRU(64, return_sequences=True, input_shape=(timesteps, n_features)),\n",
        "        GRU(32),\n",
        "        Dropout(0.2),\n",
        "        Dense(horizon)\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss=\"mse\",\n",
        "        metrics=[\"mae\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 6) í•™ìŠµ ì´ë ¥ ì‹œê°í™”\n",
        "def plot_history(history, season, model_name):\n",
        "    h = history.history\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.plot(h[\"loss\"], label=\"train_loss\")\n",
        "    if \"val_loss\" in h:\n",
        "        plt.plot(h[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.title(f\"{season} - {model_name} Training History\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"MSE loss\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 7) í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ í‰ê°€\n",
        "def evaluate_on_test(model, X_test, y_test, season, model_name):\n",
        "    \"\"\"\n",
        "    t+1, t+1~t+7 êµ¬ê°„ì—ì„œ RMSE, MAE, ìƒê´€ê³„ìˆ˜ ê³„ì‚°.\n",
        "    \"\"\"\n",
        "    if X_test.shape[0] == 0:\n",
        "        print(f\"[{season}][{model_name}] í…ŒìŠ¤íŠ¸ì…‹ ì—†ìŒ\")\n",
        "        return {}\n",
        "\n",
        "    y_pred = model.predict(X_test, verbose=0)\n",
        "    assert y_pred.shape == y_test.shape\n",
        "\n",
        "    y_true_1 = y_test[:, 0]\n",
        "    y_pred_1 = y_pred[:, 0]\n",
        "    rmse_1 = np.sqrt(mean_squared_error(y_true_1, y_pred_1))\n",
        "    mae_1  = mean_absolute_error(y_true_1, y_pred_1)\n",
        "    corr_1 = np.corrcoef(y_true_1, y_pred_1)[0, 1]\n",
        "\n",
        "    rmse_all = np.sqrt(mean_squared_error(y_test.reshape(-1), y_pred.reshape(-1)))\n",
        "    mae_all  = mean_absolute_error(y_test.reshape(-1), y_pred.reshape(-1))\n",
        "\n",
        "    print(f\"\\n[{season}][{model_name}] í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥\")\n",
        "    print(\"  (t+1)  RMSE:\", rmse_1, \" MAE:\", mae_1, \" Corr:\", corr_1)\n",
        "    print(\"  (1~7) RMSE:\", rmse_all, \" MAE:\", mae_all)\n",
        "\n",
        "    return {\n",
        "        \"rmse_1\": rmse_1, \"mae_1\": mae_1, \"corr_1\": corr_1,\n",
        "        \"rmse_all\": rmse_all, \"mae_all\": mae_all\n",
        "    }\n",
        "\n",
        "# 8) ê³„ì ˆë³„ LSTM vs GRU ë¹„êµ ë£¨í”„\n",
        "EPOCHS = 100\n",
        "BATCH  = 64\n",
        "\n",
        "summary_rows = []\n",
        "\n",
        "for season_name in [\"spring\", \"fall\"]:\n",
        "    print(f\"\\nSeason: {season_name}\")\n",
        "\n",
        "    splits = build_splits_for_season(df, season_name, dynamic_features, id_col)\n",
        "    if splits is None:\n",
        "        continue\n",
        "\n",
        "    X_train = splits[\"X_train\"]\n",
        "    y_train = splits[\"y_train\"]\n",
        "    X_val   = splits[\"X_val\"]\n",
        "    y_val   = splits[\"y_val\"]\n",
        "    X_test  = splits[\"X_test\"]\n",
        "    y_test  = splits[\"y_test\"]\n",
        "\n",
        "    if X_train.shape[0] == 0:\n",
        "        print(f\"[{season_name}] í•™ìŠµ ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ ì—†ìŒ\")\n",
        "        continue\n",
        "\n",
        "    for model_name, builder in [(\"LSTM\", build_lstm_model), (\"GRU\", build_gru_model)]:\n",
        "        print(f\"\\n{season_name} / {model_name} í•™ìŠµ\")\n",
        "\n",
        "        model = builder(LOOKBACK, n_features, HORIZON)\n",
        "        model.summary()\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor=\"val_loss\",\n",
        "                patience=10,\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        if X_val.shape[0] > 0:\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=EPOCHS,\n",
        "                batch_size=BATCH,\n",
        "                validation_data=(X_val, y_val),\n",
        "                callbacks=callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "        else:\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=EPOCHS,\n",
        "                batch_size=BATCH,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "        plot_history(history, season_name, model_name)\n",
        "\n",
        "        h = history.history\n",
        "        train_loss = h[\"loss\"][-1]\n",
        "        train_mae  = h[\"mae\"][-1]\n",
        "        if \"val_loss\" in h:\n",
        "            val_loss = h[\"val_loss\"][-1]\n",
        "            val_mae  = h[\"val_mae\"][-1]\n",
        "        else:\n",
        "            val_loss = np.nan\n",
        "            val_mae  = np.nan\n",
        "\n",
        "        metrics_test = evaluate_on_test(model, X_test, y_test, season_name, model_name)\n",
        "\n",
        "        summary_rows.append({\n",
        "            \"season\": season_name,\n",
        "            \"model\": model_name,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"train_mae\": train_mae,\n",
        "            \"val_mae\": val_mae,\n",
        "            **metrics_test\n",
        "        })\n",
        "\n",
        "# 9) LSTM vs GRU ìš”ì•½ í…Œì´ë¸”\n",
        "if summary_rows:\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    print(\"\\nLSTM vs GRU í•™ìŠµ/ê²€ì • ë¹„êµ ìš”ì•½\")\n",
        "    print(summary_df)\n",
        "\n",
        "    if IN_COLAB:\n",
        "        summary_df.to_csv(\"lstm_gru_ffdri_new_summary.csv\",\n",
        "                          index=False, encoding=\"utf-8-sig\")\n",
        "        files.download(\"lstm_gru_ffdri_new_summary.csv\")\n",
        "else:\n",
        "    print(\"ìš”ì•½í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "\n",
        "### 2025ë…„ 4ì›” 28ì¼ ëŒ€êµ¬ í•¨ì§€ì‚° ëŒ€í˜•ì‚°ë¶ˆì¼ì ëª¨ë¸ í…ŒìŠ¤íŠ¸ (GEE ê¸°ë°˜ GRU ì˜ˆì¸¡)\n",
        "\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    files = None\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import ee\n",
        "\n",
        "# 0) GEE ì´ˆê¸°í™”\n",
        "PROJECT_ID = \"solid-time-472606-u0\"\n",
        "\n",
        "try:\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "except Exception:\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize(project=PROJECT_ID)\n",
        "\n",
        "print(\"[GEE] initialized\")\n",
        "\n",
        "# 1) ì‚¬ìš©ì ì…ë ¥ (ì˜ˆì¸¡ ë‚ ì§œ, ì¢Œí‘œ, ì‹œì¦Œ ë“±)\n",
        "FORECAST_DATE = \"2025-04-28\"\n",
        "FORECAST_DATE = datetime.strptime(FORECAST_DATE, \"%Y-%m-%d\").date()\n",
        "\n",
        "NEW_LAT = 35.9181780434288\n",
        "NEW_LON = 128.566700685493\n",
        "NEIGHBOR_KM = 5\n",
        "\n",
        "SEASON   = \"spring\"\n",
        "TIMEZONE = \"Asia/Seoul\"\n",
        "\n",
        "LOOKBACK = 14\n",
        "HORIZON  = 7\n",
        "\n",
        "OUTPUT_CSV = f\"ffdri_new_forecast_NEWPOINT_{SEASON}_{FORECAST_DATE}_7d.csv\"\n",
        "\n",
        "print(\"ì…ë ¥ ì¢Œí‘œ:\", NEW_LAT, NEW_LON)\n",
        "print(\"ì„ íƒ ì‹œì¦Œ:\", SEASON)\n",
        "print(\"ì˜ˆì¸¡ ê¸°ì¤€ì¼:\", FORECAST_DATE)\n",
        "\n",
        "# 2) lstm_dataset.csv ë¡œë“œ\n",
        "if IN_COLAB:\n",
        "    print(\"lstm_dataset.csv ì—…ë¡œë“œ\")\n",
        "    uploaded = files.upload()\n",
        "    csv_name = list(uploaded.keys())[0]\n",
        "else:\n",
        "    csv_name = \"lstm_dataset.csv\"\n",
        "\n",
        "df = pd.read_csv(csv_name, parse_dates=[\"date\"])\n",
        "df = df.sort_values([\"pid\", \"date\"]).reset_index(drop=True)\n",
        "\n",
        "# season ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì›” ê¸°ë°˜ìœ¼ë¡œ ìƒì„±\n",
        "if \"season\" not in df.columns:\n",
        "    df[\"month\"] = df[\"date\"].dt.month\n",
        "    df[\"season\"] = df[\"month\"].apply(\n",
        "        lambda m: \"spring\" if m in [2, 3, 4]\n",
        "        else (\"fall\" if m in [10, 11, 12] else \"other\")\n",
        "    )\n",
        "\n",
        "df = df[df[\"season\"] == SEASON].copy()\n",
        "if df.empty:\n",
        "    raise ValueError(f\"{SEASON} ì‹œì¦Œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "df[\"doy\"] = df[\"date\"].dt.dayofyear\n",
        "\n",
        "# 3) Feature / Target ì •ì˜\n",
        "TARGET = \"FFDRI_new\"\n",
        "if TARGET not in df.columns:\n",
        "    raise ValueError(\"FFDRI_new ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "candidate_features = [\n",
        "    \"Tmean\", \"RH\", \"WSPD\", \"TP_mm\",\n",
        "    \"sunlight_era5\", \"DWI\", \"FMI\", \"TMI\",\n",
        "    \"NDVI\", \"day_weight\", \"FFDRI\"\n",
        "]\n",
        "\n",
        "FEATURES = [c for c in candidate_features if c in df.columns and c != TARGET]\n",
        "n_features = len(FEATURES)\n",
        "\n",
        "if n_features == 0:\n",
        "    raise ValueError(\"ì‚¬ìš© ê°€ëŠ¥í•œ Featureê°€ ì—†ìŠµë‹ˆë‹¤. candidate_featuresë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "\n",
        "print(\"ì‚¬ìš© Features:\", FEATURES)\n",
        "print(\"n_features:\", n_features)\n",
        "\n",
        "# 4) GRU í•™ìŠµê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ êµ¬ì„±\n",
        "def make_sequences(sub_df):\n",
        "    X_list, y_list = [], []\n",
        "\n",
        "    for pid, g in sub_df.groupby(\"pid\"):\n",
        "        g = g.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "        X_raw = g[FEATURES].values\n",
        "        y_raw = g[TARGET].values\n",
        "\n",
        "        if len(g) < LOOKBACK + HORIZON:\n",
        "            continue\n",
        "\n",
        "        for i in range(LOOKBACK, len(g) - HORIZON + 1):\n",
        "            X_list.append(X_raw[i-LOOKBACK:i])\n",
        "            y_list.append(y_raw[i:i+HORIZON])\n",
        "\n",
        "    if len(X_list) == 0:\n",
        "        return None, None\n",
        "\n",
        "    return np.array(X_list, dtype=\"float32\"), np.array(y_list, dtype=\"float32\")\n",
        "\n",
        "X_all, y_all = make_sequences(df)\n",
        "if X_all is None:\n",
        "    raise ValueError(\"ë°ì´í„°ê°€ ë¶€ì¡±í•´ ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "N = X_all.shape[0]\n",
        "idx = int(N * 0.8)\n",
        "X_train = X_all[:idx]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_2d = X_train.reshape(-1, n_features)\n",
        "scaler.fit(X_train_2d)\n",
        "\n",
        "def transform_X(z):\n",
        "    z2 = z.reshape(-1, n_features)\n",
        "    z2 = scaler.transform(z2)\n",
        "    return z2.reshape(z.shape)\n",
        "\n",
        "# 5) ì£¼ë³€ í•™ìŠµ ì¢Œí‘œ íƒìƒ‰ (NEIGHBOR_KM ì´ë‚´ pid ì„ íƒ)\n",
        "pid_locs = df.groupby(\"pid\")[[\"lat\", \"lon\"]].first().reset_index()\n",
        "\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371.0\n",
        "    dlat = np.radians(lat2 - lat1)\n",
        "    dlon = np.radians(lon2 - lon1)\n",
        "    a = np.sin(dlat/2)**2 + np.cos(np.radians(lat1))*np.cos(np.radians(lat2))*np.sin(dlon/2)**2\n",
        "    return 2 * R * np.arcsin(np.sqrt(a))\n",
        "\n",
        "pid_locs[\"dist_km\"] = pid_locs.apply(\n",
        "    lambda r: haversine(NEW_LAT, NEW_LON, r[\"lat\"], r[\"lon\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "near = pid_locs[pid_locs[\"dist_km\"] <= NEIGHBOR_KM]\n",
        "\n",
        "if near.empty:\n",
        "    print(\"ì£¼ë³€ ì¢Œí‘œ ì—†ìŒ â†’ ì „ì²´ pid ì‚¬ìš©\")\n",
        "    near_pids = pid_locs[\"pid\"].tolist()\n",
        "else:\n",
        "    print(len(near), \"ê°œ ì¢Œí‘œ ì‚¬ìš©\")\n",
        "    near_pids = near[\"pid\"].tolist()\n",
        "\n",
        "# 6) DOY ê¸°ë°˜ ê¸°í›„ í´ë¼ì´ëª¨(í‰ë…„ê°’) ê³„ì‚° (pid, doy ê¸°ì¤€)\n",
        "clim_features = df.groupby([\"pid\", \"doy\"])[FEATURES].mean()\n",
        "\n",
        "def get_climo_vector(date_obj):\n",
        "    doy = date_obj.timetuple().tm_yday\n",
        "    rows = []\n",
        "\n",
        "    for pid in near_pids:\n",
        "        key = (pid, doy)\n",
        "        if key in clim_features.index:\n",
        "            rows.append(clim_features.loc[key].values)\n",
        "\n",
        "    if rows:\n",
        "        return np.mean(rows, axis=0).astype(\"float32\")\n",
        "    return clim_features.mean().values.astype(\"float32\")\n",
        "\n",
        "# 7) GEE ERA5-Landì—ì„œ í•˜ë£¨ í‰ê·  ê¸°ìƒìš”ì†Œ ê°€ì ¸ì˜¤ê¸°\n",
        "def fetch_weather_by_date_gee(lat, lon, date_obj):\n",
        "    start = ee.Date(date_obj.strftime(\"%Y-%m-%d\"))\n",
        "    end   = start.advance(1, \"day\")\n",
        "\n",
        "    col = ee.ImageCollection(\"ECMWF/ERA5_LAND/HOURLY\").filterDate(start, end)\n",
        "\n",
        "    img_mean = col.select([\n",
        "        \"temperature_2m\",\n",
        "        \"dewpoint_temperature_2m\",\n",
        "        \"u_component_of_wind_10m\",\n",
        "        \"v_component_of_wind_10m\"\n",
        "    ]).mean()\n",
        "\n",
        "    img_sum_tp = col.select(\"total_precipitation\").sum().rename(\"total_precipitation_sum\")\n",
        "\n",
        "    img_agg = img_mean.addBands(img_sum_tp)\n",
        "\n",
        "    pt = ee.Geometry.Point(lon, lat)\n",
        "    vals = img_agg.sample(pt, scale=9000).first().getInfo()[\"properties\"]\n",
        "\n",
        "    T_K  = vals[\"temperature_2m\"]\n",
        "    Td_K = vals[\"dewpoint_temperature_2m\"]\n",
        "    u10  = vals[\"u_component_of_wind_10m\"]\n",
        "    v10  = vals[\"v_component_of_wind_10m\"]\n",
        "    tp_m = vals[\"total_precipitation_sum\"]\n",
        "\n",
        "    T_C  = T_K  - 273.15\n",
        "    Td_C = Td_K - 273.15\n",
        "\n",
        "    wspd = float((u10**2 + v10**2) ** 0.5)\n",
        "    tp_mm = float(tp_m * 1000.0)\n",
        "\n",
        "    import math\n",
        "    def esat(T):\n",
        "        return 6.112 * math.exp((17.67*T)/(T+243.5))\n",
        "\n",
        "    e  = esat(Td_C)\n",
        "    es = esat(T_C)\n",
        "    rh = float(100.0 * e / es) if es > 0 else np.nan\n",
        "\n",
        "    result = {\n",
        "        \"T_mean\":   float(T_C),\n",
        "        \"T_max\":    float(T_C),\n",
        "        \"RH_mean\":  rh,\n",
        "        \"WSPD_mean\": wspd,\n",
        "        \"TP_sum\":   tp_mm,\n",
        "    }\n",
        "    return result\n",
        "\n",
        "# 8) DWI ê³„ì‚° í•¨ìˆ˜ (ê³„ì ˆë³„ ì‹)\n",
        "def pre_spring(T, RH, W):\n",
        "    return 1/(1+np.exp(2.706 + 0.088*T - 0.055*RH - 0.023*RH - 0.104*W))\n",
        "\n",
        "def pre_fall(T, RH, W):\n",
        "    return 1/(1+np.exp(1.099 + 0.117*T - 0.069*RH - 0.182*W))\n",
        "\n",
        "def compute_DWI(weather):\n",
        "    if SEASON == \"spring\":\n",
        "        pre = pre_spring(weather[\"T_max\"], weather[\"RH_mean\"], weather[\"WSPD_mean\"])\n",
        "    else:\n",
        "        pre = pre_fall(weather[\"T_max\"], weather[\"RH_mean\"], weather[\"WSPD_mean\"])\n",
        "\n",
        "    tp = weather[\"TP_sum\"]\n",
        "    if tp >= 10:\n",
        "        rne = 0.3\n",
        "    elif tp >= 1:\n",
        "        rne = 0.7\n",
        "    else:\n",
        "        rne = 1.0\n",
        "    return pre * rne\n",
        "\n",
        "# 9) FFDRI_new ê³„ì‚°ì‹ (íšŒê·€ ê¸°ë°˜ ê³µì‹)\n",
        "def compute_FFDRI_new(dwi, fmi, tmi, sunlight, ndvi):\n",
        "    return (\n",
        "        5.3012\n",
        "        + 6.6232*dwi\n",
        "        + 0.7657*fmi\n",
        "        + 1.0994*tmi\n",
        "        + 0.0906*sunlight\n",
        "        - 10.6796*ndvi\n",
        "    )\n",
        "\n",
        "# 10) LOOKBACK ì‹œí€€ìŠ¤ êµ¬ì„± (FORECAST_DATE ê¸°ì¤€)\n",
        "feat_index = {name: i for i, name in enumerate(FEATURES)}\n",
        "seq_rows = []\n",
        "\n",
        "for offset in range(LOOKBACK-1, 0, -1):\n",
        "    d = FORECAST_DATE - timedelta(days=offset)\n",
        "    vec = get_climo_vector(d)\n",
        "    seq_rows.append(vec)\n",
        "\n",
        "today_vec = get_climo_vector(FORECAST_DATE)\n",
        "\n",
        "weather_input = fetch_weather_by_date_gee(NEW_LAT, NEW_LON, FORECAST_DATE)\n",
        "dwi_today = compute_DWI(weather_input)\n",
        "\n",
        "if \"Tmean\" in feat_index:\n",
        "    today_vec[feat_index[\"Tmean\"]] = weather_input[\"T_mean\"]\n",
        "if \"RH\" in feat_index:\n",
        "    today_vec[feat_index[\"RH\"]] = weather_input[\"RH_mean\"]\n",
        "if \"WSPD\" in feat_index:\n",
        "    today_vec[feat_index[\"WSPD\"]] = weather_input[\"WSPD_mean\"]\n",
        "if \"TP_mm\" in feat_index:\n",
        "    today_vec[feat_index[\"TP_mm\"]] = weather_input[\"TP_sum\"]\n",
        "if \"DWI\" in feat_index:\n",
        "    today_vec[feat_index[\"DWI\"]] = dwi_today\n",
        "\n",
        "seq_rows.append(today_vec)\n",
        "\n",
        "seq_feat = np.stack(seq_rows, axis=0)\n",
        "seq_feat = seq_feat.reshape(1, LOOKBACK, n_features)\n",
        "seq_feat_s = transform_X(seq_feat)\n",
        "\n",
        "# 11) GRU ëª¨ë¸ ë¡œë“œ ë° 7ì¼ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "if IN_COLAB:\n",
        "    print(\"gru ëª¨ë¸ ì—…ë¡œë“œ (ì˜ˆ: gru_spring.h5)\")\n",
        "    uploaded_model = files.upload()\n",
        "    model_name = list(uploaded_model.keys())[0]\n",
        "else:\n",
        "    model_name = f\"gru_{SEASON}.h5\"\n",
        "\n",
        "model = load_model(model_name, compile=False)\n",
        "\n",
        "print(\"model.input_shape:\", model.input_shape)\n",
        "print(\"ì…ë ¥ seq_feat_s.shape:\", seq_feat_s.shape)\n",
        "\n",
        "y_pred = model.predict(seq_feat_s)[0]\n",
        "\n",
        "# 12) ì˜¤ëŠ˜ FFDRI_new ê³„ì‚° (ë³´ê³ ìš©)\n",
        "FMI_  = today_vec[feat_index[\"FMI\"]]           if \"FMI\" in feat_index else np.nan\n",
        "TMI_  = today_vec[feat_index[\"TMI\"]]           if \"TMI\" in feat_index else np.nan\n",
        "NDVI_ = today_vec[feat_index[\"NDVI\"]]          if \"NDVI\" in feat_index else np.nan\n",
        "SUN_  = today_vec[feat_index[\"sunlight_era5\"]] if \"sunlight_era5\" in feat_index else np.nan\n",
        "\n",
        "ff_today = compute_FFDRI_new(dwi_today, FMI_, TMI_, SUN_, NDVI_)\n",
        "\n",
        "# 13) ìœ„í—˜ë“±ê¸‰ êµ¬ê°„ ì •ì˜ (ë¶„ìœ„ìˆ˜ ê¸°ë°˜)\n",
        "q25 = df[\"FFDRI_new\"].quantile(0.25)\n",
        "q50 = df[\"FFDRI_new\"].quantile(0.50)\n",
        "q75 = df[\"FFDRI_new\"].quantile(0.75)\n",
        "q90 = df[\"FFDRI_new\"].quantile(0.90)\n",
        "\n",
        "print(\"FFDRI_new quantiles (season =\", SEASON, \")\")\n",
        "print(\"Q25:\", q25, \" Q50:\", q50, \" Q75:\", q75, \" Q90:\", q90)\n",
        "\n",
        "def risk(v):\n",
        "    if v < q25:\n",
        "        return \"low\"\n",
        "    elif v < q50:\n",
        "        return \"moderate\"\n",
        "    elif v < q75:\n",
        "        return \"high\"\n",
        "    elif v < q90:\n",
        "        return \"very_high\"\n",
        "    else:\n",
        "        return \"extreme\"\n",
        "\n",
        "# 14) ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥\n",
        "print(\"\\nì˜ˆì¸¡ ê²°ê³¼ (GEE ê¸°ë°˜)\")\n",
        "print(\"ì˜ˆì¸¡ ê¸°ì¤€ì¼:\", FORECAST_DATE)\n",
        "print(\"ì…ë ¥ ì¢Œí‘œ:\", NEW_LAT, NEW_LON)\n",
        "print(\"GEE(ERA5-Land) ì…ë ¥:\", weather_input)\n",
        "print(\"DWI:\", dwi_today)\n",
        "print(\"ì˜¤ëŠ˜ FFDRI_new ê³„ì‚°ê°’:\", ff_today)\n",
        "\n",
        "for i in range(HORIZON):\n",
        "    print(f\"D+{i+1}: {y_pred[i]:.3f} | ìœ„í—˜ë“±ê¸‰: {risk(y_pred[i]+5.5)}\")\n",
        "\n",
        "# 15) CSV ì €ì¥\n",
        "out = pd.DataFrame({\n",
        "    \"base_date\": [FORECAST_DATE],\n",
        "    \"lat\": [NEW_LAT],\n",
        "    \"lon\": [NEW_LON],\n",
        "    \"FFDRI_new_today\": [ff_today],\n",
        "})\n",
        "\n",
        "for i in range(HORIZON):\n",
        "    out[f\"FFDRI_new_model_D{i+1}\"] = [y_pred[i]]\n",
        "    out[f\"risk_D{i+1}\"] = [risk(y_pred[i])]\n",
        "\n",
        "out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    files.download(OUTPUT_CSV)\n",
        "\n",
        "\n",
        "\n",
        "### ê³„ì ˆ ë¶„ë¦¬ ì ìš© í…ŒìŠ¤íŠ¸ (Open-Meteo + GRU)\n",
        "\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    files = None\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# 0) ì‚¬ìš©ì ì…ë ¥\n",
        "NEW_LAT = 35.9181780434288\n",
        "NEW_LON = 128.566700685493\n",
        "NEIGHBOR_KM = 5\n",
        "\n",
        "SEASON   = \"spring\"\n",
        "TIMEZONE = \"Asia/Seoul\"\n",
        "\n",
        "LOOKBACK = 14\n",
        "HORIZON  = 7\n",
        "\n",
        "today = datetime.now().date()\n",
        "OUTPUT_CSV = f\"ffdri_new_forecast_NEWPOINT_{SEASON}_{today}_7d.csv\"\n",
        "\n",
        "print(\"ì…ë ¥ ì¢Œí‘œ:\", NEW_LAT, NEW_LON)\n",
        "print(\"ì„ íƒ ì‹œì¦Œ:\", SEASON)\n",
        "\n",
        "# 1) lstm_dataset.csv ë¡œë“œ\n",
        "if IN_COLAB:\n",
        "    print(\"lstm_dataset.csv ì—…ë¡œë“œ\")\n",
        "    uploaded = files.upload()\n",
        "    csv_name = list(uploaded.keys())[0]\n",
        "else:\n",
        "    csv_name = \"lstm_dataset.csv\"\n",
        "\n",
        "df = pd.read_csv(csv_name, parse_dates=[\"date\"])\n",
        "df = df.sort_values([\"pid\", \"date\"]).reset_index(drop=True)\n",
        "\n",
        "if \"season\" not in df.columns:\n",
        "    df[\"month\"] = df[\"date\"].dt.month\n",
        "    df[\"season\"] = df[\"month\"].apply(\n",
        "        lambda m: \"spring\" if m in [2, 3, 4]\n",
        "        else (\"fall\" if m in [10, 11, 12] else \"other\")\n",
        "    )\n",
        "\n",
        "df = df[df[\"season\"] == SEASON].copy()\n",
        "if df.empty:\n",
        "    raise ValueError(f\"{SEASON} ì‹œì¦Œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "df[\"doy\"] = df[\"date\"].dt.dayofyear\n",
        "\n",
        "# 2) Feature / Target ì •ì˜\n",
        "TARGET = \"FFDRI_new\"\n",
        "if TARGET not in df.columns:\n",
        "    raise ValueError(\"FFDRI_new ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤. (í•™ìŠµ ë•Œì™€ ë™ì¼ CSV)\")\n",
        "\n",
        "candidate_features = [\n",
        "    \"Tmean\", \"RH\", \"WSPD\", \"TP_mm\",\n",
        "    \"sunlight_era5\", \"DWI\", \"FMI\", \"TMI\",\n",
        "    \"NDVI\", \"day_weight\", \"FFDRI\"\n",
        "]\n",
        "\n",
        "FEATURES = [c for c in candidate_features if c in df.columns and c != TARGET]\n",
        "n_features = len(FEATURES)\n",
        "\n",
        "if n_features == 0:\n",
        "    raise ValueError(\"ì‚¬ìš© ê°€ëŠ¥í•œ Featureê°€ ì—†ìŠµë‹ˆë‹¤. candidate_featuresë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "\n",
        "print(\"ì‚¬ìš© Feature:\", FEATURES)\n",
        "print(\"Feature ê°œìˆ˜:\", n_features)\n",
        "\n",
        "# 3) GRU í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì‹œí€€ìŠ¤/ìŠ¤ì¼€ì¼ëŸ¬ ë³µì›\n",
        "def make_sequences(sub_df):\n",
        "    X_list, y_list = [], []\n",
        "\n",
        "    for pid, g in sub_df.groupby(\"pid\"):\n",
        "        g = g.sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "        X_raw = g[FEATURES].values\n",
        "        y_raw = g[TARGET].values\n",
        "\n",
        "        if len(g) < LOOKBACK + HORIZON:\n",
        "            continue\n",
        "\n",
        "        for i in range(LOOKBACK, len(g) - HORIZON + 1):\n",
        "            X_list.append(X_raw[i-LOOKBACK:i])\n",
        "            y_list.append(y_raw[i:i+HORIZON])\n",
        "\n",
        "    if len(X_list) == 0:\n",
        "        return None, None\n",
        "\n",
        "    return np.array(X_list, dtype=\"float32\"), np.array(y_list, dtype=\"float32\")\n",
        "\n",
        "X_all, y_all = make_sequences(df)\n",
        "if X_all is None:\n",
        "    raise ValueError(\"ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë°ì´í„° ê¸¸ì´ ë¶€ì¡±)\")\n",
        "\n",
        "N = X_all.shape[0]\n",
        "idx = int(N * 0.8)\n",
        "X_train = X_all[:idx]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_2d = X_train.reshape(-1, n_features)\n",
        "scaler.fit(X_train_2d)\n",
        "\n",
        "def transform_X(z):\n",
        "    z2 = z.reshape(-1, n_features)\n",
        "    z2 = scaler.transform(z2)\n",
        "    return z2.reshape(z.shape)\n",
        "\n",
        "# 4) ì£¼ë³€ í•™ìŠµ ì¢Œí‘œ íƒìƒ‰\n",
        "pid_locs = df.groupby(\"pid\")[[\"lat\", \"lon\"]].first().reset_index()\n",
        "\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371.0\n",
        "    dlat = np.radians(lat2 - lat1)\n",
        "    dlon = np.radians(lon2 - lon1)\n",
        "    a = (\n",
        "        np.sin(dlat/2)**2\n",
        "        + np.cos(np.radians(lat1))*np.cos(np.radians(lat2))\n",
        "        * np.sin(dlon/2)**2\n",
        "    )\n",
        "    return 2 * R * np.arcsin(np.sqrt(a))\n",
        "\n",
        "pid_locs[\"dist_km\"] = pid_locs.apply(\n",
        "    lambda row: haversine(NEW_LAT, NEW_LON, row[\"lat\"], row[\"lon\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "near = pid_locs[pid_locs[\"dist_km\"] <= NEIGHBOR_KM]\n",
        "\n",
        "if near.empty:\n",
        "    print(f\"ì£¼ë³€ {NEIGHBOR_KM}km ë‚´ í•™ìŠµì¢Œí‘œ ì—†ìŒ â†’ ì „ì²´ pid ì‚¬ìš©\")\n",
        "    near_pids = pid_locs[\"pid\"].tolist()\n",
        "else:\n",
        "    print(f\"{len(near)}ê°œ í•™ìŠµì¢Œí‘œê°€ {NEIGHBOR_KM}km ë‚´ ì¡´ì¬\")\n",
        "    near_pids = near[\"pid\"].tolist()\n",
        "\n",
        "# 5) DOYë³„ Feature í´ë¼ì´ëª¨\n",
        "clim_features = df.groupby([\"pid\", \"doy\"])[FEATURES].mean()\n",
        "\n",
        "def get_climo_row_for_date(d):\n",
        "    doy = d.timetuple().tm_yday\n",
        "    rows = []\n",
        "\n",
        "    for pid in near_pids:\n",
        "        key = (pid, doy)\n",
        "        if key in clim_features.index:\n",
        "            rows.append(clim_features.loc[key].values)\n",
        "\n",
        "    if rows:\n",
        "        return np.mean(rows, axis=0).astype(\"float32\")\n",
        "    return clim_features.mean().values.astype(\"float32\")\n",
        "\n",
        "# 6) Open-Meteoì—ì„œ ì˜¤ëŠ˜ ê¸°ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "def fetch_today_weather(lat, lon):\n",
        "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
        "    params = {\n",
        "        \"latitude\": lat,\n",
        "        \"longitude\": lon,\n",
        "        \"hourly\": \"temperature_2m,relative_humidity_2m,wind_speed_10m,precipitation\",\n",
        "        \"forecast_days\": 1,\n",
        "        \"timezone\": TIMEZONE,\n",
        "    }\n",
        "    res = requests.get(url, params=params)\n",
        "    res.raise_for_status()\n",
        "    data = res.json()\n",
        "\n",
        "    h = pd.DataFrame(data[\"hourly\"])\n",
        "    h[\"time\"] = pd.to_datetime(h[\"time\"])\n",
        "    h[\"date\"] = h[\"time\"].dt.date\n",
        "    today_block = h[h[\"date\"] == today]\n",
        "\n",
        "    if today_block.empty:\n",
        "        raise RuntimeError(\"Open-Meteo ê²°ê³¼ì—ì„œ ì˜¤ëŠ˜ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    return {\n",
        "        \"T_max\":   float(today_block[\"temperature_2m\"].max()),\n",
        "        \"RH_mean\": float(today_block[\"relative_humidity_2m\"].mean()),\n",
        "        \"WSPD_mean\": float(today_block[\"wind_speed_10m\"].mean()),\n",
        "        \"TP_sum\":  float(today_block[\"precipitation\"].sum())\n",
        "    }\n",
        "\n",
        "# 7) ì˜¤ëŠ˜ DWI ê³„ì‚° (ê³„ì ˆë³„ ì‹)\n",
        "def pre_spring(T, RH, W):\n",
        "    return 1/(1+np.exp(2.706 + 0.088*T - 0.055*RH - 0.023*RH - 0.104*W))\n",
        "\n",
        "def pre_fall(T, RH, W):\n",
        "    return 1/(1+np.exp(1.099 + 0.117*T - 0.069*RH - 0.182*W))\n",
        "\n",
        "def compute_DWI(weather):\n",
        "    if SEASON == \"spring\":\n",
        "        pre = pre_spring(weather[\"T_max\"], weather[\"RH_mean\"], weather[\"WSPD_mean\"])\n",
        "    else:\n",
        "        pre = pre_fall(weather[\"T_max\"], weather[\"RH_mean\"], weather[\"WSPD_mean\"])\n",
        "\n",
        "    tp = weather[\"TP_sum\"]\n",
        "    if tp >= 10:\n",
        "        rne = 0.3\n",
        "    elif tp >= 1:\n",
        "        rne = 0.7\n",
        "    else:\n",
        "        rne = 1.0\n",
        "    return pre * rne\n",
        "\n",
        "weather_today = fetch_today_weather(NEW_LAT, NEW_LON)\n",
        "dwi_today = compute_DWI(weather_today)\n",
        "\n",
        "# 8) FFDRI_new ê³„ì‚° í•¨ìˆ˜\n",
        "def compute_FFDRI_new(dwi, fmi, tmi, sunlight, ndvi):\n",
        "    return (\n",
        "        5.3012\n",
        "        + 6.6232*dwi\n",
        "        + 0.7657*fmi\n",
        "        + 1.0994*tmi\n",
        "        + 0.0906*sunlight\n",
        "        - 10.6796*ndvi\n",
        "    )\n",
        "\n",
        "# 9) ìƒˆ ì¢Œí‘œì˜ 14ì¼ì¹˜ Feature ì‹œí€€ìŠ¤ ë§Œë“¤ê¸°\n",
        "feat_index = {name: i for i, name in enumerate(FEATURES)}\n",
        "seq_rows = []\n",
        "\n",
        "for offset in range(LOOKBACK-1, 0, -1):\n",
        "    d = today - timedelta(days=offset)\n",
        "    row = get_climo_row_for_date(d)\n",
        "    seq_rows.append(row)\n",
        "\n",
        "today_feat = get_climo_row_for_date(today)\n",
        "\n",
        "if \"Tmean\" in feat_index:\n",
        "    today_feat[feat_index[\"Tmean\"]] = weather_today[\"T_max\"]\n",
        "if \"RH\" in feat_index:\n",
        "    today_feat[feat_index[\"RH\"]] = weather_today[\"RH_mean\"]\n",
        "if \"WSPD\" in feat_index:\n",
        "    today_feat[feat_index[\"WSPD\"]] = weather_today[\"WSPD_mean\"]\n",
        "if \"TP_mm\" in feat_index:\n",
        "    today_feat[feat_index[\"TP_mm\"]] = weather_today[\"TP_sum\"]\n",
        "if \"DWI\" in feat_index:\n",
        "    today_feat[feat_index[\"DWI\"]] = dwi_today\n",
        "\n",
        "seq_rows.append(today_feat)\n",
        "\n",
        "seq_feat = np.stack(seq_rows, axis=0)\n",
        "seq_feat = seq_feat.reshape(1, LOOKBACK, n_features)\n",
        "\n",
        "# 10) ì…ë ¥ Feature í‘œì¤€í™”\n",
        "seq_feat_s = transform_X(seq_feat)\n",
        "\n",
        "# 11) GRU ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡\n",
        "if IN_COLAB:\n",
        "    print(\"GRU ëª¨ë¸(.h5) ì—…ë¡œë“œ (ì˜ˆ: gru_fall.h5)\")\n",
        "    uploaded_model = files.upload()\n",
        "    model_name = list(uploaded_model.keys())[0]\n",
        "else:\n",
        "    model_name = f\"gru_{SEASON}.h5\"\n",
        "\n",
        "model = load_model(model_name, compile=False)\n",
        "print(\"model.input_shape:\", model.input_shape)\n",
        "print(\"ì…ë ¥ seq_feat_s.shape:\", seq_feat_s.shape)\n",
        "\n",
        "y_pred = model.predict(seq_feat_s)[0]\n",
        "\n",
        "# 12) ì˜¤ëŠ˜ FFDRI_new ê³„ì‚° (ë³´ê³ ìš©)\n",
        "FMI_   = today_feat[feat_index[\"FMI\"]]   if \"FMI\" in feat_index else np.nan\n",
        "TMI_   = today_feat[feat_index[\"TMI\"]]   if \"TMI\" in feat_index else np.nan\n",
        "NDVI_  = today_feat[feat_index[\"NDVI\"]]  if \"NDVI\" in feat_index else np.nan\n",
        "SUN_   = today_feat[feat_index[\"sunlight_era5\"]] if \"sunlight_era5\" in feat_index else np.nan\n",
        "\n",
        "ff_today = compute_FFDRI_new(dwi_today, FMI_, TMI_, SUN_, NDVI_)\n",
        "\n",
        "# 13) ìœ„í—˜ë“±ê¸‰ êµ¬ê°„ ì •ì˜ (ë¶„ìœ„ìˆ˜ ê¸°ë°˜)\n",
        "q25 = df[\"FFDRI_new\"].quantile(0.25)\n",
        "q50 = df[\"FFDRI_new\"].quantile(0.50)\n",
        "q75 = df[\"FFDRI_new\"].quantile(0.75)\n",
        "q90 = df[\"FFDRI_new\"].quantile(0.90)\n",
        "\n",
        "print(\"FFDRI_new quantiles (season =\", SEASON, \")\")\n",
        "print(\"Q25:\", q25, \" Q50:\", q50, \" Q75:\", q75, \" Q90:\", q90)\n",
        "\n",
        "def risk(v):\n",
        "    if v < q25:\n",
        "        return \"low\"\n",
        "    elif v < q50:\n",
        "        return \"moderate\"\n",
        "    elif v < q75:\n",
        "        return \"high\"\n",
        "    elif v < q90:\n",
        "        return \"very_high\"\n",
        "    else:\n",
        "        return \"extreme\"\n",
        "\n",
        "# 14) ê²°ê³¼ ì¶œë ¥\n",
        "print(\"\\nì˜ˆì¸¡ ê²°ê³¼ (ìƒˆ ì¢Œí‘œ)\")\n",
        "print(\"ì…ë ¥ ì¢Œí‘œ :\", NEW_LAT, NEW_LON)\n",
        "print(\"ì˜¤ëŠ˜ ë‚ ì§œ :\", today)\n",
        "print(\"ì˜¤ëŠ˜ FFDRI_new (ê³„ì‚°ê°’) :\", ff_today)\n",
        "print(\"Open-Meteo ì˜¤ëŠ˜ T/RH/WSPD/TP:\", weather_today)\n",
        "print(\"ì˜¤ëŠ˜ DWI :\", dwi_today)\n",
        "\n",
        "for h in range(HORIZON):\n",
        "    print(f\"D+{h+1} = {y_pred[h]:.3f} | ìœ„í—˜ë“±ê¸‰: {risk(y_pred[h])}\")\n",
        "\n",
        "# 15) CSV ì €ì¥\n",
        "out = pd.DataFrame({\n",
        "    \"base_date\": [today],\n",
        "    \"lat\": [NEW_LAT],\n",
        "    \"lon\": [NEW_LON],\n",
        "    \"FFDRI_new_today\": [ff_today],\n",
        "})\n",
        "\n",
        "for h in range(HORIZON):\n",
        "    out[f\"FFDRI_new_model_{h+1}d\"] = [y_pred[h]]\n",
        "    out[f\"risk_{h+1}d\"] = [risk(y_pred[h])]\n",
        "\n",
        "out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    files.download(OUTPUT_CSV)\n",
        "\n",
        "\n",
        "\n",
        "### ê²½ë¶Â·ëŒ€êµ¬ ëŒ€í‘œì‚° ìœ„í—˜ë„ ì˜ˆì¸¡ ë° ì§€ë„ ì‹œê°í™”\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "except ImportError:\n",
        "    files = None\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "\n",
        "# 0) ì‚¬ìš©ì ì„¤ì •\n",
        "SEASON   = \"fall\"\n",
        "SEQ_LEN  = 30\n",
        "HORIZON  = 7\n",
        "TARGET_H = 3\n",
        "TIMEZONE = \"Asia/Seoul\"\n",
        "NEIGHBOR_KM = 5\n",
        "\n",
        "today = datetime.now().date()\n",
        "\n",
        "SITES_INFO = [\n",
        "    {\"name\": \"Hamji-san\",       \"lat\": 35.9181780434288, \"lon\": 128.566700685493},\n",
        "    {\"name\": \"Palgong-san\",     \"lat\": 36.0225318418839, \"lon\": 128.736206404928},\n",
        "    {\"name\": \"Geumo-san\",       \"lat\": 36.0923500736404, \"lon\": 128.301361851721},\n",
        "    {\"name\": \"Juwang-san\",      \"lat\": 36.3938590686737, \"lon\": 129.142072525794},\n",
        "    {\"name\": \"Cheongnyang-san\", \"lat\": 36.8013774501963, \"lon\": 128.939160702182},\n",
        "]\n",
        "\n",
        "def get_season_from_date(d):\n",
        "    m = d.month\n",
        "    return \"spring\" if 1 <= m <= 7 else \"fall\"\n",
        "\n",
        "# 1) lstm_dataset.csv ì—…ë¡œë“œ\n",
        "print(\"\\ncsv íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.\")\n",
        "uploaded = files.upload()\n",
        "csv_name = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(csv_name, parse_dates=[\"date\"])\n",
        "df = df.sort_values([\"pid\", \"date\"])\n",
        "\n",
        "if \"season\" in df.columns and SEASON in (\"spring\", \"fall\"):\n",
        "    df = df[df[\"season\"] == SEASON]\n",
        "\n",
        "df[\"doy\"] = df[\"date\"].dt.dayofyear\n",
        "\n",
        "# 2) FFDRI_new ê³„ì‚°\n",
        "sun_cols = [c for c in df.columns if \"sun\" in c.lower()]\n",
        "ndvi_cols = [c for c in df.columns if \"ndvi\" in c.lower()]\n",
        "\n",
        "SUN_COL = sun_cols[0]\n",
        "NDVI_COL = ndvi_cols[0]\n",
        "\n",
        "def compute_FFDRI_new(dwi, fmi, tmi, sun, ndvi):\n",
        "    return (\n",
        "        5.3012\n",
        "        + 6.6232*dwi\n",
        "        + 0.7657*fmi\n",
        "        + 1.0994*tmi\n",
        "        + 0.0906*sun\n",
        "        - 10.6796*ndvi\n",
        "    )\n",
        "\n",
        "df[\"FFDRI_new\"] = compute_FFDRI_new(\n",
        "    df[\"DWI\"], df[\"FMI\"], df[\"TMI\"], df[SUN_COL], df[NDVI_COL]\n",
        ")\n",
        "\n",
        "ffdri_mean = df[\"FFDRI_new\"].mean()\n",
        "ffdri_std  = df[\"FFDRI_new\"].std()\n",
        "\n",
        "def standardize(x):\n",
        "    return (x - ffdri_mean) / ffdri_std\n",
        "\n",
        "def inv_standardize(x):\n",
        "    return x * ffdri_std + ffdri_mean\n",
        "\n",
        "# 3) GRU ëª¨ë¸ ì—…ë¡œë“œ\n",
        "print(\"\\nGRU ëª¨ë¸(.h5)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.\")\n",
        "uploaded_model = files.upload()\n",
        "model_name = list(uploaded_model.keys())[0]\n",
        "model = load_model(model_name, compile=False)\n",
        "\n",
        "# 4) Open-Meteo ê¸°ë°˜ ì˜¤ëŠ˜ ê¸°ìƒ â†’ DWI ê³„ì‚°\n",
        "def fetch_today_weather(lat, lon):\n",
        "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
        "    params = {\n",
        "        \"latitude\": lat,\n",
        "        \"longitude\": lon,\n",
        "        \"hourly\": \"temperature_2m,relative_humidity_2m,wind_speed_10m,precipitation\",\n",
        "        \"forecast_days\": 1,\n",
        "        \"timezone\": TIMEZONE,\n",
        "    }\n",
        "    data = requests.get(url, params=params).json()\n",
        "    h = pd.DataFrame(data[\"hourly\"])\n",
        "    h[\"time\"] = pd.to_datetime(h[\"time\"])\n",
        "    h[\"date\"] = h[\"time\"].dt.date\n",
        "    block = h[h[\"date\"] == today]\n",
        "    if block.empty:\n",
        "        block = h\n",
        "    return {\n",
        "        \"T_max\": block[\"temperature_2m\"].max(),\n",
        "        \"RH_mean\": block[\"relative_humidity_2m\"].mean(),\n",
        "        \"WSPD_mean\": block[\"wind_speed_10m\"].mean(),\n",
        "        \"TP_sum\": block[\"precipitation\"].sum(),\n",
        "    }\n",
        "\n",
        "def pre_spring(T, RH, EH):\n",
        "    return 1/(1+np.exp(2.706 + 0.088*T - 0.055*RH - 0.023*EH))\n",
        "\n",
        "def pre_fall(T, RH, EH):\n",
        "    return 1/(1+np.exp(1.099 + 0.117*T - 0.069*RH - 0.182*EH))\n",
        "\n",
        "def compute_DWI_from_weather(w, season):\n",
        "    if season == \"auto\":\n",
        "        season = get_season_from_date(today)\n",
        "    if season == \"spring\":\n",
        "        pre = pre_spring(w[\"T_max\"], w[\"RH_mean\"], w[\"WSPD_mean\"])\n",
        "    else:\n",
        "        pre = pre_fall(w[\"T_max\"], w[\"RH_mean\"], w[\"WSPD_mean\"])\n",
        "    tp = w[\"TP_sum\"]\n",
        "    if tp >= 10:\n",
        "        rne = 0.3\n",
        "    elif tp >= 1:\n",
        "        rne = 0.7\n",
        "    else:\n",
        "        rne = 1.0\n",
        "    return pre * rne\n",
        "\n",
        "# 5) 5ê°œ ì‚°ì— ëŒ€í•´ 7ì¼ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "pid_locs = df.groupby(\"pid\")[[\"lat\", \"lon\"]].first().reset_index()\n",
        "clim_ffdri = df.groupby([\"pid\", \"doy\"])[\"FFDRI_new\"].mean()\n",
        "\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371\n",
        "    dlat = np.radians(lat2 - lat1)\n",
        "    dlon = np.radians(lon2 - lon1)\n",
        "    a = np.sin(dlat/2)**2 + np.cos(np.radians(lat1))*np.cos(np.radians(lat2))*np.sin(dlon/2)**2\n",
        "    return 2 * R * np.arcsin(np.sqrt(a))\n",
        "\n",
        "q25 = df[\"FFDRI_new\"].quantile(0.25)\n",
        "q50 = df[\"FFDRI_new\"].quantile(0.50)\n",
        "q75 = df[\"FFDRI_new\"].quantile(0.75)\n",
        "q90 = df[\"FFDRI_new\"].quantile(0.90)\n",
        "\n",
        "def risk_level(v):\n",
        "    if v < q25:\n",
        "        return \"low\"\n",
        "    elif v < q50:\n",
        "        return \"moderate\"\n",
        "    elif v < q75:\n",
        "        return \"high\"\n",
        "    elif v < q90:\n",
        "        return \"very_high\"\n",
        "    else:\n",
        "        return \"extreme\"\n",
        "\n",
        "sites_records = []\n",
        "\n",
        "for site in SITES_INFO:\n",
        "    lat, lon = site[\"lat\"], site[\"lon\"]\n",
        "\n",
        "    pid_locs[\"dist\"] = pid_locs.apply(\n",
        "        lambda r: haversine(lat, lon, r[\"lat\"], r[\"lon\"]), axis=1\n",
        "    )\n",
        "    near = pid_locs[pid_locs[\"dist\"] <= NEIGHBOR_KM]\n",
        "    if near.empty:\n",
        "        near_pids = pid_locs[\"pid\"].tolist()\n",
        "    else:\n",
        "        near_pids = near[\"pid\"].tolist()\n",
        "\n",
        "    w = fetch_today_weather(lat, lon)\n",
        "    dwi_today = compute_DWI_from_weather(w, SEASON)\n",
        "\n",
        "    doy_today = today.timetuple().tm_yday\n",
        "    sub = df[df[\"pid\"].isin(near_pids)]\n",
        "    sub_today = sub[sub[\"doy\"] == doy_today]\n",
        "\n",
        "    if sub_today.empty:\n",
        "        FMI_  = sub[\"FMI\"].mean()\n",
        "        TMI_  = sub[\"TMI\"].mean()\n",
        "        NDVI_ = sub[NDVI_COL].mean()\n",
        "        SUN_  = sub[SUN_COL].mean()\n",
        "    else:\n",
        "        FMI_  = sub_today[\"FMI\"].mean()\n",
        "        TMI_  = sub_today[\"TMI\"].mean()\n",
        "        NDVI_ = sub_today[NDVI_COL].mean()\n",
        "        SUN_  = sub_today[SUN_COL].mean()\n",
        "\n",
        "    ff_today = compute_FFDRI_new(dwi_today, FMI_, TMI_, SUN_, NDVI_)\n",
        "\n",
        "    seq = []\n",
        "    for off in range(SEQ_LEN-1, 0, -1):\n",
        "        d = today - timedelta(days=off)\n",
        "        doy = d.timetuple().tm_yday\n",
        "        vals = []\n",
        "        for pid in near_pids:\n",
        "            key = (pid, doy)\n",
        "            if key in clim_ffdri.index:\n",
        "                vals.append(float(clim_ffdri.loc[key]))\n",
        "        seq.append(np.mean(vals) if vals else clim_ffdri.mean())\n",
        "    seq.append(ff_today)\n",
        "\n",
        "    seq_std = standardize(np.array(seq)).reshape(1, SEQ_LEN, 1)\n",
        "    y_std = model.predict(seq_std, verbose=0)\n",
        "    y_pred = inv_standardize(y_std)[0]\n",
        "\n",
        "    rec = {\"name\": site[\"name\"], \"lat\": lat, \"lon\": lon, \"FFDRI_today\": ff_today}\n",
        "    for i in range(HORIZON):\n",
        "        rec[f\"FFDRI_D{i+1}\"] = float(y_pred[i])\n",
        "        rec[f\"risk_D{i+1}\"] = risk_level(float(y_pred[i]))\n",
        "    sites_records.append(rec)\n",
        "\n",
        "sites_df = pd.DataFrame(sites_records)\n",
        "\n",
        "# 6) D+TARGET_H ìœ„í—˜ë„ ì¸ë±ìŠ¤\n",
        "risk_order = [\"low\", \"moderate\", \"high\", \"very_high\", \"extreme\"]\n",
        "risk_to_idx = {r: i+1 for i, r in enumerate(risk_order)}\n",
        "\n",
        "sites_df[\"risk_index\"] = sites_df[f\"risk_D{TARGET_H}\"].map(risk_to_idx)\n",
        "\n",
        "# 7) í–‰ì •ê²½ê³„ Shapefile ì—…ë¡œë“œ ë° ì¢Œí‘œê³„ ì„¤ì •\n",
        "print(\"\\nShapefile (.shp, .shx, .dbf, .prj)ì„ ëª¨ë‘ ì—…ë¡œë“œí•˜ì„¸ìš”.\")\n",
        "uploaded_shp = files.upload()\n",
        "\n",
        "shp_files = [n for n in uploaded_shp.keys() if n.lower().endswith(\".shp\")]\n",
        "boundary_shp = shp_files[0]\n",
        "\n",
        "os.environ[\"SHAPE_RESTORE_SHX\"] = \"YES\"\n",
        "boundary = gpd.read_file(boundary_shp)\n",
        "\n",
        "boundary = boundary.set_crs(epsg=5179, allow_override=True)\n",
        "boundary_4326 = boundary.to_crs(epsg=4326)\n",
        "\n",
        "b = boundary_4326.geometry.bounds\n",
        "boundary_4326 = boundary_4326[b[\"maxx\"] < 131]\n",
        "\n",
        "g_sites = gpd.GeoDataFrame(\n",
        "    sites_df,\n",
        "    geometry=[Point(xy) for xy in zip(sites_df[\"lon\"], sites_df[\"lat\"])],\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "# 8) ì§€ë„ ì‹œê°í™” (ì˜ë¬¸ íƒ€ì´í‹€)\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "boundary_4326.plot(ax=ax, color=\"white\", edgecolor=\"black\")\n",
        "\n",
        "risk_colors = [\n",
        "    \"#FFCCCC\",\n",
        "    \"#FF6666\",\n",
        "    \"#FF0000\",\n",
        "    \"#CC0000\",\n",
        "    \"#800000\",\n",
        "]\n",
        "cmap = ListedColormap(risk_colors)\n",
        "bounds = np.arange(0.5, 6.5, 1)\n",
        "norm = BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "g_sites.plot(\n",
        "    ax=ax,\n",
        "    column=\"risk_index\",\n",
        "    cmap=cmap,\n",
        "    norm=norm,\n",
        "    markersize=180,\n",
        "    edgecolor=\"black\",\n",
        "    linewidth=0.8,\n",
        "    legend=True,\n",
        "    vmin=1,\n",
        "    vmax=5,\n",
        ")\n",
        "\n",
        "for _, row in g_sites.iterrows():\n",
        "    ax.text(\n",
        "        row.geometry.x + 0.02,\n",
        "        row.geometry.y + 0.02,\n",
        "        row[\"name\"],\n",
        "        fontsize=12\n",
        "    )\n",
        "\n",
        "ax.set_aspect(\"equal\", adjustable=\"datalim\")\n",
        "ax.set_xlabel(\"Longitude\", fontsize=12)\n",
        "ax.set_ylabel(\"Latitude\", fontsize=12)\n",
        "\n",
        "forecast_date = today + timedelta(days=TARGET_H)\n",
        "\n",
        "ax.set_title(\n",
        "    f\"FFDRI_new Risk Map (Forecast Date: {forecast_date}, D+{TARGET_H})\",\n",
        "    fontsize=16\n",
        ")\n",
        "\n",
        "minx, miny, maxx, maxy = boundary_4326.total_bounds\n",
        "ax.set_xlim(minx-0.05, maxx+0.05)\n",
        "ax.set_ylim(miny+0.02, maxy-0.02)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 9) CSV ì €ì¥\n",
        "out_csv = f\"FFD_forecast_D{TARGET_H}_{today}.csv\"\n",
        "sites_df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
        "files.download(out_csv)\n",
        "\n",
        "# 10) PNG ì €ì¥\n",
        "out_png = f\"FFD_RISKMAP_D{TARGET_H}_{today}.png\"\n",
        "fig.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
        "files.download(out_png)\n"
      ]
    }
  ]
}