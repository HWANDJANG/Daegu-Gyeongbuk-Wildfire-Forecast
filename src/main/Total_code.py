#!/usr/bin/env python
# coding: utf-8

# In[ ]:


### DWI(ê¸°ìƒì§€ìˆ˜) ì‚°ì¶œì½”ë“œ

# ë‚ ì§œÃ—ì¢Œí‘œë³„ë¡œ ERA5 ê¸°ë°˜ ê¸°í›„ë³€ìˆ˜(Tmean, RH, WSPD, TP_mm)ì™€ DEMì„ ì¶”ì¶œí•˜ëŠ” ì½”ë“œ
# ì…ë ¥: CSV (í•„ìˆ˜ ì»¬ëŸ¼: date(YYYY-MM-DD), lat, lon  / ìˆœì„œëŠ” ë¬´ê´€)
# ì¶œë ¥: fri_inputs_by_row.csv
#   - ì»¬ëŸ¼: date, pid, lon, lat, Tmean(â„ƒ), RH(%), WSPD(m/s), TP_mm(mm/day), DEM(m)
# íŠ¹ì§•:
#   - ë‚ ì§œë³„ ë°°ì¹˜ ì²˜ë¦¬ë¡œ getInfo í˜¸ì¶œ ìˆ˜ë¥¼ ì œì–´
#   - ERA5-Landì— ê°’ì´ ì—†ì„ ë•Œ ERA5(Global)ë¡œ ìë™ ë³´ì™„(unmask)
#   - ERA5 í•´ìƒë„(~9km)ì— ë§ì¶˜ ê¸°ë³¸ scale/ë²„í¼ ì‚¬ìš©(í•„ìš”ì‹œ ì¡°ì • ê°€ëŠ¥)
#   - ê°„ë‹¨í•œ QC ë¡œê·¸ë¡œ NaN ë¹„ìœ¨, ì¢Œí‘œ ë²”ìœ„ ë“±ì„ í™•ì¸

!pip -q install earthengine-api pandas tqdm

# 1) CSV ì—…ë¡œë“œ: ì¢Œí‘œÂ·ë‚ ì§œ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
from google.colab import files
uploaded = files.upload()   # ì˜ˆ: latlon_with_date.csv
CSV_FILENAME = next(iter(uploaded))
CSV_PATH = f"/content/{CSV_FILENAME}"

# 2) ê¸°ë³¸ ì„¤ì •: ìƒ˜í”Œ ë°˜ê²½, ìŠ¤ì¼€ì¼, ë‚ ì§œ ë³´ì • ì˜µì…˜
# ERA5 ê²©ì í¬ê¸°ê°€ ì•½ 9kmì´ë¯€ë¡œ, ê¸°ë³¸ ë°˜ê²½ê³¼ ìŠ¤ì¼€ì¼ì„ 9000më¡œ ì„¤ì •
SAMPLE_RADIUS_M = 9000      # í¬ì¸íŠ¸ ì£¼ë³€ í‰ê· ì„ ë‚¼ ì› ë°˜ê²½(m)
SAMPLE_SCALE_M  = 9000      # reduceRegionsì— ì‚¬ìš©ë  ìŠ¤ì¼€ì¼(m)
OUT_CSV = "/content/fri_inputs_by_row.csv"

# KST ê¸°ì¤€ ì¼í‰ê· ì„ ë§ì¶”ê³  ì‹¶ì€ ê²½ìš° ë‚ ì§œë¥¼ í•˜ë£¨ ì´ë™ì‹œí‚¤ëŠ” ë“± ë³´ì • ê°€ëŠ¥
# ê¸°ë³¸ì€ 0 (ë³´ì • ì—†ìŒ)
DATE_SHIFT_DAYS = 0

# 3) GEE ì´ˆê¸°í™”: í”„ë¡œì íŠ¸ ì„¤ì • ë° ì¸ì¦
import ee, pandas as pd, datetime as dt
from tqdm import tqdm

PROJECT_ID = "solid-time-472606-u0"  # ë³¸ì¸ GEE í”„ë¡œì íŠ¸ ID
try:
    ee.Initialize(project=PROJECT_ID)
except Exception:
    ee.Authenticate(project=PROJECT_ID)
    ee.Initialize(project=PROJECT_ID)
print("[GEE] initialized")

# 4) ì…ë ¥ ë¡œë“œ ë° ì •ê·œí™”: í—¤ë” ì •ë¦¬, í˜• ë³€í™˜, í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬
# - BOM ì œê±°, ì†Œë¬¸ì í—¤ë” í†µì¼, date/lat/lon í˜•ì‹ ì •ë¦¬, ê²°ì¸¡/ì¤‘ë³µ ì œê±°
df = pd.read_csv(CSV_PATH, encoding="utf-8-sig", engine="python")
df.columns = [c.strip().lower().replace("\ufeff", "") for c in df.columns]

need = {"lat", "lon", "date"}
if not (need <= set(df.columns)):
    raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½. í˜„ì¬ í—¤ë”: {list(df.columns)} (í•„ìš”: {sorted(need)})")

df["lon"]  = pd.to_numeric(df["lon"], errors="coerce")
df["lat"]  = pd.to_numeric(df["lat"], errors="coerce")
df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date

# ë‚ ì§œ ë³´ì •ì´ í•„ìš”í•  ë•Œë§Œ ì ìš© (UTC/KST ê²½ê³„ ì´ìŠˆ ì™„í™”ìš©)
if DATE_SHIFT_DAYS != 0:
    df["date"] = df["date"].apply(
        lambda d: (dt.date.fromisoformat(str(d)) + dt.timedelta(days=DATE_SHIFT_DAYS))
        if pd.notnull(d) else d
    )

df = df.dropna(subset=["lon","lat","date"]).drop_duplicates().reset_index(drop=True)
df["pid"] = df.index.astype(int)

print(f"[INFO] rows after cleaning: {len(df)}")
print(df.head())

# 5) GEE ë°ì´í„°ì…‹ ì •ì˜: ERA5(ìœ¡ì§€/ì „ì§€êµ¬)ì™€ DEM(SRTM)
ERA5_LAND   = ee.ImageCollection("ECMWF/ERA5_LAND/HOURLY")
ERA5_GLOBAL = ee.ImageCollection("ECMWF/ERA5/HOURLY")       # ë°”ë‹¤ í¬í•¨
SRTM        = ee.Image("USGS/SRTMGL1_003").select("elevation").rename("DEM")

# 6) ì‹œê°„ë³„ ERA5 ì´ë¯¸ì§€ë¥¼ ì¼ ë‹¨ìœ„ ìš”ì•½ìœ¼ë¡œ ë³€í™˜
def _per_hour_to_vars(im):
    # ê¸°ì˜¨(K)ì„ ì„­ì”¨(â„ƒ)ë¡œ ë³€í™˜
    T  = im.select("temperature_2m").subtract(273.15).rename("Tmean")
    Td = im.select("dewpoint_temperature_2m").subtract(273.15)
    # ìƒëŒ€ìŠµë„(%): Magnus ê³µì‹ ê¸°ë°˜ ê·¼ì‚¬
    a, b = 17.625, 243.04
    RH = Td.expression(
        "100*exp(a*Td/(b+Td) - a*T/(b+T))",
        {"a": a, "b": b, "Td": Td, "T": T}
    ).rename("RH")
    # í’ì†(m/s): u, v ì„±ë¶„ìœ¼ë¡œë¶€í„° ê³„ì‚°
    U = im.select("u_component_of_wind_10m")
    V = im.select("v_component_of_wind_10m")
    WSPD = U.pow(2).add(V.pow(2)).sqrt().rename("WSPD")
    # ê°•ìˆ˜ëŸ‰: ëˆ„ì  ê°•ìˆ˜(m)ë¥¼ mmë¡œ í™˜ì‚°
    TP = im.select("total_precipitation").multiply(1000).rename("TP_mm")
    return T.addBands([RH, WSPD, TP])

def _daily_from(ic, date_str):
    # ì£¼ì–´ì§„ ë‚ ì§œ [d0, d1) êµ¬ê°„ì˜ ì‹œê°„ ìë£Œë¥¼ ì¼ í‰ê· /ì¼í•©ê³„ë¡œ ì§‘ê³„
    d0 = ee.Date(date_str); d1 = d0.advance(1, "day")
    hourly = ic.filterDate(d0, d1).map(_per_hour_to_vars)
    Tmean = hourly.select("Tmean").mean()
    RH    = hourly.select("RH").mean()
    WSPD  = hourly.select("WSPD").mean()
    TP    = hourly.select("TP_mm").sum()
    return Tmean.addBands([RH, WSPD, TP])

def daily_era5(date_str):
    # ERA5-Land ê°’ì„ ìš°ì„  ì‚¬ìš©, ë§ˆìŠ¤í¬ëœ í”½ì…€ì€ ERA5(Global)ë¡œ ë³´ì™„
    land = _daily_from(ERA5_LAND, date_str)
    globe = _daily_from(ERA5_GLOBAL, date_str)
    fused = land.unmask(globe)
    # DEM ë°´ë“œì™€ ë‚ ì§œ ì†ì„± ì¶”ê°€
    return fused.addBands(SRTM).set({"date": date_str})

# 7) reduceRegions ê²°ê³¼ì—ì„œ ì›í•˜ëŠ” í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” í—¬í¼
def pick(p, *names):
    for k in names:
        v = p.get(k)
        if v is not None:
            return v
    return None

# 8) ë‚ ì§œë³„ë¡œ ë°°ì¹˜ ìƒ˜í”Œë§: ê°™ì€ ë‚ ì§œì˜ í¬ì¸íŠ¸ë¥¼ í•œ ë²ˆì— reduceRegions
rows_out = []
unique_dates = sorted({d.isoformat() for d in df["date"].unique()})
print(f"[INFO] unique dates in CSV: {len(unique_dates)} â†’ {unique_dates[:5]}{' ...' if len(unique_dates)>5 else ''}")

for d in tqdm(unique_dates, desc="[sampling by date]"):
    # í˜„ì¬ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” í–‰ë§Œ ì„ íƒ
    sub = df[df["date"] == dt.date.fromisoformat(d)].copy()

    # ê° í¬ì¸íŠ¸ë¥¼ ë²„í¼(ì›ì˜ì—­)ë¡œ í™•ì¥í•œ FeatureCollection ìƒì„±
    fc = ee.FeatureCollection([
        ee.Feature(
            ee.Geometry.Point([float(r["lon"]), float(r["lat"])]).buffer(SAMPLE_RADIUS_M),
            {"pid": int(r["pid"]), "lon": float(r["lon"]), "lat": float(r["lat"])}
        )
        for _, r in sub.iterrows()
    ])

    # í•´ë‹¹ ë‚ ì§œì˜ ì¼ì¼ ERA5+DEM ì´ë¯¸ì§€ ìƒì„±
    img = daily_era5(d)

    # ë²„í¼ ì˜ì—­ì— ëŒ€í•´ í”½ì…€ í‰ê· (reduceRegions) ìˆ˜í–‰
    reduced = img.reduceRegions(
        collection=fc,
        reducer=ee.Reducer.mean(),
        scale=SAMPLE_SCALE_M
    ).map(lambda f: f.set({"date": d}))

    # ê²°ê³¼ë¥¼ í´ë¼ì´ì–¸íŠ¸ë¡œ ê°€ì ¸ì™€ ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì 
    feats = reduced.getInfo().get("features", [])
    for f in feats:
        p = f.get("properties", {})
        rows_out.append({
            "date": p.get("date"),
            "pid":  p.get("pid"),
            "lon":  p.get("lon"),
            "lat":  p.get("lat"),
            "Tmean": pick(p, "Tmean_mean", "Tmean"),
            "RH":    pick(p, "RH_mean", "RH"),
            "WSPD":  pick(p, "WSPD_mean", "WSPD"),
            "TP_mm": pick(p, "TP_mm_mean", "TP_mm"),
            "DEM":   pick(p, "DEM_mean", "DEM", "elevation_mean", "elevation"),
        })

# 9) ê²°ê³¼ ì €ì¥, ê°„ë‹¨ QC, íŒŒì¼ ë‹¤ìš´ë¡œë“œ
df_out = pd.DataFrame(rows_out).sort_values(["date","pid"]).reset_index(drop=True)
df_out.to_csv(OUT_CSV, index=False, encoding="utf-8-sig")

# ê²°ì¸¡ì¹˜ QC: ëª¨ë“  ê¸°ìƒë³€ìˆ˜ê°€ NaNì¸ í–‰ê³¼ ì¼ë¶€ë§Œ NaNì¸ í–‰ ê°œìˆ˜ í™•ì¸
num_total   = len(df_out)
num_all_nan = df_out[["Tmean","RH","WSPD","TP_mm"]].isna().all(axis=1).sum()
num_any_nan = df_out[["Tmean","RH","WSPD","TP_mm"]].isna().any(axis=1).sum()
print(f"[SAVED] {OUT_CSV}  rows={num_total}")
print(f"[QC] all-NaN rows = {num_all_nan} / any-NaN rows = {num_any_nan}")

# í•œêµ­ ëŒ€ëµ ê²½ê³„(ìœ„ë„ 33~39, ê²½ë„ 124~132) ë²—ì–´ë‚œ ì¢Œí‘œê°€ ìˆëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸
bad = df[(df["lat"]<33)|(df["lat"]>39)|(df["lon"]<124)|(df["lon"]>132)]
if len(bad):
    print("[WARN] KR bounds outliers (first 5):")
    print(bad[["pid","date","lat","lon"]].head())

from google.colab import files
files.download(OUT_CSV)

### FMI(ì„ìƒì§€ìˆ˜) ì‚°ì¶œì½”ë“œ

# ì„ìƒë„ ì½”ë“œ(forest)ë¥¼ ì‚°ë¦¼ ìœ í˜•ë³„ FMIë¡œ ë³€í™˜í•˜ëŠ” ì½”ë“œ
# ì…ë ¥: ì—‘ì…€/CSV 1ê°œ (forest ì½”ë“œ ì¹¼ëŸ¼ í¬í•¨)
# ì¶œë ¥: ì›ë³¸ + FMI, FMI_missing ì»¬ëŸ¼ì´ ì¶”ê°€ëœ CSV

# 0) ì¤€ë¹„: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import io, os, re
import numpy as np
import pandas as pd
from google.colab import files

# 1) íŒŒì¼ ì—…ë¡œë“œ: ì—‘ì…€ ë˜ëŠ” CSV 1ê°œ ì„ íƒ
uploaded = files.upload()  # .xlsx, .xls, .csv ëª¨ë‘ í—ˆìš©
assert len(uploaded) == 1, "íŒŒì¼ì„ í•˜ë‚˜ë§Œ ì—…ë¡œë“œí•˜ì„¸ìš”."
in_fname = list(uploaded.keys())[0]
raw = uploaded[in_fname]

# 2) íŒŒì¼ ë¡œë”© í—¬í¼: í™•ì¥ìì™€ ì¸ì½”ë”©ì„ ìë™ íŒë³„í•˜ì—¬ DataFrameìœ¼ë¡œ ë¡œë“œ
def load_table(name: str, buf: bytes) -> pd.DataFrame:
    lower = name.lower()
    if lower.endswith((".xlsx", ".xls")):
        return pd.read_excel(io.BytesIO(buf))  # ì²« ì‹œíŠ¸ ë¡œë“œ
    elif lower.endswith(".csv"):
        # CSVëŠ” utf-8-sig ìš°ì„ , ì‹¤íŒ¨ ì‹œ cp949 ì‹œë„
        try:
            return pd.read_csv(io.StringIO(buf.decode("utf-8-sig")))
        except Exception:
            return pd.read_csv(io.StringIO(buf.decode("cp949")))
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ìì…ë‹ˆë‹¤. .xlsx/.xls/.csv íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”.")

df = load_table(in_fname, raw)

# 3) forest ì½”ë“œ ì»¬ëŸ¼ëª… ì„¤ì • (íŒŒì¼ì— ë§ê²Œ í•„ìš” ì‹œ ì—¬ê¸°ë§Œ ìˆ˜ì •)
FOREST_COL = "forest"

if FOREST_COL not in df.columns:
    raise KeyError(f"ì—…ë¡œë“œí•œ íŒŒì¼ì— '{FOREST_COL}' ì¹¼ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¹¼ëŸ¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”.")

# 4) FMI ë§¤í•‘ í•¨ìˆ˜: ì‚°ë¦¼ì²­ ì½”ë“œ êµ¬ê°„ì„ FMI ì ìˆ˜(ìˆ«ì)ë¡œ ë³€í™˜
def map_code_to_fmi(raw_code):
    try:
        if pd.isna(raw_code):
            return np.nan
        code = int(raw_code)

        # ì¹¨ì—½ìˆ˜ë¦¼(10~21) â†’ FMI=10
        if 10 <= code <= 21:
            return 10
        # í™œì—½ìˆ˜ë¦¼(30~49, 60~68) â†’ FMI=2
        elif (30 <= code <= 49) or (60 <= code <= 68):
            return 2
        # í˜¼íš¨ë¦¼(77) â†’ FMI=3
        elif code == 77:
            return 3
        # ì£½ë¦¼(78) â†’ FMI=8
        elif code == 78:
            return 8
        # ë¬´ë¦½ëª©ì§€(81,82) â†’ FMI=5
        elif code in (81, 82):
            return 5
        # ë¹„ì‚°ë¦¼(91~99, -1) â†’ FMI=0
        elif (91 <= code <= 99) or (code == -1):
            return 0
        # ì •ì˜ë˜ì§€ ì•Šì€ ì½”ë“œ â†’ NaN
        else:
            return np.nan
    except Exception:
        return np.nan

# 5) FMI ê³„ì‚° ë° ë³´ì¡° í”Œë˜ê·¸ ìƒì„±
df["FMI"] = df[FOREST_COL].apply(map_code_to_fmi)
# FMI_missing: ë§¤í•‘ ì‹¤íŒ¨ ë˜ëŠ” ê²°ì¸¡(1ì´ë©´ FMIê°€ ë¹„ì–´ìˆìŒ)
df["FMI_missing"] = df["FMI"].isna().astype(int)

# (ì„ íƒ) forest_type(ë²”ì£¼í˜•)ê¹Œì§€ í•„ìš”í•˜ë©´ ì•„ë˜ ë¸”ë¡ ì£¼ì„ í•´ì œ
# def map_forest_type(code):
#     try:
#         if pd.isna(code): return "unknown"
#         c = int(code)
#         if 10 <= c <= 21: return "conifer"
#         if (30 <= c <= 49) or (60 <= c <= 68): return "broadleaf"
#         if c == 77: return "mixed"
#         if c == 78: return "bamboo"
#         if c in (81, 82): return "grassland"
#         if (91 <= c <= 99) or (c == -1): return "nonforest"
#         return "unknown"
#     except Exception:
#         return "unknown"
# df["forest_type"] = df[FOREST_COL].apply(map_forest_type)

# 6) ê²°ê³¼ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ: ì› íŒŒì¼ëª…ì— _with_fmi ë¶™ì—¬ CSV ì €ì¥
base = re.sub(r"\.(xlsx|xls|csv)$", "", in_fname, flags=re.IGNORECASE)
out_csv = f"{base}_with_fmi.csv"
df.to_csv(out_csv, index=False, encoding="utf-8-sig")
files.download(out_csv)

# (ì„ íƒ) ì—‘ì…€ë¡œë„ ë‚´ë³´ë‚´ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
# out_xlsx = f"{base}_with_fmi.xlsx"
# df.to_excel(out_xlsx, index=False)
# files.download(out_xlsx)

### TMI(ì§€í˜•ì§€ìˆ˜) ì‚°ì¶œì½”ë“œ

# DEMì„ ì´ìš©í•´ ê° ì¢Œí‘œì˜ Slope(ê²½ì‚¬ë„)ì™€ TMI(ê°„ì´ ì§€í˜•ì§€ìˆ˜)ë¥¼ ê³„ì‚°í•˜ëŠ” ì½”ë“œ
# ì…ë ¥: CSV (í•„ìˆ˜ ì»¬ëŸ¼: lon, lat)
# ì¶œë ¥: ì›ë³¸ + DEM, Slope_deg, TMIê°€ ì¶”ê°€ëœ points_with_topography.csv

!pip -q install earthengine-api pandas chardet

import ee, pandas as pd, math, io, chardet
from google.colab import files

# 0) ì„¤ì •: GEE í”„ë¡œì íŠ¸, DEM ìì‚°, TMI ì»¤ë„ ë°˜ê²½, ì¶œë ¥ íŒŒì¼ëª…
PROJECT_ID = 'solid-time-472606-u0'         # ë³¸ì¸ GEE í”„ë¡œì íŠ¸ ID
DEM_ASSET  = 'USGS/SRTMGL1_003'            # ë‹¤ë¥¸ DEM ìì‚°ì´ ìˆë‹¤ë©´ êµì²´ ê°€ëŠ¥
TMI_KERNEL_M = 300                          # TMI ê³„ì‚°ì— ì‚¬ìš©í•  ì£¼ë³€ ë°˜ê²½(ë¯¸í„°)
OUTPUT_CSV = 'points_with_topography.csv'

# 1) GEE ì´ˆê¸°í™”: í”„ë¡œì íŠ¸ ê¸°ë°˜ ì¸ì¦ ë° ì—°ê²°
try:
    ee.Initialize(project=PROJECT_ID)
    print(f'[GEE] Initialized with project = {PROJECT_ID}')
except Exception:
    print('[GEE] OAuth required. Follow the popupâ€¦')
    ee.Authenticate()
    ee.Initialize(project=PROJECT_ID)

# 2) CSV ì—…ë¡œë“œ: ì¢Œí‘œ íŒŒì¼ ì„ íƒ ë° ì¸ì½”ë”© ìë™ íŒë³„
print('[Upload] CSV íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (lon, lat í•„ìˆ˜).')
uploaded = files.upload()
assert len(uploaded) == 1, 'í•œ ê°œì˜ CSVë§Œ ì—…ë¡œë“œí•˜ì„¸ìš”.'
fname, raw = next(iter(uploaded.items()))

# ì¸ì½”ë”© ìë™ íŒë³„ í›„, ì‹¤íŒ¨ ì‹œ í”í•œ ì¸ì½”ë”©ìœ¼ë¡œ ì¬ì‹œë„
enc = chardet.detect(raw)['encoding'] or 'utf-8'
try:
    df = pd.read_csv(io.BytesIO(raw), encoding=enc)
except Exception:
    for alt in ['utf-8-sig','cp949','euc-kr']:
        try:
            df = pd.read_csv(io.BytesIO(raw), encoding=alt)
            enc = alt
            break
        except Exception:
            pass
print(f'[Upload] Loaded "{fname}" with encoding={enc}')

# í•„ìˆ˜ ì¢Œí‘œ ì»¬ëŸ¼ í™•ì¸
assert {'lon','lat'}.issubset(df.columns), "CSVì— lon, lat ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤."

# ê° í–‰ì— ê³ ìœ  row_idë¥¼ ë¶€ì—¬í•˜ì—¬ GEE ê²°ê³¼ì™€ ë‹¤ì‹œ ë§¤ì¹­
df = df.reset_index().rename(columns={'index':'row_id'})

# 3) í¬ì¸íŠ¸ë¥¼ GEE FeatureCollectionìœ¼ë¡œ ë³€í™˜
def row_to_feature(row):
    return ee.Feature(
        ee.Geometry.Point([float(row['lon']), float(row['lat'])]),
        {'row_id': int(row['row_id'])}
    )
fc = ee.FeatureCollection([row_to_feature(r) for r in df.to_dict('records')])

# 4) DEM, ê²½ì‚¬ë„, TMI ê³„ì‚°
dem   = ee.Image(DEM_ASSET).select(['elevation']).rename('DEM')
slope = ee.Terrain.slope(dem).rename('Slope_deg')
slope_rad = slope.multiply(math.pi/180.0)

# ì£¼ë³€ ì§‘ìˆ˜ë©´ì  ê·¼ì‚¬(A_local): ë°˜ê²½ TMI_KERNEL_M ë‚´ í”½ì…€ ë©´ì  í•©
pixel_area = ee.Image.pixelArea()
kernel     = ee.Kernel.circle(TMI_KERNEL_M, 'meters', normalize=False)
A_local    = pixel_area.reduceNeighborhood(ee.Reducer.sum(), kernel).rename('A_local_m2')

# TMI â‰ˆ ln(A_local) - ln(tan(slope)), ê¸‰ê²½ì‚¬ëŠ” tan(slope) ê°’ì´ ì»¤ì§
eps = ee.Number(1e-6)  # 0 íšŒí”¼ìš© ì‘ì€ ê°’
tmi = A_local.add(1).log().subtract(slope_rad.tan().add(eps).log()).rename('TMI')

# DEM, Slope_deg, TMIë¥¼ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ ê²°í•©
topo_img = ee.Image.cat([dem, slope, tmi])

# 5) sampleRegionsë¡œ ê° í¬ì¸íŠ¸ ìœ„ì¹˜ì—ì„œ DEM, Slope_deg, TMI ì¶”ì¶œ
samples = topo_img.sampleRegions(
    collection=fc,
    properties=['row_id'],
    scale=30,           # DEM í•´ìƒë„(ì•½ 30m)ì— ë§ì¶° ìƒ˜í”Œë§
    geometries=False
)

# ê²°ê³¼ë¥¼ í´ë¼ì´ì–¸íŠ¸ë¡œ ë‚´ë ¤ë°›ì•„ DataFrameìœ¼ë¡œ ë³€í™˜
res = samples.getInfo()
rows = [f['properties'] for f in res['features']]
topo_df = pd.DataFrame(rows)  # row_id, DEM, Slope_deg, TMI

# ì›ë³¸ dfì™€ ê³„ì‚° ê²°ê³¼ë¥¼ row_id ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
out = df.merge(topo_df, on='row_id', how='left').drop(columns=['row_id'])

# 6) CSV ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ
out.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
files.download(OUTPUT_CSV)
print(f'[DONE] Saved & Downloaded â†’ {OUTPUT_CSV}')


### ì¢Œí‘œì¡°ì •

# fmi == -1 ì¸ ì¢Œí‘œë¥¼ ERA5-Land ì˜¨ë„ì¥ì„ ì´ìš©í•´
# ê°™ì€ ì‹œê°„Â·ë‚ ì§œ ì£¼ë³€ì˜ "ìµœê³  ê¸°ì˜¨ í”½ì…€" ìœ„ì¹˜ë¡œ ì´ë™ì‹œí‚¤ëŠ” ìŠ¤í¬ë¦½íŠ¸
# ì…ë ¥:
#   - CSV (í•„ìˆ˜ ì»¬ëŸ¼: lat, lon, fmi, date ë˜ëŠ” datetime)
#   - date: YYYY-MM-DD, datetime: YYYY-MM-DD HH:MM:SS
# ì‚¬ìš© ë°ì´í„°:
#   - ECMWF/ERA5_LAND/HOURLY ì˜ temperature_2m
# ì¶œë ¥:
#   - /content/updated_coords.csv        : ì¢Œí‘œ ë³´ì • ê²°ê³¼ (utf-8-sig, Excel í˜¸í™˜)
#   - /content/update_summary.txt        : ì „ì²´ ê°±ì‹  í†µê³„ ìš”ì•½
#   - /content/update_failures.csv       : ëŒ€ì²´ ì‹¤íŒ¨ í–‰ (ìˆì„ ë•Œë§Œ ì €ì¥)

!pip -q install earthengine-api pandas tqdm

import ee, pandas as pd, numpy as np, datetime as dt
from tqdm import tqdm
from google.colab import files

# 0) ì„¤ì •: GEE í”„ë¡œì íŠ¸, íƒìƒ‰ ë°˜ê²½, ì‹œê°„ì°½, ì¶œë ¥ ì˜µì…˜
PROJECT_ID        = 'solid-time-472606-u0'   # ë³¸ì¸ GEE í”„ë¡œì íŠ¸ ID
BUFFER_KM_STEPS   = [5, 8, 12]              # ì£¼ë³€ íƒìƒ‰ ë°˜ê²½(km), ë‹¨ê³„ì ìœ¼ë¡œ í™•ì¥
SCALE_M           = 2000                    # ìƒ˜í”Œ í•´ìƒë„(m), ì‘ì„ìˆ˜ë¡ ì •ë°€Â·ëŠë¦¼
HOUR_MARGIN       = 1                       # datetime ê¸°ì¤€ Â±HOUR_MARGIN ì‹œê°„ì°½
VERBOSE           = True                    # ì¤‘ê°„ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€

# 1) GEE ì´ˆê¸°í™”: í”„ë¡œì íŠ¸ ê¸°ë°˜ ì¸ì¦ ë° ì—°ê²°
try:
    ee.Initialize(project=PROJECT_ID)
except Exception:
    ee.Authenticate(project=PROJECT_ID)
    ee.Initialize(project=PROJECT_ID)
print('[GEE] Initialized')

# 2) íŒŒì¼ ì—…ë¡œë“œ: ì¢Œí‘œÂ·ë‚ ì§œÂ·fmi ì •ë³´ê°€ ë‹´ê¸´ CSV ë¶ˆëŸ¬ì˜¤ê¸°
print("ğŸ”¹ CSV íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì˜ˆ: input_points.csv)")
uploaded = files.upload()
CSV_PATH = list(uploaded.keys())[0]
print(f"ì—…ë¡œë“œ ì™„ë£Œ: {CSV_PATH}")

df = pd.read_csv(CSV_PATH)

# í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬: lat, lon, fmi ì™€ date ë˜ëŠ” datetime í•„ìš”
need = {'lat','lon','fmi'}
assert need.issubset(df.columns), f"CSVì— {need} ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
has_datetime = 'datetime' in df.columns
has_date     = 'date' in df.columns
assert has_datetime or has_date, "CSVì— datetime ë˜ëŠ” date ì»¬ëŸ¼ ì¤‘ í•˜ë‚˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."

# datetime/date íŒŒì‹±: Excel í˜¸í™˜ì„ ìœ„í•´ datetimeì€ ë‚˜ì¤‘ì— ë¬¸ìì—´ë¡œ ë‹¤ì‹œ ì €ì¥
if has_datetime:
    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
if has_date:
    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.date

# 3) ERA5-Land ì˜¨ë„ ì»¬ë ‰ì…˜ ì •ì˜
ERA = ee.ImageCollection("ECMWF/ERA5_LAND/HOURLY").select('temperature_2m')

def parse_time_window(row):
    """í–‰ì—ì„œ ì‹œê°„ì°½(ee.Date start, end) ê³„ì‚°: datetime ìˆìœ¼ë©´ Â±HOUR_MARGIN, ì—†ìœ¼ë©´ í•˜ë£¨"""
    if has_datetime and pd.notnull(row.get('datetime')):
        t0 = pd.to_datetime(row['datetime'])
        start = ee.Date(t0.tz_localize('UTC').to_pydatetime()).advance(-HOUR_MARGIN, 'hour')
        end   = ee.Date(t0.tz_localize('UTC').to_pydatetime()).advance( HOUR_MARGIN+1, 'hour')
    else:
        d0 = pd.to_datetime(row['date']).date()
        start = ee.Date(dt.datetime(d0.year, d0.month, d0.day))
        end   = start.advance(1, 'day')
    return start, end

def find_hottest_point_latlon(lat, lon, start, end):
    """
    ì…ë ¥/ì™¸ë¶€ ì¸í„°í˜ì´ìŠ¤ëŠ” (lat, lon).
    ë‚´ë¶€ GEE í¬ì¸íŠ¸ëŠ” (lon, lat).
    ë°˜í™˜: (new_lat, new_lon, tmax_K) ë˜ëŠ” None
    """
    p = ee.Geometry.Point([float(lon), float(lat)])
    img_time_max = ERA.filterDate(start, end).max()

    for buf_km in BUFFER_KM_STEPS:
        region = p.buffer(buf_km * 1000)

        # 1) ì§€ì • ë°˜ê²½ ë‚´ ìµœëŒ€ ì˜¨ë„ ê³„ì‚°
        max_obj = img_time_max.reduceRegion(
            reducer=ee.Reducer.max(),
            geometry=region,
            scale=SCALE_M,
            bestEffort=True
        )
        max_val = ee.Number(max_obj.get('temperature_2m'))

        # ê°’ì´ ì—†ìœ¼ë©´ ë” í° ë°˜ê²½ìœ¼ë¡œ ì¬ì‹œë„
        try:
            _ = max_val.getInfo()
        except Exception:
            if VERBOSE: print(f"  â€¢ no value @ {buf_km}km â†’ retry")
            continue

        # 2) ìµœëŒ€ ì˜¨ë„ë¥¼ ê°€ì§€ëŠ” í”½ì…€ì˜ ìœ„ê²½ë„ ì¶”ì¶œ
        lonlat = ee.Image.pixelLonLat()
        max_mask = img_time_max.eq(max_val)
        lonlat_masked = lonlat.updateMask(max_mask)

        coord = lonlat_masked.reduceRegion(
            reducer=ee.Reducer.firstNonNull(),
            geometry=region,
            scale=SCALE_M,
            bestEffort=True
        )

        try:
            new_lon = ee.Number(coord.get('longitude')).getInfo()
            new_lat = ee.Number(coord.get('latitude')).getInfo()
            tmax_k  = max_val.getInfo()
            return new_lat, new_lon, tmax_k
        except Exception:
            if VERBOSE: print(f"  â€¢ no coord @ {buf_km}km â†’ retry")
            continue

    return None

# 4) ë©”ì¸ ë£¨í”„: ê° í–‰ì— ëŒ€í•´ fmi==-1 ì´ë©´ ì£¼ë³€ ìµœê³  ê¸°ì˜¨ í”½ì…€ë¡œ ì¢Œí‘œ ì´ë™
updated_lat, updated_lon, picked_tempK = [], [], []
updated_mask, fail_rows = [], []

records = df.to_dict('records')
for i, row in enumerate(tqdm(records, desc='Processing')):
    lat, lon = float(row['lat']), float(row['lon'])
    fmi = row['fmi']
    start, end = parse_time_window(row)

    if fmi == -1:
        try:
            res = find_hottest_point_latlon(lat, lon, start, end)
            if res is None:
                # ëŒ€ì²´ ì‹¤íŒ¨: ì›ë˜ ì¢Œí‘œë¥¼ ìœ ì§€
                updated_lat.append(lat)
                updated_lon.append(lon)
                picked_tempK.append(np.nan)
                updated_mask.append(False)
                fail_rows.append(i)
                if VERBOSE:
                    print(f"[{i}] fallback keep original (no hottest pixel found)")
            else:
                nl, nlo, tmaxk = res
                updated_lat.append(nl)
                updated_lon.append(nlo)
                picked_tempK.append(tmaxk)
                updated_mask.append(True)
                if VERBOSE:
                    print(f"[{i}] fmi=-1 â†’ ({nl:.5f}, {nlo:.5f}) Tmax(K)={tmaxk:.2f}")
        except Exception as e:
            updated_lat.append(lat)
            updated_lon.append(lon)
            picked_tempK.append(np.nan)
            updated_mask.append(False)
            fail_rows.append(i)
            if VERBOSE:
                print(f"[{i}] error â†’ keep original: {e}")
    else:
        # fmi != -1 ì¸ ì¢Œí‘œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        updated_lat.append(lat)
        updated_lon.append(lon)
        picked_tempK.append(np.nan)
        updated_mask.append(False)

df['new_lat'] = updated_lat
df['new_lon'] = updated_lon
df['hottest_temp_K'] = picked_tempK
df['was_updated'] = updated_mask

# ì‹¤ì œ ì¢Œí‘œ êµì²´: fmi==-1 ì´ê³  ëŒ€ì²´ì— ì„±ê³µí•œ í–‰ë§Œ lat, lonì„ new_lat/new_lonìœ¼ë¡œ ë³€ê²½
mask_replace = (df['fmi'] == -1) & (df['was_updated'])
df.loc[mask_replace, 'lat'] = df.loc[mask_replace, 'new_lat']
df.loc[mask_replace, 'lon'] = df.loc[mask_replace, 'new_lon']

# 5) ê²°ê³¼ ì €ì¥ ë° ìš”ì•½ íŒŒì¼ ìƒì„±
# datetime ì»¬ëŸ¼ì€ Excelì—ì„œ ë³´ê¸° ì¢‹ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
if has_datetime:
    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')

OUT_MAIN = '/content/updated_coords.csv'          # utf-8-sig (BOM í¬í•¨)
OUT_FAIL = '/content/update_failures.csv'
OUT_SUMM = '/content/update_summary.txt'

df.to_csv(OUT_MAIN, index=False, encoding='utf-8-sig', float_format='%.6f')

# ëŒ€ì²´ ì‹¤íŒ¨ í–‰ë§Œ ë”°ë¡œ ì €ì¥ (ì°¸ê³ ìš©)
if fail_rows:
    df.iloc[fail_rows].to_csv(OUT_FAIL, index=False, encoding='utf-8-sig', float_format='%.6f')

# ìš”ì•½ ë¦¬í¬íŠ¸ ì‘ì„±: ì „ì²´ í–‰ ìˆ˜, fmi==-1 ê°œìˆ˜, ì‹¤ì œë¡œ ê°±ì‹ ëœ í–‰ ìˆ˜, ì‹¤íŒ¨ í–‰ ìˆ˜
total = len(df)
n_minus1 = int((df['fmi'] == -1).sum())
n_updated = int(mask_replace.sum())
n_failed  = len(fail_rows)
with open(OUT_SUMM, 'w', encoding='utf-8') as f:
    f.write(f"Total rows           : {total}\n")
    f.write(f"fmi == -1 rows       : {n_minus1}\n")
    f.write(f"Updated (-1â†’hot)     : {n_updated}\n")
    f.write(f"Failed (kept original): {n_failed}\n")
print(open(OUT_SUMM, encoding='utf-8').read())

print(f"ì €ì¥ ì™„ë£Œ: {OUT_MAIN}")
if fail_rows:
    print(f"ì‹¤íŒ¨ ëª©ë¡: {OUT_FAIL}")

# ìµœì¢… ì¶œë ¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì¢Œí‘œ ê²°ê³¼, ì‹¤íŒ¨ ëª©ë¡, ìš”ì•½ í…ìŠ¤íŠ¸)
files.download(OUT_MAIN)
if fail_rows:
    files.download(OUT_FAIL)
files.download(OUT_SUMM)

### ì‚°ë¶ˆ ë¹„ë°œìƒ ì§€ì—­ ëœë¤ ìƒ˜í”Œë§

# ì–‘ì„± í™”ì (pos.csv)ì„ ê¸°ì¤€ìœ¼ë¡œ, ë™ì¼ ë‚ ì§œ(Â±K_DAYS)Â·ë™ì¼ í´ëŸ¬ìŠ¤í„°(ì§€ì—­+ê³„ì ˆ)ì—ì„œ
# í™”ì  ì£¼ë³€ BUFFER_M ë°”ê¹¥ì—ì„œ ìŒì„±(ë¹„ë°œìƒ) ì¢Œí‘œë¥¼ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
# ì…ë ¥:
#   - pos.csv (date, lon, lat, region, season, label í¬í•¨, label=1)
# ì¶œë ¥:
#   - sampling_plan_for_extraction.csv (ì–‘ì„±+ìŒì„± í†µí•© ìƒ˜í”Œë§ ê³„íší‘œ)

import os, sys, math, io, datetime as dt
import numpy as np, pandas as pd
from shapely.geometry import Point
from shapely.strtree import STRtree
from pyproj import Transformer

# 0) íŒŒì¼ ì—…ë¡œë“œ: Colab ë˜ëŠ” ë¡œì»¬ í™˜ê²½ì—ì„œ pos.csv ì„ íƒ
def pick_file_interactive():
    # Colab í™˜ê²½: files.upload ì‚¬ìš©
    try:
        from google.colab import files
        up = files.upload()
        fname = next(iter(up.keys()))
        with open(fname, 'wb') as f:
            f.write(up[fname])
        print("[Colab] Uploaded:", fname)
        return fname
    except Exception:
        pass
    # ì¼ë°˜ Jupyter/ë¡œì»¬ í™˜ê²½: Tkinter íŒŒì¼ ì„ íƒì°½ ì‚¬ìš©
    try:
        import tkinter as tk
        from tkinter import filedialog
        root = tk.Tk(); root.withdraw()
        fname = filedialog.askopenfilename(title="Select pos.csv", filetypes=[("CSV","*.csv"),("All","*.*")])
        if not fname: raise RuntimeError("No file selected")
        print("[Local] Selected:", fname)
        return fname
    except Exception as e:
        raise RuntimeError("íŒŒì¼ ì„ íƒ ì‹¤íŒ¨. Colabì´ë©´ google.colab.files.uploadë¥¼, ë¡œì»¬ì´ë©´ Tkinter ì‚¬ìš© ê°€ëŠ¥ í™˜ê²½ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.") from e

POS_CSV = pick_file_interactive()

# 1) ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°: ìŒì„± ë¹„ìœ¨, í™”ì  ë°°ì œ ë²„í¼, ë‚ ì§œ ë²”ìœ„, ë‚œìˆ˜ ì‹œë“œ
RATIO_NEG_PER_POS = 1.5    # ìŒì„±/ì–‘ì„± ëª©í‘œ ë¹„ìœ¨
BUFFER_M          = 1500   # í™”ì  ì£¼ë³€ ë°°ì œ ê±°ë¦¬(m)
K_DAYS            = 0       # ê°™ì€ ë‚ ì§œ(Â±K_DAYS) ë²”ìœ„
SEED              = 42

# ì¢Œí‘œê³„ ë³€í™˜ê¸°: ê²½ìœ„ë„(WGS84) â†” íˆ¬ì˜ì¢Œí‘œ(EPSG:5179, í•œêµ­ ì¤‘ë¶€ê¶Œ ì˜ˆì‹œ)
to_m  = Transformer.from_crs("EPSG:4326", "EPSG:5179", always_xy=True).transform
to_deg= Transformer.from_crs("EPSG:5179", "EPSG:4326", always_xy=True).transform

rng = np.random.default_rng(SEED)

# 2) ì–‘ì„± ë°ì´í„° ë¡œë“œ ë° ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬
pos = pd.read_csv(POS_CSV)
need_cols = {'date','lon','lat','region','season','label'}
missing = need_cols - set(map(str.lower, pos.columns))

# ì›ë³¸ ì»¬ëŸ¼ëª… â†’ ì†Œë¬¸ì í‚¤ë¡œ ë§¤í•‘
cols_map = {c.lower(): c for c in pos.columns}
def col(name): return cols_map[name]

if missing:
    raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}. í•„ìš”í•œ ì»¬ëŸ¼: {sorted(list(need_cols))}")

pos = pos.rename(columns={col('date'):'date', col('lon'):'lon', col('lat'):'lat',
                          col('region'):'region', col('season'):'season', col('label'):'label'})
pos['label'] = 1
pos['date']  = pd.to_datetime(pos['date']).dt.date

# region+season ì¡°í•©ì„ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„° í‚¤ë¡œ ì‚¬ìš©
pos['cluster'] = pos['region'].astype(str) + '|' + pos['season'].astype(str)
print(f"[INFO] Loaded positives: {len(pos)} rows, clusters={pos['cluster'].nunique()}")

# 3) ë‚ ì§œë³„ í™”ì  ë²„í¼(íˆ¬ì˜ì¢Œí‘œê³„ì—ì„œ BUFFER_M ë§Œí¼) ìƒì„±
def to_point_m(lon, lat):
    x, y = to_m(lon, lat)
    return Point(x, y)

pos['date_dt'] = pd.to_datetime(pos['date'])
buf_rows = []
for d, df_d in pos.groupby('date_dt'):
    # í•´ë‹¹ ë‚ ì§œì˜ ëª¨ë“  í™”ì ì„ BUFFER_M ë§Œí¼ ë²„í¼ë§
    geoms_m = [to_point_m(lon, lat).buffer(BUFFER_M) for lon,lat in zip(df_d['lon'], df_d['lat'])]
    buf_rows.append((d, geoms_m, STRtree(geoms_m)))

def get_buffer_union_for_date(d):
    # d ê¸°ì¤€ Â±K_DAYS ë‚ ì§œì˜ ëª¨ë“  í™”ì  ë²„í¼ë¥¼ í•©ì§‘í•©ìœ¼ë¡œ ê³„ì‚°
    dates = [d + dt.timedelta(days=k) for k in range(-K_DAYS, K_DAYS+1)]
    geoms = []
    for dd, geoms_m, _ in buf_rows:
        if dd.date() in [x.date() for x in dates]:
            geoms.extend(geoms_m)
    if not geoms:
        return None
    u = geoms[0]
    for g in geoms[1:]:
        u = u.union(g)
    return u

# 4) í´ëŸ¬ìŠ¤í„°Ã—ë‚ ì§œë³„ ëª©í‘œ ìŒì„± ê°œìˆ˜ ê³„ì‚°
targets = (pos.groupby(['cluster','date'])
             .size().rename('n_pos').reset_index())
targets['n_neg_target'] = np.ceil(RATIO_NEG_PER_POS * targets['n_pos']).astype(int)

# 5) ëœë¤ ìƒ˜í”Œë§ ë°•ìŠ¤ ì„¤ì •: ìµœì†Œ/ìµœëŒ€ ê²½ìœ„ë„ ë°•ìŠ¤ ì•ˆì—ì„œ ìŒì„± ìœ„ì¹˜ ìƒì„±
#   (í•„ìš”í•˜ë©´ ì´ ë°•ìŠ¤ë¥¼ í–‰ì •ê²½ê³„(GeoJSON ë“±)ë¡œ êµì²´ ê°€ëŠ¥)
min_lon, max_lon = pos['lon'].min()-0.5, pos['lon'].max()+0.5
min_lat, max_lat = pos['lat'].min()-0.5, pos['lat'].max()+0.5

def random_points_in_box(n):
    lons = rng.uniform(min_lon, max_lon, n)
    lats = rng.uniform(min_lat, max_lat, n)
    return list(zip(lons, lats))

# 6) ìŒì„± ìƒ˜í”Œë§: í™”ì  ë²„í¼ ë°–ì—ì„œ ëª©í‘œ ê°œìˆ˜ë§Œí¼ ìŒì„± ì¢Œí‘œ ìƒì„±
neg_rows = []
for (cluster, date), row in targets.set_index(['cluster','date']).iterrows():
    need = int(row['n_neg_target'])
    bu = get_buffer_union_for_date(pd.to_datetime(date))
    got = 0
    trials = 0
    cap = max(need * 300, 5000)  # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ìƒí•œ

    while got < need and trials < cap:
        batch_n = min(need - got, 500)
        for lon, lat in random_points_in_box(batch_n):
            trials += 1
            pt_m = to_point_m(lon, lat)
            # í™”ì  ë²„í¼ ì•ˆì´ë©´ ìŒì„± í›„ë³´ì—ì„œ ì œì™¸
            if bu is not None and bu.contains(pt_m):
                continue
            neg_rows.append(dict(
                date=date, lon=lon, lat=lat,
                region=cluster.split('|')[0],
                season=cluster.split('|')[1],
                label=0
            ))
            got += 1
            if got >= need: break

    if got < need:
        # ì§€ì •í•œ ë²„í¼/ë‚ ì§œ ë²”ìœ„ì—ì„œ ìŒì„±ì´ ë¶€ì¡±í•  ë•Œ ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
        print(f"[WARN] ë¶€ì¡±: cluster={cluster}, date={date}, target_neg={need}, got={got}. "
              f"K_DAYS í™•ëŒ€ ë˜ëŠ” BUFFER_M ì™„í™” ê³ ë ¤.")

neg = pd.DataFrame(neg_rows)

# 7) ìµœì¢… ìƒ˜í”Œë§ ê³„íš ë³‘í•© ë° ì €ì¥
pos_out = pos[['date','lon','lat','region','season','label']].copy()
plan = pd.concat([pos_out, neg], ignore_index=True)

out_csv = "sampling_plan_for_extraction.csv"
plan.to_csv(out_csv, index=False)
print(f"[DONE] saved â†’ {out_csv} (rows={len(plan)})")

# ìš”ì•½ ë¦¬í¬íŠ¸: ì „ì²´ label ë¹„ìœ¨ ë° regionÂ·seasonÂ·labelë³„ ê°œìˆ˜ í…Œì´ë¸”
print("\n[Summary]")
print(plan['label'].value_counts())
print(plan.groupby(['region','season','label']).size().unstack(fill_value=0))

from google.colab import files
files.download("sampling_plan_for_extraction.csv")


### ë¶„ë¥˜

# ì‚°ë¶ˆ ë°œìƒ ì—¬ë¶€(label)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ìŠ¤í¬ë¦½íŠ¸
# ì…ë ¥:
#   - CSV 1ê°œ (í•„ìˆ˜ ì»¬ëŸ¼: label, date, lon, lat, TP_mm, Tmax, Tmin, RH, WSPD, FMI, DEM, Slope)
#   - ì„ íƒ ì»¬ëŸ¼: Tmean, TMI, FFDRI, FRI_norm ë“±ì€ ìˆìœ¼ë©´ ì‚¬ìš©
# ì²˜ë¦¬:
#   - ì¢Œí‘œë³„ë¡œ ì‹œê³„ì—´ ì •ë ¬ í›„ ì´ë™í‰ê· Â·ëˆ„ì ê°•ìˆ˜Â·ê±´ì¡°ì¼ìˆ˜(DrySpell) íŒŒìƒ
#   - 2015â€“2022ë…„ì„ í•™ìŠµ, 2023/2024ë…„ì„ í™€ë“œì•„ì›ƒ í‰ê°€
#   - RandomForest/ExtraTrees í•™ìŠµ, RF ê¸°ì¤€ OOFë¡œ ì„ê³„ê°’ ìµœì í™”(F2 ìµœëŒ€, P í•˜í•œ 0.35)
#   - season_clusterë³„ë¡œ ë‹¤ë¥¸ ì„ê³„ê°’ ì ìš©
# ì¶œë ¥:
#   - /content/artifacts/thresholds.json      : ê³„ì ˆë³„ ì„ê³„ê°’
#   - /content/artifacts/feature_list.json    : ì‚¬ìš© í”¼ì²˜ ë¦¬ìŠ¤íŠ¸
#   - /content/artifacts/clf_rf.joblib        : RF ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
#   - /content/pred_cls_2023.csv, pred_cls_2024.csv : ì—°ë„ë³„ ì˜ˆì¸¡/ê²½ë³´ ê²°ê³¼

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ (Colab ê¸°ì¤€)
!pip -q install scikit-learn imbalanced-learn lightgbm joblib pandas

# CSV ì—…ë¡œë“œ (Colab ìœ„ì ¯ìœ¼ë¡œ ì§ì ‘ ì„ íƒ)
from google.colab import files
up = files.upload()  # CSV ì§ì ‘ ì„ íƒ

import io, pandas as pd, numpy as np, json, joblib, os, warnings
warnings.filterwarnings("ignore")

CSV_PATH = next(iter(up))  # ì—…ë¡œë“œí•œ ì²« ë²ˆì§¸ íŒŒì¼ëª…
print("Loaded:", CSV_PATH)

# CSV ë¡œë“œ (ì—¬ëŸ¬ ì¸ì½”ë”©ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„)
enc_trials = ["utf-8", "cp949", "euc-kr"]
for enc in enc_trials:
    try:
        df = pd.read_csv(io.BytesIO(up[CSV_PATH]), encoding=enc, parse_dates=["date"])
        print(f"[INFO] Read CSV with encoding={enc}, rows={len(df)}")
        break
    except Exception as e:
        last_err = e
else:
    raise last_err

# í•„ìˆ˜/ì„ íƒ ì»¬ëŸ¼ ì ê²€ ë° Tmin ë³´ì •
must_cols = ["label","date","lon","lat","TP_mm","Tmax","Tmin","RH","WSPD","FMI","DEM","Slope"]
miss = [c for c in must_cols if c not in df.columns]
if miss:
    # Tminë§Œ ì—†ëŠ” ê²½ìš°: Tmeanì„ ì´ìš©í•´ ê·¼ì‚¬ ìƒì„±
    if miss == ["Tmin"]:
        if "Tmean" in df.columns:
            df["Tmin"] = 2*df["Tmean"] - df["Tmax"]
            print("[INFO] 'Tmin' ë¯¸ì¡´ì¬ â†’ Tmean,Tmaxë¡œ ê·¼ì‚¬ ìƒì„±.")
        else:
            raise ValueError(f"í•„ìˆ˜ ì¹¼ëŸ¼ ëˆ„ë½: {miss} (ë˜ëŠ” Tmean ì œê³µ)")
    else:
        raise ValueError(f"í•„ìˆ˜ ì¹¼ëŸ¼ ëˆ„ë½: {miss}")

opt_cols = ["Tmean","TMI","FFDRI","FRI_norm","FRI_grade","range","RNE","EH","pDWI","DWI_by_month","DWI","season_cluster"]
print("[INFO] Optional present:", [c for c in opt_cols if c in df.columns])

# ì—°ë„ íŒŒìƒ ë³€ìˆ˜ ì¶”ê°€
df["year"] = df["date"].dt.year

# ì¼êµì°¨(DTR) íŒŒìƒ
df["DTR"] = df["Tmax"] - df["Tmin"]

# season_clusterê°€ ì—†ìœ¼ë©´ ë‹¨ìˆœ ê·œì¹™ìœ¼ë¡œ ë´„/ì—¬ë¦„/ê°€ì„ê²¨ìš¸ ë¶„ë¦¬
if "season_cluster" not in df.columns:
    m = df["date"].dt.month
    season = np.where(m.isin([3,4,5]), "spring",
              np.where(m.isin([6,7,8]), "summer", "fall_winter"))
    df["season_cluster"] = season
    print("[INFO] season_cluster ìƒì„± ì™„ë£Œ (rule-based: spring/summer/fall_winter)")

# ì¢Œí‘œ ë‹¨ìœ„ ì‹œê³„ì—´ ê·¸ë£¹ í‚¤ (í•„ìš”í•˜ë©´ season_clusterë¥¼ í¬í•¨í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥)
group_keys = ["lat","lon"]

def add_memory_feats(g):
    # ê° ì¢Œí‘œë³„ë¡œ ì‹œê°„ìˆœ ì •ë ¬ ì´í›„ ì´ë™íŠ¹ì§•ì„ ê³„ì‚°í•œë‹¤ê³  ê°€ì •
    # 3ì¼, 7ì¼ ì´ë™í‰ê·  (ê¸°ì˜¨, ìŠµë„, í’ì†, ê°•ìˆ˜, ì¼êµì°¨)
    for c in ["Tmax","RH","WSPD","TP_mm","DTR"]:
        g[f"{c}_ma3"] = g[c].rolling(3, min_periods=1).mean()
        g[f"{c}_ma7"] = g[c].rolling(7, min_periods=1).mean()
    # ìµœê·¼ 3/7ì¼ ëˆ„ì ê°•ìˆ˜ (ê°•ìˆ˜ëŸ‰ ëˆ„ì )
    g["TP_3obs_sum"] = g["TP_mm"].rolling(3, min_periods=1).sum()
    g["TP_7obs_sum"] = g["TP_mm"].rolling(7, min_periods=1).sum()
    # DrySpell: 0.1mm ì´í•˜ ë¬´ê°•ìˆ˜ì¼ ì—°ì† ê¸¸ì´
    dry = (g["TP_mm"].fillna(0) <= 0.1).astype(int)
    g["DrySpell"] = dry.groupby((dry==0).cumsum()).cumcount() * dry
    return g

# ì¢Œí‘œë³„Â·ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ í›„ ë©”ëª¨ë¦¬ í”¼ì²˜ ìƒì„±
df = df.sort_values(["lat","lon","date"]).groupby(group_keys, group_keys=False).apply(add_memory_feats).reset_index(drop=True)

# ë¶„ë¥˜ ëª¨ë¸ ì…ë ¥ í”¼ì²˜ êµ¬ì„±
FEATURES = [
    "Tmax","RH","WSPD","TP_mm","DTR",
    "Tmax_ma3","Tmax_ma7","RH_ma3","RH_ma7","WSPD_ma3","WSPD_ma7","DTR_ma3","DTR_ma7",
    "TP_3obs_sum","TP_7obs_sum","DrySpell",
    "Slope","FMI","DEM"
]
TARGET = "label"
CLUSTER = "season_cluster"

# ë¬´í•œëŒ€ ê°’ ì œê±° ë° ìˆ˜ì¹˜ ê²°ì¸¡ì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
df[FEATURES] = df[FEATURES].replace([np.inf,-np.inf], np.nan)
df[FEATURES] = df[FEATURES].fillna(df[FEATURES].median())

# í•™ìŠµ/í‰ê°€ ê¸°ê°„ ë¶„í• 
train = df[(df["year"]>=2015) & (df["year"]<=2022)].copy()
test23 = df[df["year"]==2023].copy()
ext24  = df[df["year"]==2024].copy()

print("Shapes â†’", "train:", train.shape, "test23:", test23.shape, "ext24:", ext24.shape)

from sklearn.model_selection import StratifiedGroupKFold
from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_fscore_support
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from scipy.optimize import minimize_scalar
import numpy as np, os, json, joblib

SEED = 123
N_SPLITS = 5

def get_model(name="RF"):
    # RandomForestì™€ ExtraTrees ë‘ ê°€ì§€ íŠ¸ë¦¬ ê¸°ë°˜ ë¶„ë¥˜ê¸° ì •ì˜
    if name=="RF":
        return RandomForestClassifier(
            n_estimators=300, max_depth=None, min_samples_leaf=2,
            n_jobs=-1, random_state=SEED, class_weight="balanced"
        )
    if name=="ET":
        return ExtraTreesClassifier(
            n_estimators=500, max_depth=None, min_samples_leaf=2,
            n_jobs=-1, random_state=SEED, class_weight="balanced"
        )
    raise ValueError

def cv_fit_predict(train_df, feats, target, groups, model_name="RF"):
    # ì—°ë„(year)ë¥¼ ê·¸ë£¹ìœ¼ë¡œ í•˜ëŠ” StratifiedGroupKFold êµì°¨ê²€ì¦ ìˆ˜í–‰
    # ë°˜í™˜:
    #   - foldë³„ í•™ìŠµëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
    #   - ì „ì²´ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ OOF ì˜ˆì¸¡ í™•ë¥ 
    #   - PR-AUC, ROC-AUC ì„±ëŠ¥ ì§€í‘œ
    X = train_df[feats].values
    y = train_df[target].values
    g = train_df[groups].values
    cv = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)
    oof = np.zeros(len(train_df))
    models = []
    for tr, va in cv.split(X, y, groups=g):
        clf = get_model(model_name)
        clf.fit(X[tr], y[tr])
        p  = clf.predict_proba(X[va])[:,1]
        oof[va] = p
        models.append(clf)
    pr, roc = average_precision_score(y, oof), roc_auc_score(y, oof)
    return models, oof, {"PR-AUC":pr, "ROC-AUC":roc}

# RF, ET ë‘ ëª¨ë¸ì— ëŒ€í•´ êµì°¨ê²€ì¦ ê¸°ë°˜ ì„±ëŠ¥ ë¹„êµ
models_rf, oof_rf, metr_rf = cv_fit_predict(train, FEATURES, TARGET, "year", "RF")
models_et, oof_et, metr_et = cv_fit_predict(train, FEATURES, TARGET, "year", "ET")
print("[RF] ", metr_rf)
print("[ET] ", metr_et)

# ì„ê³„ê°’ íƒìƒ‰: F2-score ìµœëŒ€ + Precision í•˜í•œ(â‰¥0.35) ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” threshold ì„ íƒ
def find_threshold_f2_with_precision(y_true, y_prob, min_prec=0.35):
    # 0~1 êµ¬ê°„ì—ì„œ ì´˜ì´˜í•œ ê²©ì(threshold)ë¥¼ ìŠ¤ìº”í•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ íƒìƒ‰
    best_t, best = 0.0, (-1,-1,-1)  # (P,R,F2)
    for thr in np.linspace(0,1,2001):
        yp = (y_prob>=thr).astype(int)
        p,r,f2,_ = precision_recall_fscore_support(y_true, yp, beta=2.0,
                                                   average="binary", zero_division=0)
        if p>=min_prec and f2>best[2]:
            best_t, best = float(thr), (p,r,f2)
    if best[2] >= 0:
        return best_t, {"precision":best[0], "recall":best[1], "f2":best[2]}
    # Precision í•˜í•œì„ ë§Œì¡±í•˜ëŠ” ê°’ì´ ì—†ìœ¼ë©´ F2ë§Œ ìµœëŒ€ê°€ ë˜ëŠ” ì„ê³„ê°’ìœ¼ë¡œ ëŒ€ì²´
    best_t2, best2 = 0.0, (-1,-1,-1)
    for thr in np.linspace(0,1,2001):
        yp = (y_prob>=thr).astype(int)
        p,r,f2,_ = precision_recall_fscore_support(y_true, yp, beta=2.0,
                                                   average="binary", zero_division=0)
        if f2>best2[2]:
            best_t2, best2 = float(thr), (p,r,f2)
    return best_t2, {"precision":best2[0], "recall":best2[1], "f2":best2[2]}

# ì „ì²´ í•™ìŠµê¸°ê°„ OOF í™•ë¥ ì„ ì´ìš©í•´ ì „ì—­ ì„ê³„ê°’ ê³„ì‚°
y_true_tr = train[TARGET].values
t_star, stats = find_threshold_f2_with_precision(y_true_tr, oof_rf, min_prec=0.35)
print(f"[Global t*] {t_star:.4f}  stats={stats}")

# OOF ê²°ê³¼ë¥¼ Seriesë¡œ ë§Œë“¤ì–´ index ê¸°ë°˜ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì„œë¸Œì…‹ ì„ íƒ
oof_series = pd.Series(oof_rf, index=train.index)

# season_clusterë³„ë¡œ ê°œë³„ ì„ê³„ê°’ ê³„ì‚° (ë°ì´í„°ê°€ ì „ë¶€ 0 ë˜ëŠ” 1ì´ë©´ ì „ì—­ ì„ê³„ê°’ ì‚¬ìš©)
thresholds_by_cluster = {}
for cid, gdf in train.groupby("season_cluster"):
    idx = gdf.index
    y_c = gdf[TARGET].values
    p_c = oof_series.loc[idx].values
    if (y_c.sum()==0) or (y_c.sum()==len(y_c)):
        t, s = t_star, {"precision":np.nan,"recall":np.nan,"f2":np.nan}
    else:
        t, s = find_threshold_f2_with_precision(y_c, p_c, 0.35)
    thresholds_by_cluster[str(cid)] = float(t)

thresholds_by_cluster["__global__"] = float(t_star)
print("thresholds_by_cluster =", thresholds_by_cluster)

# RF ëª¨ë¸ ë° ì„ê³„ê°’, í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì•„í‹°íŒ©íŠ¸ë¡œ ì €ì¥
os.makedirs("/content/artifacts", exist_ok=True)
with open("/content/artifacts/thresholds.json","w") as f: json.dump(thresholds_by_cluster, f, indent=2)
with open("/content/artifacts/feature_list.json","w") as f: json.dump(FEATURES, f, indent=2)
joblib.dump(models_rf, "/content/artifacts/clf_rf.joblib")
print("Saved artifacts â†’ /content/artifacts")

def predict_mean_proba(models, X):
    # foldë³„ RF í™•ë¥  ì˜ˆì¸¡ì„ í‰ê· ë‚´ì–´ ìµœì¢… í™•ë¥ ë¡œ ì‚¬ìš©
    return np.mean([m.predict_proba(X)[:,1] for m in models], axis=0)

def evaluate_binary(y, p, thr):
    # ì£¼ì–´ì§„ ì„ê³„ê°’(thr)ì— ëŒ€í•´ PR-AUC, ROC-AUC, Precision/Recall/F2 ë“± ì‚°ì¶œ
    from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_fscore_support
    yhat = (p>=thr).astype(int)
    pr = average_precision_score(y, p)
    roc = roc_auc_score(y, p)
    P,R,F2,_ = precision_recall_fscore_support(y, yhat, beta=2.0, average="binary", zero_division=0)
    return {"PR-AUC":pr, "ROC-AUC":roc, "P":P, "R":R, "F2":F2, "N_pred_pos":int(yhat.sum()), "N":int(len(yhat))}

# ì €ì¥ëœ ì„ê³„ê°’ ë¡œë“œ
with open("/content/artifacts/thresholds.json") as f:
    THR = json.load(f)

# 2023, 2024 ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ ë° ê²½ë³´ CSV ìƒì„±
for name, hold in {"2023":test23, "2024":ext24}.items():
    if len(hold)==0:
        print(f"[{name}] no rows."); continue
    Xh = hold[FEATURES].values
    ph = predict_mean_proba(models_rf, Xh)

    # ì°¸ê³ ìš©: ê³ ì • ì„ê³„ê°’ 0.5 ê¸°ì¤€ ì„±ëŠ¥ ì¶œë ¥
    rep05 = evaluate_binary(hold[TARGET].values, ph, 0.5)
    print(f"[{name}] metrics@0.5 â†’", rep05)

    # ì‹¤ì œ ê²½ë³´ëŠ” season_clusterë³„ ì„ê³„ê°’ ì ìš©
    thr_vec = np.array([ THR.get(str(c), THR["__global__"]) for c in hold["season_cluster"].astype(str) ])
    yhat = (ph >= thr_vec).astype(int)

    out = hold[["date","lon","lat","season_cluster",TARGET]].copy()
    out["y_prob"] = ph
    out["y_pred"] = yhat
    out_path = f"/content/pred_cls_{name}.csv"
    out.to_csv(out_path, index=False)
    print("Saved:", out_path, "| ê²½ë³´ëŸ‰:", yhat.sum(), "/", len(yhat))

    from google.colab import files

# ìƒì„±ëœ ì˜ˆì¸¡ ê²°ê³¼ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ (2023, 2024)
files.download("/content/pred_cls_2023.csv")
files.download("/content/pred_cls_2024.csv")



### íšŒê·€

# FFDRI ë˜ëŠ” FRI_normê³¼ ê°™ì€ ì—°ì†í˜• ìœ„í—˜ì§€ìˆ˜ë¥¼ íšŒê·€ë¡œ ì˜ˆì¸¡í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
# íŠ¹ì§•:
#   - ì—…ë¡œë“œëœ ì›ë³¸ CSVì—ì„œ íƒ€ê¹ƒ(FFDRI/FRI_norm)ì„ ìë™ íƒì§€
#   - ì¢Œí‘œÂ·ê³„ì ˆë³„ ì¼ ë‹¨ìœ„ë¡œ ì¡°ë°€í™”(asfreq('D')) í›„ ì§§ì€ ê²°ì¸¡ë§Œ ë³´ê°„
#   - ê° ì‹œì ì—ì„œ í–¥í›„ Lì¼(0~7ì¼) ë¦¬ë“œ íƒ€ê¹ƒì„ ìƒì„±(row-lead êµ¬ì¡°)
#   - LightGBM ìš°ì„ , ë°ì´í„°ê°€ ì‘ê±°ë‚˜ ê¹Šì€ ë¦¬ë“œì—ì„œëŠ” ElasticNetìœ¼ë¡œ í´ë°±
#   - 2015â€“2022 í•™ìŠµ, 2023/2024ì— ëŒ€í•œ ë¦¬ë“œë³„ ì„±ëŠ¥í‘œ ë° ì˜ˆì¸¡ CSV, ë“±ê¸‰ì»· ì €ì¥
# ì¶œë ¥:
#   - /content/artifacts_reg/reg_L0_7_rowlead.joblib     : ë¦¬ë“œë³„ íšŒê·€ ëª¨ë¸ dict
#   - /content/artifacts_reg/reg_features_used.json      : ë¦¬ë“œë³„ ì‹¤ì œ ì‚¬ìš© í”¼ì²˜ ëª©ë¡
#   - /content/artifacts_reg/grades_reg.json             : ìœ„í—˜ë„ ë“±ê¸‰ êµ¬ê°„(Low/Moderate/High/VeryHigh)
#   - /content/pred_reg_2023_rowlead.csv, _2024_rowlead.csv : ë¦¬ë“œë³„ ì˜ˆì¸¡ê°’/ë“±ê¸‰
#   - /content/artifacts_reg_bundle_rowlead.zip          : íšŒê·€ ê´€ë ¨ ì•„í‹°íŒ©íŠ¸ ì••ì¶•ë³¸

# íšŒê·€ì— í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
import os, json, shutil, joblib, numpy as np, pandas as pd, re
from scipy.stats import pearsonr

# Colab/ë¡œì»¬ ê³µìš© ë‹¤ìš´ë¡œë“œ í—¬í¼
try:
    from google.colab import files
except Exception:
    class _Files:
        def upload(self): raise RuntimeError("ë¡œì»¬ í™˜ê²½: files.upload() ë¯¸ì§€ì›. ëŒ€ì‹  ê²½ë¡œ ì§€ì • ë¡œë“œ í•„ìš”.")
        def download(self, path): print(f"[local] íŒŒì¼ ì €ì¥ë¨: {path}")
    files = _Files()

# ì›ë³¸ ë°ì´í„° CSV ì—…ë¡œë“œ (ì˜ˆ: wildfire_dataset.csv)
print("ì›ë³¸ ë°ì´í„° CSV ì—…ë¡œë“œ (ì˜ˆ: wildfire_dataset.csv)")
up = files.upload()
PATH = list(up.keys())[0]

# ì¸ì½”ë”© ìë™ ì‹œë„ (utf-8-sig â†’ utf-8 â†’ cp949 â†’ euc-kr ìˆœì„œ)
_encs = ["utf-8-sig","utf-8","cp949","euc-kr"]
for enc in _encs:
    try:
        df = pd.read_csv(PATH, encoding=enc)
        print(f"ë¡œë“œ ì„±ê³µ: {PATH} (encoding={enc}) shape={df.shape}")
        break
    except Exception as e:
        last_err = e
else:
    raise RuntimeError(f"CSV ë¡œë“œ ì‹¤íŒ¨: {last_err}")

# ë‚ ì§œ ì»¬ëŸ¼ í‘œì¤€í™” (dateê°€ ì—†ìœ¼ë©´ dt/ë‚ ì§œ/ymd í›„ë³´ë¥¼ dateë¡œ ë¦¬ë„¤ì„)
if "date" not in df.columns:
    cand = [c for c in df.columns if c.lower() in ("dt","ë‚ ì§œ","ymd")]
    if cand:
        df = df.rename(columns={cand[0]: "date"})
    else:
        raise ValueError("date ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤ (YYYY-MM-DD).")
df["date"] = pd.to_datetime(df["date"], errors="coerce")

# ì—°ë„ íŒŒìƒ ë³€ìˆ˜ (ì—†ìœ¼ë©´ ìƒì„±)
if "year" not in df.columns:
    df["year"] = df["date"].dt.year

# season_clusterê°€ ì—†ìœ¼ë©´ ê°„ë‹¨ ê·œì¹™ìœ¼ë¡œ ìƒì„± (ë´„: 2~4, ê°€ì„Â·ê²¨ìš¸: 10~12, ë‚˜ë¨¸ì§€ëŠ” summer)
if "season_cluster" not in df.columns:
    m = df["date"].dt.month
    season = np.select([m.isin([2,3,4]), m.isin([10,11,12])],
                       ["spring","fall_winter"], default="summer")
    df["season_cluster"] = season

# ìœ„ê²½ë„ ì´ë¦„ ì •ë¦¬ (ì†Œë¬¸ìí™” + longitude/latitude â†’ lon/lat ì¹˜í™˜)
def _scrub_cols(cols):
    fixed = {}
    for c in cols:
        cl = c.strip().lower()
        cl = cl.replace("longitude","lon").replace("latitude","lat")
        fixed[c] = cl
    return fixed
df = df.rename(columns=_scrub_cols(df.columns))

# lon/lat í•„ìˆ˜: ì—†ìœ¼ë©´ ìœ ì‚¬ ì´ë¦„ì„ ì°¾ì•„ì„œ ë¦¬ë„¤ì„
for need in ["lon","lat"]:
    if need not in df.columns:
        cand = [c for c in df.columns if re.fullmatch(rf".*\b{need}\b.*", c.lower())]
        if cand:
            df = df.rename(columns={cand[0]: need})
        else:
            raise ValueError(f"{need} ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

# íƒ€ê¹ƒ ì»¬ëŸ¼ ìë™ íƒì§€ (FFDRI â†’ FRI_norm ìˆœìœ¼ë¡œ ìš°ì„ , ì—†ìœ¼ë©´ íŒ¨í„´/ìˆ«ìí˜• í›„ë³´ ì•ˆë‚´)
def pick_target_column(_df):
    cols = list(_df.columns)
    lowmap = {c.lower(): c for c in cols}
    for key in ["ffdri","fri_norm"]:
        if key in lowmap: return lowmap[key]
    for pat in [r"ffdr[iy]?", r"fri[_\s-]*norm", r"^fri$"]:
        for c in cols:
            if re.search(pat, c.lower()):
                return c
    num_candidates = [c for c in cols if np.issubdtype(_df[c].dropna().dtype, np.number)]
    print("íƒ€ê¹ƒ(FFDRI/FRI_norm) ìë™íƒì§€ ì‹¤íŒ¨. ìˆ«ìí˜• í›„ë³´:", num_candidates)
    user = input("íƒ€ê¹ƒ ì¹¼ëŸ¼ëª… ì…ë ¥: ").strip()
    if user in cols: return user
    raise ValueError("íšŒê·€ íƒ€ê¹ƒ ì¹¼ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

TARGET_CONT = pick_target_column(df)
TARGET_CONT = TARGET_CONT.strip()

# ê¸°ë³¸ ê¸°ìƒÂ·ì§€í˜• í”¼ì²˜ ìŠ¤í‚¤ë§ˆ (í‘œì¤€ ì´ë¦„)
base_feats_std = [
    "tmax","rh","wspd","tp_mm","dtr",
    "tmax_ma3","tmax_ma7","rh_ma3","rh_ma7",
    "wspd_ma3","wspd_ma7","dtr_ma3","dtr_ma7",
    "tp_3obs_sum","tp_7obs_sum","dryspell","slope","fmi","dem"
]

# ë°ì´í„°ì…‹ ë‚´ ë‹¤ì–‘í•œ ì´ë¦„ì„ í‘œì¤€ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•œ ë³„ì¹­
ALIASES = {
    "tmax":       ["tmax","t_max","tasmax","tmax_c","tmax_deg","tmax_â„ƒ","tmax_celsius"],
    "rh":         ["rh","relhum","humidity","relative_humidity","rh_%"],
    "wspd":       ["wspd","ws","wind","wind_speed","wspd_mps"],
    "tp_mm":      ["tp_mm","tp","precip","precip_mm","prcp_mm","rain_mm","ppt_mm"],
    "dtr":        ["dtr","diurnal_range","tmax_tmin_diff","tdiff"],
    "tmax_ma3":   ["tmax_ma3","tmax_3ma","tmax_ma_3"],
    "tmax_ma7":   ["tmax_ma7","tmax_7ma","tmax_ma_7"],
    "rh_ma3":     ["rh_ma3","rh_3ma","rh_ma_3"],
    "rh_ma7":     ["rh_ma7","rh_7ma","rh_ma_7"],
    "wspd_ma3":   ["wspd_ma3","wspd_3ma","ws_3ma"],
    "wspd_ma7":   ["wspd_ma7","wspd_7ma","ws_7ma"],
    "dtr_ma3":    ["dtr_ma3","dtr_3ma"],
    "dtr_ma7":    ["dtr_ma7","dtr_7ma"],
    "tp_3obs_sum":["tp_3obs_sum","tp_3d_sum","precip_3d_sum","rain_3d_sum"],
    "tp_7obs_sum":["tp_7obs_sum","tp_7d_sum","precip_7d_sum","rain_7d_sum"],
    "dryspell":   ["dryspell","dry_spell","drydays","dry_days"],
    "slope":      ["slope","dem_slope","srtm_slope"],
    "fmi":        ["fmi","forest_moisture_index","fuel_moisture_index"],
    "dem":        ["dem","elev","elevation","altitude","srtm"]
}

present = set(df.columns)
for std, aliases in ALIASES.items():
    found = None
    for a in aliases:
        if a in present:
            found = a; break
    if found and (found != std) and (std not in df.columns):
        df = df.rename(columns={found: std})

# ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ íšŒê·€ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ (ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ì‚¬ìš©)
FEATURES = [c for c in base_feats_std if c in df.columns]
REG_FEATURES = FEATURES[:]
print(f"ê¸°ë³¸ í”¼ì²˜ ìˆ˜: {len(REG_FEATURES)} / í›„ë³´ {len(base_feats_std)}")

# íšŒê·€ìš© íŒŒë¼ë¯¸í„° ì •ì˜
DATE = "date"
ID_COLS = ["season_cluster","lat","lon"]
DAILY_FFILL_LIMIT = 3
INTERP_LIMIT      = 3
LAG_DAYS = 7
NA_RATIO_CUT = 0.7
MIN_VAR = 1e-10
MIN_FEATS = 5
MIN_Y = 8
MAX_L = 7

# ë°ì¼ë¦¬í™” ë° ì§§ì€ ê²°ì¸¡ ë³´ê°„ì— ì‚¬ìš©í•  ì»¬ëŸ¼
interp_feats = [c for c in (REG_FEATURES + [TARGET_CONT]) if c in df.columns]

# ì¢Œí‘œÂ·ê³„ì ˆë³„ë¡œ ë‚ ì§œ ì¸ë±ìŠ¤ë¥¼ "ë§¤ì¼"ë¡œ í™•ì¥í•˜ê³ 
# ì§§ì€ ê°„ê²©ì˜ ê²°ì¸¡ë§Œ ì‹œê°„ ë³´ê°„/ì•ë’¤ ì±„ìš°ê¸°ë¡œ ë©”ìš°ëŠ” í•¨ìˆ˜
def densify_daily(df_in):
    out_list = []
    for keys, g in df_in.sort_values(DATE).groupby(ID_COLS, dropna=False):
        g = g.set_index(DATE).sort_index()

        # ìµœëŒ€ ì˜ˆì¸¡ ë¦¬ë“œ(MAX_L)ì¼ ì´í›„ê¹Œì§€ ì¸ë±ìŠ¤ë¥¼ í™•ì¥
        end_ext = g.index.max() + pd.Timedelta(days=MAX_L)
        full_idx = pd.date_range(g.index.min(), end_ext, freq="D")
        g = g.reindex(full_idx)

        # ID ì»¬ëŸ¼ ê°’ ìœ ì§€
        for i, col in enumerate(ID_COLS):
            g[col] = keys[i]

        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•´ ì‹œê°„ ë³´ê°„/ì•ë’¤ ì±„ìš°ê¸°ë¥¼ ì ìš©
        num_cols = [c for c in interp_feats if c in g.columns]

        # ê¸°ìƒ/ì§€í˜• í”¼ì²˜: ì•ë’¤ ì§§ì€ ê²°ì¸¡ì—ë§Œ ë³´ê°„/ì±„ìš°ê¸° í—ˆìš©
        g[num_cols] = g[num_cols].interpolate(method="time", limit=INTERP_LIMIT, limit_direction="both")
        g[num_cols] = g[num_cols].ffill(limit=DAILY_FFILL_LIMIT).bfill(limit=DAILY_FFILL_LIMIT)

        # íƒ€ê¹ƒì€ ë¯¸ë˜ ë¦¬ë“œ ë¼ë²¨ ìƒì„±ì„ ìœ„í•´ ì˜¤ë¥¸ìª½(ë¯¸ë˜ ë°©í–¥)ìœ¼ë¡œ ì¡°ê¸ˆ ë” í—ˆìš©
        if TARGET_CONT in g.columns:
            g[TARGET_CONT] = (
                g[TARGET_CONT]
                  .interpolate(method="time", limit=INTERP_LIMIT, limit_direction="forward")
                  .ffill(limit=MAX_L)
            )

        g = g.reset_index().rename(columns={"index": DATE})
        out_list.append(g)
    return pd.concat(out_list, ignore_index=True)

# ì¢Œí‘œÂ·ê³„ì ˆë³„ ë°ì¼ë¦¬ ì‹œê³„ì—´ ìƒì„±
df_dense = densify_daily(df)
print("Dense shape:", df_dense.shape)

# íƒ€ê¹ƒì˜ ê³¼ê±°ê°’(ë˜ê·¸) í”¼ì²˜ ì¶”ê°€ (0ì¼ì „, 1ì¼ì „, ..., LAG_DAYS-1ì¼ì „)
grp = df_dense.sort_values(ID_COLS + [DATE]).groupby(ID_COLS, group_keys=False)
for k in range(0, LAG_DAYS):
    lag_col = f"{TARGET_CONT}_lag{k}"
    df_dense[lag_col] = grp[TARGET_CONT].shift(k)

lag_feats = [f"{TARGET_CONT}_lag{k}" for k in range(0, LAG_DAYS)]
REG_FEATURES = REG_FEATURES + [c for c in lag_feats if c in df_dense.columns]
print(f"ë˜ê·¸ í¬í•¨ í”¼ì²˜ ìˆ˜: {len(REG_FEATURES)}")

# date+L ë§¤ì¹­ ëŒ€ì‹ , ê·¸ë£¹ ë‚´ì—ì„œ ë‹¨ìˆœ ì‹œí”„íŠ¸ë¡œ ë¦¬ë“œ íƒ€ê¹ƒ ìƒì„±
# ê° Lì— ëŒ€í•´ í˜„ì¬ ì‹œì ì—ì„œ Lì¼ ë’¤ì˜ ê°’ì„ íƒ€ê¹ƒìœ¼ë¡œ ì‚¬ìš©
def make_leads_by_shift(df_in, id_cols, date_col, target_col, max_l=7):
    out = df_in.sort_values(id_cols + [date_col]).copy()
    g = out.groupby(id_cols, group_keys=False)
    for L in range(0, max_l+1):
        out[f"{target_col}_L{L}"] = g[target_col].shift(-L)
    return out

df_reg = make_leads_by_shift(df_dense, ID_COLS, DATE, TARGET_CONT, max_l=MAX_L)

# íšŒê·€ìš© í•™ìŠµ/í‰ê°€ ê¸°ê°„ ë¶„í• 
train_r = df_reg[(df_reg["year"]>=2015) & (df_reg["year"]<=2022)].copy()
test23_r = df_reg[df_reg["year"]==2023].copy()
ext24_r  = df_reg[df_reg["year"]==2024].copy()
print("Shapes (reg):", "train:", train_r.shape, "2023:", test23_r.shape, "2024:", ext24_r.shape)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import ElasticNet, Ridge
from sklearn.dummy import DummyRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from lightgbm import LGBMRegressor

# ì†Œê·œëª¨ ë°ì´í„°ì— ì í•©í•œ LightGBM íšŒê·€ê¸° ìƒì„± í•¨ìˆ˜
def make_lgbm_smalldata():
    params = dict(
        n_estimators=1200, learning_rate=0.02,
        num_leaves=31, max_depth=6,
        subsample=0.9, colsample_bytree=0.9,
        min_child_samples=5, random_state=123, n_jobs=-1
    )
    try:
        params.update(dict(min_data_in_bin=1, feature_pre_filter=False, force_col_wise=True))
        _ = LGBMRegressor(**params)
    except TypeError:
        for k in ["min_data_in_bin","feature_pre_filter","force_col_wise"]:
            params.pop(k, None)
    return LGBMRegressor(**params)

# ë¦¬ë“œ Lì— ëŒ€í•´ í•™ìŠµ/í‰ê°€ìš© X, yë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜
def prepare_xy(sub, L, feats, target_col, na_ratio_cut=NA_RATIO_CUT):
    y = sub[f"{target_col}_L{L}"].values
    X = sub[feats].replace([np.inf,-np.inf], np.nan)
    ok = ~np.isnan(y)
    X, y = X.loc[ok], y[ok]
    # ì»¬ëŸ¼ë³„ ê²°ì¸¡ ë¹„ìœ¨ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ì œê±°
    keep = X.isna().mean() <= na_ratio_cut
    X = X.loc[:, keep.values]
    # ì¤‘ì•™ê°’ ëŒ€ì²´ í›„ ë¶„ì‚°ì´ ê±°ì˜ ì—†ëŠ” ì»¬ëŸ¼ ì œê±°
    X = X.fillna(X.median())
    var = X.var(ddof=0)
    X = X.loc[:, var > MIN_VAR]
    # í”¼ì²˜ ìˆ˜ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê¸°ë³¸ ê¸°ìƒ ì½”ì–´ í”¼ì²˜ë¥¼ ì¶”ê°€ë¡œ ë³´ì¥
    if X.shape[1] < MIN_FEATS:
        core = [c for c in ["tmax","rh","wspd","tp_mm","dtr"] if c in sub.columns]
        add = sub.loc[ok, core].replace([np.inf,-np.inf], np.nan).fillna(sub[core].median())
        X = pd.concat([X, add], axis=1)
        var = X.var(ddof=0)
        X = X.loc[:, var > MIN_VAR]
    return X, y, list(X.columns)

# ì†Œê·œëª¨ ë°ì´í„°ìš© íšŒê·€ ëª¨ë¸ í•™ìŠµ (LGBM ìš°ì„ , í•„ìš” ì‹œ ElasticNetìœ¼ë¡œ í´ë°±)
def train_regressor_small_data(X, y, L=None):
    # ë¦¬ë“œê°€ í¬ê±°ë‚˜ í‘œë³¸ì´ ì ìœ¼ë©´ ENetì„ ìš°ì„  ì‚¬ìš©
    if L is None or len(y) < 80 or (isinstance(L, int) and L >= 5):
        model = Pipeline([("scaler", StandardScaler()),
                          ("enet", ElasticNet(alpha=0.05, l1_ratio=0.2, max_iter=10000, random_state=123))])
        model.fit(X, y);  return model, "ENet"
    lgb = make_lgbm_smalldata(); lgb.fit(X, y)
    # LightGBMì´ ì‚¬ì‹¤ìƒ í•™ìŠµë˜ì§€ ì•Šì€ ê²½ìš° ENetìœ¼ë¡œ ëŒ€ì²´
    if getattr(lgb, "best_iteration_", 1) <= 1 or (np.asarray(getattr(lgb, "feature_importances_", []))==0).all():
        model = Pipeline([("scaler", StandardScaler()),
                          ("enet", ElasticNet(alpha=0.03, l1_ratio=0.15, max_iter=10000, random_state=123))])
        model.fit(X, y);  return model, "ENet(fallback)"
    return lgb, "LGBM"

# íšŒê·€ í‰ê°€ í•¨ìˆ˜ (MAE, RMSE, ìƒê´€ê³„ìˆ˜ r, ìƒëŒ€ RMSE ë“± ê³„ì‚°)
def eval_reg(hold, L, model, feats):
    y = hold[f"{TARGET_CONT}_L{L}"].values
    X = hold[feats].replace([np.inf,-np.inf], np.nan)
    ok = ~np.isnan(y)
    y, X = y[ok], X[ok]
    if len(y)==0 or X.shape[1]==0:
        return {"MAE":np.nan,"RMSE":np.nan,"r":np.nan,"rRMSE":np.nan}, np.array([]), np.array([])
    ph = model.predict(X.fillna(X.median()))
    mae  = mean_absolute_error(y, ph)
    rmse = np.sqrt(mean_squared_error(y, ph))
    r    = pearsonr(y, ph)[0] if (len(y)>2 and np.std(ph)>0 and np.std(y)>0) else np.nan
    rrmse = rmse / (np.nanmean(y)+1e-9)
    return {"MAE":mae,"RMSE":rmse,"r":r,"rRMSE":rrmse}, ph, y

# ë°ì´í„°ê°€ ê±°ì˜ ì—†ê±°ë‚˜ íƒ€ê¹ƒì´ ëª¨ë‘ NaNì¸ ê²½ìš° ì‚¬ìš©í•  ì•ˆì „í•œ í‰ê· ê°’ ê³„ì‚°
def safe_mean_constant(sub_df, target_col, L):
    mu = np.nanmean(sub_df[f"{target_col}_L{L}"].values)
    if not np.isfinite(mu): mu = np.nanmean(sub_df[target_col].values)
    if not np.isfinite(mu): mu = 0.0
    return float(mu)

# íšŒê·€ ëª¨ë¸ ë° ì‚¬ìš© í”¼ì²˜ ì €ì¥ ê²½ë¡œ ìƒì„±
os.makedirs("/content/artifacts_reg", exist_ok=True)
reg_models = {}; feature_used_by_L = {}

# ë¦¬ë“œ L=0~MAX_Lê¹Œì§€ ìˆœíšŒí•˜ë©° ê°œë³„ íšŒê·€ ëª¨ë¸ í•™ìŠµ
for L in range(0, MAX_L+1):
    Xtr, ytr, used_feats = prepare_xy(train_r, L, REG_FEATURES, TARGET_CONT)
    if len(ytr) < MIN_Y or len(used_feats)==0:
        mu = safe_mean_constant(train_r, TARGET_CONT, L)
        mdl, tag = DummyRegressor(strategy="constant", constant=mu), "Mean"
        mdl.fit(pd.DataFrame({"c":[0.0]}), [mu]); used_feats=[]
    else:
        mdl, tag = train_regressor_small_data(Xtr, ytr)
    reg_models[L] = mdl; feature_used_by_L[L] = used_feats
    print(f"[L={L}] model={tag}, n={len(ytr)}, used_feats={len(used_feats)}")

# ë¦¬ë“œë³„ ëª¨ë¸ dictì™€ ì‚¬ìš© í”¼ì²˜ dict ì €ì¥
joblib.dump(reg_models, "/content/artifacts_reg/reg_L0_7_rowlead.joblib")
with open("/content/artifacts_reg/reg_features_used.json","w") as f: json.dump(feature_used_by_L, f, indent=2)
print("Saved: /content/artifacts_reg/reg_L0_7_rowlead.joblib, reg_features_used.json")

# í›ˆë ¨ ë°ì´í„°ì˜ í”¼ì²˜ë³„ ì¤‘ì•™ê°’ (ê²°ì¸¡ ëŒ€ì²´ì— ì¼ê´€ë˜ê²Œ ì‚¬ìš©)
train_medians = train_r[REG_FEATURES].replace([np.inf,-np.inf], np.nan).median()

# 2023, 2024ì— ëŒ€í•´ ë¦¬ë“œë³„ íšŒê·€ ì„±ëŠ¥í‘œ ì¶œë ¥
for name, hold in {"2023":test23_r, "2024":ext24_r}.items():
    rows=[]
    for L in range(0, MAX_L+1):
        featsL = feature_used_by_L[L]
        if len(featsL)==0:
            metr = {"MAE":np.nan,"RMSE":np.nan,"r":np.nan,"rRMSE":np.nan}
        else:
            y = hold[f"{TARGET_CONT}_L{L}"].values
            X = hold[featsL].replace([np.inf,-np.inf], np.nan)
            ok = ~np.isnan(y); y, X = y[ok], X[ok]
            if len(y)==0 or X.shape[1]==0:
                metr = {"MAE":np.nan,"RMSE":np.nan,"r":np.nan,"rRMSE":np.nan}
            else:
                X = X.fillna(train_medians.reindex(featsL))
                ph = reg_models[L].predict(X)
                mae  = mean_absolute_error(y, ph)
                rmse = np.sqrt(mean_squared_error(y, ph))
                r    = pearsonr(y, ph)[0] if (len(y)>2 and np.std(ph)>0 and np.std(y)>0) else np.nan
                rrmse = rmse / (np.nanmean(y)+1e-9)
                metr = {"MAE":mae,"RMSE":rmse,"r":r,"rRMSE":rrmse}
        rows.append({"L":L, **metr})
    print(f"[{name}]")
    try:
        from IPython.display import display
        display(pd.DataFrame(rows))
    except:
        print(pd.DataFrame(rows).to_string(index=False))

# í›ˆë ¨ ë°ì´í„° ë¶„í¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ FFDRI/FRI_norm ë“±ê¸‰ êµ¬ê°„ ê³„ì‚°
score = train_r[TARGET_CONT].values
valid_n = np.isfinite(score).sum()
q = np.nanquantile(score, [0.5, 0.65, 0.85]) if valid_n>10 else [np.nanquantile(score,p) for p in [0.25,0.5,0.75]]
grades = {"Low":[-1e18,float(q[0])],"Moderate":[float(q[0]),float(q[1])],
          "High":[float(q[1]),float(q[2])],"VeryHigh":[float(q[2]),1e18]}
with open("/content/artifacts_reg/grades_reg.json","w") as f: json.dump(grades, f, indent=2)
print("Saved: /content/artifacts_reg/grades_reg.json")

# ì—°ì† ì ìˆ˜ë¥¼ ë“±ê¸‰ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼
def score_to_grade(x):
    for k,(a,b) in grades.items():
        if a<=x<b: return k
    return "VeryHigh"

# 2023, 2024ì— ëŒ€í•´ ê° ë¦¬ë“œë³„ ì˜ˆì¸¡ê°’ê³¼ L0 ë“±ê¸‰ì„ í¬í•¨í•œ CSV ì €ì¥
for name, hold in {"2023":test23_r, "2024":ext24_r}.items():
    if len(hold)==0:
        print(f"[{name}] no rows.")
        continue
    out = hold[["date","lon","lat","season_cluster"]].copy()
    for L in range(0, MAX_L+1):
        featsL = feature_used_by_L[L]
        if len(featsL)==0:
            mu = safe_mean_constant(train_r, TARGET_CONT, L)
            yhat = np.full(len(out), mu, dtype=float)
        else:
            Xh = hold[featsL].replace([np.inf,-np.inf], np.nan)
            Xh = Xh.fillna(train_medians.reindex(featsL))
            yhat = reg_models[L].predict(Xh)
        out[f"{TARGET_CONT}_hat_L{L}"] = yhat
    out["grade_hat_L0"] = [score_to_grade(x) for x in out[f"{TARGET_CONT}_hat_L0"].values]
    p = f"/content/pred_reg_{name}_rowlead.csv"; out.to_csv(p, index=False)
    print("Saved:", p)
    try: files.download(p)
    except: pass

# íšŒê·€ ì•„í‹°íŒ©íŠ¸ í´ë”ë¥¼ ZIPìœ¼ë¡œ ë¬¶ì–´ ì €ì¥
shutil.make_archive("/content/artifacts_reg_bundle_rowlead", "zip", "/content/artifacts_reg")
try: files.download("/content/artifacts_reg_bundle_rowlead.zip")
except: print("[local] ZIP ì €ì¥ ì™„ë£Œ: /content/artifacts_reg_bundle_rowlead.zip")

# ë””ë²„ê·¸ìš©: ë¦¬ë“œë³„ ì‹¤ì œ ì‚¬ìš© í”¼ì²˜ ëª©ë¡ ì¼ë¶€ ì¶œë ¥
for L in range(0, MAX_L+1):
    feats = feature_used_by_L.get(L, [])
    print(f"[DEBUG] L={L} used_feats={len(feats)} -> {feats[:10]}")



### í´ëŸ¬ìŠ¤í„°ë§

# NDVI, ëˆ„ì  ì¼ì‚¬ëŸ‰(DSR_total_MJm^2), FFDRIë¥¼ ì´ìš©í•´ ì‚°ë¦¼ ìƒíƒœ/ìœ„í—˜ ìœ í˜•ì„ êµ°ì§‘í™”í•˜ëŠ” ì½”ë“œ
# ì…ë ¥:
#   - df_out_removed (ì „ì²˜ë¦¬ í›„ ì´ìƒì¹˜ ì œê±°ëœ ë°ì´í„°í”„ë ˆì„, NDVI/DSR/FFDRI í¬í•¨)
# ì²˜ë¦¬:
#   - RobustScalerë¡œ ì´ìƒì¹˜ì— ê°•ê±´í•œ ìŠ¤ì¼€ì¼ë§
#   - KMeans(k=2)ë¡œ ë‘ ê°œì˜ í´ëŸ¬ìŠ¤í„°ë¡œ ë‚˜ëˆ”
#   - PCA 2ì°¨ì›ìœ¼ë¡œ ì°¨ì› ì¶•ì†Œ í›„ êµ°ì§‘ì„ ì‹œê°í™”
# ì¶œë ¥:
#   - df["cluster_k2"]ì— êµ°ì§‘ ë ˆì´ë¸” ì¶”ê°€
#   - PCA ì¢Œí‘œ(_PCA1, _PCA2) ë° ì‚°ì ë„ í”Œë¡¯

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import RobustScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

RANDOM_STATE = 42
FEATURES = ['NDVI', 'DSR_total_MJm^2','FFDRI']

# ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ df_out_removedë¥¼ ë³µì‚¬í•´ì„œ ì‚¬ìš©
df = df_out_removed.copy()

# ì›ë³¸ í”¼ì²˜ë§Œ ì¶”ì¶œ
X_raw = df[FEATURES].copy()

# ì´ìƒì¹˜ì— ëœ ë¯¼ê°í•œ RobustScalerë¡œ ìŠ¤ì¼€ì¼ë§
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X_raw)

# ìŠ¤ì¼€ì¼ë§ëœ ê°’ì„ ë³„ë„ DataFrameìœ¼ë¡œ ë³´ê´€ (í•„ìš” ì‹œ ì¶”ê°€ ë¶„ì„ìš©)
X_scaled_df = pd.DataFrame(X_scaled, columns=[f"scaled_{c}" for c in FEATURES], index=df.index)

# KMeans(k=2) êµ°ì§‘í™” ìˆ˜í–‰
kmeans = KMeans(n_clusters=2, random_state=RANDOM_STATE, n_init=10)
labels = kmeans.fit_predict(X_scaled)
df["cluster_k2"] = labels

# PCAë¥¼ ì´ìš©í•´ 2ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜ (ì‹œê°í™”ìš©)
pca = PCA(n_components=2, random_state=RANDOM_STATE)
X_pca = pca.fit_transform(X_scaled)
df["_PCA1"], df["_PCA2"] = X_pca[:, 0], X_pca[:, 1]

# PCA ê³µê°„ì—ì„œ êµ°ì§‘ë³„ ì‚°ì ë„ ì‹œê°í™”
plt.figure(figsize=(7, 6))
sns.scatterplot(data=df, x="_PCA1", y="_PCA2", hue="cluster_k2",
                s=50, alpha=0.85, edgecolor="none")
plt.title("K-Means (k=2) with Robust Scaling")
plt.xlabel(f"PCA1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)")
plt.ylabel(f"PCA2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)")
plt.legend(title="Cluster")
plt.tight_layout()
plt.show()


### ì¢Œí‘œ ì¶”ì¶œ ë° ìœ„í—˜ í›„ë³´ ì§€ì—­ ì„ ì •

# ì¢Œí‘œë³„(2019-01-01 ~ 2024-12-31)ë¡œ ERA5 ê¸°ìƒ ì¼í‰ê· ì„ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
# ì…ë ¥:
#   - /content/sites_by_type.csv  (í•„ìˆ˜ ì»¬ëŸ¼: lat, lon)
# ì¶œë ¥:
#   - /content/fri_inputs_by_row.csv
# í¬í•¨ ë³€ìˆ˜:
#   - Tmean  (ì¼ í‰ê·  ê¸°ì˜¨, â„ƒ)
#   - RH     (ìƒëŒ€ìŠµë„, %)
#   - WSPD   (í’ì†, m/s)
#   - TP_mm  (ì¼ê°•ìˆ˜ëŸ‰, mm/day)
# íŠ¹ì§•:
#   - ERA5-Land ìš°ì„  ì‚¬ìš©, ê²°ì¸¡ ì‹œ ERA5-Globalë¡œ ë³´ì™„
#   - íƒ€ì„ì•„ì›ƒ ì™„í™” ì˜µì…˜(tileScale) ë° ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì´ìš©í•œ ì¬ì‹œë„
#   - ì¼ì • ê°„ê²©ë§ˆë‹¤ CSVë¥¼ ì €ì¥í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ë°©ì‹ìœ¼ë¡œ ì´ì–´ì„œ ì‹¤í–‰ ê°€ëŠ¥

!pip -q install earthengine-api pandas tqdm

import ee, pandas as pd, datetime as dt, time
from google.colab import files
from tqdm import tqdm

# GEE í”„ë¡œì íŠ¸ ë° ê²½ë¡œ, ìƒ˜í”Œë§ ê´€ë ¨ íŒŒë¼ë¯¸í„° ì„¤ì •
PROJECT_ID = "solid-time-472606-u0"     # ë³¸ì¸ GEE í”„ë¡œì íŠ¸ ID
CSV_PATH   = "/content/sites_by_type.csv"
OUT_CSV    = "/content/fri_inputs_by_row.csv"

# ERA5 í•´ìƒë„(~9km) ê¸°ì¤€ ê¶Œì¥ ë°˜ê²½/ìŠ¤ì¼€ì¼
SAMPLE_RADIUS_M = 9000      # í¬ì¸íŠ¸ ì£¼ë³€ í‰ê· ì„ ë‚¼ ë°˜ê²½
SAMPLE_SCALE_M  = 9000      # reduceRegions ìŠ¤ì¼€ì¼
TILESCALE       = 4         # ì„œë²„ ë©”ëª¨ë¦¬/íƒ€ì„ì•„ì›ƒ ì¡°ì ˆìš© (í•„ìš” ì‹œ 8ê¹Œì§€ ì¡°ì • ê°€ëŠ¥)

# ì¶”ì¶œ ë‚ ì§œ ë²”ìœ„
DATE_START = dt.date(2019, 1, 1)
DATE_END   = dt.date(2024,12,31)

# í•„ìš” ì‹œ ë‚ ì§œë¥¼ í†µì§¸ë¡œ ì´ë™ì‹œí‚¬ ë³´ì • ê°’ (ì¼ ë‹¨ìœ„, ë³´í†µ 0 ìœ ì§€)
DATE_SHIFT_DAYS = 0

# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°(ì¼ ë‹¨ìœ„)
CHECKPOINT_EVERY = 50

# getInfo ì¬ì‹œë„ íšŸìˆ˜ì™€ ë°±ì˜¤í”„ ê³„ìˆ˜
MAX_RETRY   = 6
BACKOFF_BASE= 1.5

# GEE ì´ˆê¸°í™”
try:
    ee.Initialize(project=PROJECT_ID)
except Exception:
    ee.Authenticate(project=PROJECT_ID)
    ee.Initialize(project=PROJECT_ID)
print("[GEE] initialized")
ee.data.setDeadline(120000)  # ìš”ì²­ íƒ€ì„ë¦¬ë°‹(ms)

# ì…ë ¥ CSV ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ í•„ìš” ì‹œ ì—…ë¡œë“œ
try:
    _ = open(CSV_PATH, "r")
except FileNotFoundError:
    print("[UPLOAD] sites_by_type.csv íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”")
    uploaded = files.upload()
    if "sites_by_type.csv" not in uploaded:
        name = next(iter(uploaded))
        with open(CSV_PATH, "wb") as f:
            f.write(uploaded[name])
        print(f"[INFO] ì—…ë¡œë“œ íŒŒì¼ì„ sites_by_type.csvë¡œ ì €ì¥: {name} â†’ sites_by_type.csv")
    else:
        print("[INFO] sites_by_type.csv ì—…ë¡œë“œ ì™„ë£Œ")

# ì¢Œí‘œ ëª©ë¡ ë¡œë“œ ë° ì „ì²˜ë¦¬
df_sites = pd.read_csv(CSV_PATH, encoding="utf-8-sig", engine="python")
df_sites.columns = [c.strip().lower().replace("\ufeff","") for c in df_sites.columns]

need = {"lat","lon"}
if not (need <= set(df_sites.columns)):
    raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½. í˜„ì¬ í—¤ë”: {list(df_sites.columns)} (í•„ìš”: {sorted(need)})")

df_sites["lon"] = pd.to_numeric(df_sites["lon"], errors="coerce")
df_sites["lat"] = pd.to_numeric(df_sites["lat"], errors="coerce")
df_sites = df_sites.dropna(subset=["lon","lat"]).drop_duplicates(subset=["lon","lat"]).reset_index(drop=True)
df_sites["pid"] = df_sites.index.astype(int)

print(f"[INFO] sites: {len(df_sites)} rows (ì˜ˆìƒ: 15)")
print(df_sites.head())

# ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
def daterange(d0, d1):
    cur = d0
    while cur <= d1:
        yield cur
        cur = cur + dt.timedelta(days=1)

unique_dates = [d for d in daterange(DATE_START, DATE_END)]
if DATE_SHIFT_DAYS != 0:
    unique_dates = [d + dt.timedelta(days=DATE_SHIFT_DAYS) for d in unique_dates]
unique_dates_str = [d.isoformat() for d in unique_dates]
print(f"[INFO] date range: {unique_dates_str[0]} ~ {unique_dates_str[-1]}  (days={len(unique_dates_str)})  # ì˜ˆìƒ=2192")

# ERA5-Land, ERA5-Global ì»¬ë ‰ì…˜ ì •ì˜
ERA5_LAND   = ee.ImageCollection("ECMWF/ERA5_LAND/HOURLY")
ERA5_GLOBAL = ee.ImageCollection("ECMWF/ERA5/HOURLY")

# ì‹œê°„ë‹¹ ì´ë¯¸ì§€ë¥¼ ì¼í‰ê· /ì¼í•©ì‚° ë³€ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def _per_hour_to_vars(im):
    # ì˜¨ë„(K)ë¥¼ â„ƒë¡œ ë³€í™˜
    T  = im.select("temperature_2m").subtract(273.15).rename("Tmean")
    Td = im.select("dewpoint_temperature_2m").subtract(273.15)
    # Magnus ê³µì‹ì„ ì´ìš©í•œ ìƒëŒ€ìŠµë„ ê³„ì‚°
    a, b = 17.625, 243.04
    RH = Td.expression(
        "100*exp(a*Td/(b+Td) - a*T/(b+T))",
        {"a": a, "b": b, "Td": Td, "T": T}
    ).rename("RH")
    # 10m ë°”ëŒì„±ë¶„ì—ì„œ í’ì†(m/s) ê³„ì‚°
    U = im.select("u_component_of_wind_10m")
    V = im.select("v_component_of_wind_10m")
    WSPD = U.pow(2).add(V.pow(2)).sqrt().rename("WSPD")
    # ê°•ìˆ˜ëŸ‰(m)ì„ mmë¡œ ë³€í™˜
    TP = im.select("total_precipitation").multiply(1000).rename("TP_mm")
    return T.addBands([RH, WSPD, TP])

# íŠ¹ì • ë‚ ì§œì˜ ì¼í‰ê· /ì¼í•©ì‚° ì´ë¯¸ì§€ ìƒì„±
def _daily_from(ic, date_str):
    d0 = ee.Date(date_str); d1 = d0.advance(1, "day")
    hourly = ic.filterDate(d0, d1).map(_per_hour_to_vars)
    Tmean = hourly.select("Tmean").mean()
    RH    = hourly.select("RH").mean()
    WSPD  = hourly.select("WSPD").mean()
    TP    = hourly.select("TP_mm").sum()
    return Tmean.addBands([RH, WSPD, TP])

# ERA5-Landì™€ ERA5-Globalë¥¼ ê²°í•©í•œ ì¼ ë‹¨ìœ„ ì´ë¯¸ì§€ ìƒì„±
def daily_era5(date_str):
    land  = _daily_from(ERA5_LAND,   date_str)
    globe = _daily_from(ERA5_GLOBAL, date_str)
    fused = land.unmask(globe)  # Landì—ì„œ ëˆ„ë½ëœ í”½ì…€ì€ Globalë¡œ ëŒ€ì²´
    return fused.set({"date": date_str})

# getInfoì— ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ì ìš©
def safe_getInfo(ee_obj):
    for k in range(MAX_RETRY):
        try:
            return ee_obj.getInfo()
        except Exception as e:
            wait = BACKOFF_BASE**k
            print(f"[WARN] getInfo retry {k+1}/{MAX_RETRY} in {wait:.2f}s -> {e}")
            time.sleep(wait)
    return ee_obj.getInfo()

# ì´ì–´ë‹¬ë¦¬ê¸°ë¥¼ ìœ„í•œ ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ (ìˆë‹¤ë©´ ì¬ì‚¬ìš©)
try:
    df_out = pd.read_csv(OUT_CSV)
    done_keys = set(zip(df_out['date'].astype(str), df_out['pid'].astype(int)))
    rows_out = df_out.to_dict('records')
    print(f"[RESUME] existing rows: {len(rows_out)} (ì´ì–´ë‹¬ë¦¬ê¸°)")
except Exception:
    rows_out = []
    done_keys = set()

# ë‚ ì§œ Ã— ì¢Œí‘œ ì¡°í•©ì— ëŒ€í•´ ìƒ˜í”Œë§ ì‹¤í–‰
total_dates = len(unique_dates_str)
print(f"[INFO] sampling by date Ã— {len(df_sites)} sites  (dates={total_dates})")

processed_since_cp = 0
for idx, d in enumerate(tqdm(unique_dates_str, desc="[sampling by date]"), 1):
    # ì´ ë‚ ì§œì™€ pid ì¡°í•©ì´ ëª¨ë‘ ì™„ë£Œë˜ì–´ ìˆìœ¼ë©´ ê±´ë„ˆëœ€
    if all((d, int(pid)) in done_keys for pid in df_sites['pid'].tolist()):
        continue

    # ë‚ ì§œë³„ ì¢Œí‘œë¥¼ FeatureCollectionìœ¼ë¡œ êµ¬ì„±, ê° í¬ì¸íŠ¸ëŠ” ë²„í¼ ì˜ì—­ìœ¼ë¡œ í™•ì¥
    fc = ee.FeatureCollection([
        ee.Feature(
            ee.Geometry.Point([float(r["lon"]), float(r["lat"])]).buffer(SAMPLE_RADIUS_M),
            {"pid": int(r["pid"]), "lon": float(r["lon"]), "lat": float(r["lat"])}
        )
        for _, r in df_sites.iterrows()
    ])

    # í•´ë‹¹ ë‚ ì§œì˜ ERA5 ì¼ë³„ ì´ë¯¸ì§€ êµ¬ì„±
    img = daily_era5(d)

    # reduceRegionsë¡œ ëª¨ë“  í¬ì¸íŠ¸ì— ëŒ€í•´ í‰ê· ê°’ ì¶”ì¶œ
    reduced = img.reduceRegions(
        collection=fc,
        reducer=ee.Reducer.mean(),
        scale=SAMPLE_SCALE_M,
        tileScale=TILESCALE
    ).map(lambda f: f.set({"date": d}))

    data  = safe_getInfo(reduced)
    feats = data.get("features", []) if data else []
    for f in feats:
        p = f.get("properties", {})
        key = (str(p.get("date")), int(p.get("pid")))
        if key in done_keys:
            continue
        rows_out.append({
            "date":  p.get("date"),
            "pid":   p.get("pid"),
            "lon":   p.get("lon"),
            "lat":   p.get("lat"),
            "Tmean": p.get("Tmean_mean", p.get("Tmean")),
            "RH":    p.get("RH_mean",    p.get("RH")),
            "WSPD":  p.get("WSPD_mean",  p.get("WSPD")),
            "TP_mm": p.get("TP_mm_mean", p.get("TP_mm")),
        })
        done_keys.add(key)

    processed_since_cp += 1
    if processed_since_cp >= CHECKPOINT_EVERY or idx == total_dates:
        tmp = pd.DataFrame(rows_out).sort_values(["date","pid"]).reset_index(drop=True)
        tmp.to_csv(OUT_CSV, index=False, encoding="utf-8-sig")
        print(f"[CP] saved {len(tmp)} rows at {d}  ({idx}/{total_dates})")
        processed_since_cp = 0

# ìµœì¢… ê²°ê³¼ ì €ì¥ ë° í’ˆì§ˆ ì ê²€
df_out = pd.DataFrame(rows_out).sort_values(["date","pid"]).reset_index(drop=True)
df_out.to_csv(OUT_CSV, index=False, encoding="utf-8-sig")

num_total   = len(df_out)
num_dates   = df_out["date"].nunique()
num_sites   = df_out["pid"].nunique()
num_all_nan = df_out[["Tmean","RH","WSPD","TP_mm"]].isna().all(axis=1).sum()
num_any_nan = df_out[["Tmean","RH","WSPD","TP_mm"]].isna().any(axis=1).sum()

print(f"[SAVED] {OUT_CSV}  rows={num_total}  (sites={num_sites}, dates={num_dates})")
print(f"[QC] all-NaN rows = {num_all_nan} / any-NaN rows = {num_any_nan}")

# ì¢Œí‘œê°€ í•œêµ­ ëŒ€ëµ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê²½ìš° ê²½ê³ 
bad = df_sites[(df_sites["lat"]<33)|(df_sites["lat"]>39)|(df_sites["lon"]<124)|(df_sites["lon"]>132)]
if len(bad):
    print("[WARN] KR bounds outliers (first 5):")
    print(bad[["pid","lat","lon"]].head())

# CSV ë‹¤ìš´ë¡œë“œ (Colab í™˜ê²½)
files.download(OUT_CSV)



### í•˜ë£¨ ì¼ì¡°ëŸ‰ ì¶”ì¶œ

# MODIS/061/MCD18A1ì—ì„œ 3ì‹œê°„ ë‹¨ìœ„ DSR ë°´ë“œë¥¼ í•©ì‚°í•´
# í•˜ë£¨ ëˆ„ì  ì¼ì‚¬ëŸ‰(DSR_total_MJm^2)ì„ êµ¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
# ì…ë ¥:
#   - TOTAL_DWI_with_NDVI11111_filled.xlsx  (date, lon, lat í¬í•¨)
# ì¶œë ¥:
#   - TOTAL_DWI_with_NDVI11111_filled_with_DSR.xlsx
# ì²˜ë¦¬:
#   - ê° í–‰ì˜ ë‚ ì§œ/ì¢Œí‘œì— ëŒ€í•´ DSR_total_MJm^2 ê³„ì‚°
#   - ì—ëŸ¬ ì‹œ NaN ì²˜ë¦¬, ì „ì²´ë¥¼ ì—‘ì…€ë¡œ ì €ì¥

import ee, pandas as pd, numpy as np
from datetime import datetime, timedelta
from tqdm import tqdm

# GEE ì´ˆê¸°í™” (project ì§€ì •, ì‹¤íŒ¨ ì‹œ ì¸ì¦)
try:
    ee.Initialize(project='matprocject11')
    print("GEE initialized (project=matprocject11).")
except Exception:
    ee.Authenticate()
    ee.Initialize()
    print("GEE authenticated & initialized.")

# íŒŒì¼ ê²½ë¡œ ë° ì»¬ëŸ¼ ì´ë¦„
INPUT_XLSX = "/content/TOTAL_DWI_with_NDVI11111_filled.xlsx"
OUTPUT_XLSX = "/content/TOTAL_DWI_with_NDVI11111_filled_with_DSR.xlsx"

DATE_COL = "date"
LON_COL  = "lon"
LAT_COL  = "lat"

# ì‚¬ìš©í•  MODIS DSR ë°´ë“œ ëª©ë¡
MCD18A1_DSR_BANDS = [
    'GMT_0000_DSR','GMT_0300_DSR','GMT_0600_DSR','GMT_0900_DSR',
    'GMT_1200_DSR','GMT_1500_DSR','GMT_1800_DSR','GMT_2100_DSR'
]

def daily_dsr_total_MJm2(lon: float, lat: float, date_str: str, scale: int = 1000):
    """
    íŠ¹ì • ë‚ ì§œ(UTC)ì™€ ì¢Œí‘œì—ì„œ MODIS/061/MCD18A1ì˜ 3ì‹œê°„ DSR ë°´ë“œë¥¼ í•©ì‚°í•´
    í•˜ë£¨ ëˆ„ì  ë³µì‚¬ëŸ‰(MJ/m^2)ì„ ë°˜í™˜.
    ì´ë¯¸ì§€ ë¶€ì¬ ë˜ëŠ” ì˜¤ë¥˜ ì‹œ np.nan ë°˜í™˜.
    """
    try:
        d0 = ee.Date(date_str)
        d1 = d0.advance(1, 'day')
        pt = ee.Geometry.Point([float(lon), float(lat)])

        ic = ee.ImageCollection('MODIS/061/MCD18A1').filterDate(d0, d1)
        img = ic.first()
        if img.getInfo() is None:
            return np.nan

        band_names = img.bandNames()
        actual_bands = ee.List(MCD18A1_DSR_BANDS).filter(ee.Filter.inList('item', band_names))

        # 3ì‹œê°„ ë‹¨ìœ„ W/m^2 ê°’ì„ í•©ì‚° í›„ J/m^2, MJ/m^2ë¡œ ë³€í™˜
        dsr_sum_wm2 = img.select(actual_bands).reduce(ee.Reducer.sum())
        dsr_total_Jm2 = dsr_sum_wm2.multiply(10800)
        dsr_total_MJm2 = dsr_total_Jm2.divide(1e6)

        val = dsr_total_MJm2.reduceRegion(
            reducer=ee.Reducer.first(),
            geometry=pt,
            scale=scale,
            bestEffort=True
        ).get('sum')

        # ë°´ë“œ í‚¤ ì´ë¦„ì´ ë‹¬ë¼ì§„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë³´ì™„ ë¡œì§
        if val is None:
            keys = dsr_total_MJm2.bandNames().getInfo()
            if keys:
                val = dsr_total_MJm2.reduceRegion(
                    reducer=ee.Reducer.first(),
                    geometry=pt,
                    scale=scale,
                    bestEffort=True
                ).get(keys[0])

        result = ee.Number(val).getInfo() if val is not None else np.nan
        return float(result) if result is not None else np.nan

    except Exception:
        return np.nan

# ì…ë ¥ ì—‘ì…€ ì½ê¸°
df = pd.read_excel(INPUT_XLSX)

# ë‚ ì§œë¥¼ ë¬¸ìì—´(YYYY-MM-DD)ë¡œ í†µì¼
def to_date_str(x):
    if pd.isna(x):
        return None
    if isinstance(x, (datetime, pd.Timestamp)):
        return x.strftime("%Y-%m-%d")
    s = str(x).strip()
    return s[:10]

if DATE_COL not in df.columns or LON_COL not in df.columns or LAT_COL not in df.columns:
    raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë¨: '{DATE_COL}', '{LON_COL}', '{LAT_COL}' í•„ìš”")

df["_date_str"] = df[DATE_COL].apply(to_date_str)

# ê° í–‰ì— ëŒ€í•´ DSR_total_MJm^2 ê³„ì‚°
vals = []
for i, row in tqdm(df.iterrows(), total=len(df), desc="Computing DSR_total_MJm^2"):
    date_str = row["_date_str"]
    lon = row[LON_COL]
    lat = row[LAT_COL]

    if pd.isna(date_str) or pd.isna(lon) or pd.isna(lat):
        vals.append(np.nan)
        continue

    v = daily_dsr_total_MJm2(float(lon), float(lat), date_str)
    vals.append(v)

df["DSR_total_MJm^2"] = vals

df.drop(columns=["_date_str"], inplace=True)
df.to_excel(OUTPUT_XLSX, index=False)
print("Saved:", OUTPUT_XLSX)



### NDVI ì¶”ì¶œ 1ì°¨ (DWI ê¸°ë°˜ ì „ì²´ ë°ì´í„°ì…‹)

# LANDSAT NDVIì™€ MODIS NDVIë¥¼ ìš°ì„ ìˆœìœ„ë¡œ ì‚¬ìš©í•´
# ê° ë‚ ì§œÂ·ì¢Œí‘œì— ëŒ€í•´ NDVIë¥¼ ì±„ìš°ëŠ” ìŠ¤í¬ë¦½íŠ¸
# ì…ë ¥:
#   - TOTAL_DWI_CatForest_cleaned.xlsx
# ì¶œë ¥:
#   - TOTAL_DWI_with_NDVI11111.xlsx
# íŠ¹ì§•:
#   - LANDSAT ìš°ì„ , ì‹¤íŒ¨ ì‹œ MODIS
#   - ì‹œê°„ ì°½ ë° ê³µê°„ ë²„í¼ í¬ê¸°ë¥¼ ì ì§„ì ìœ¼ë¡œ í™•ì¥
#   - NDVI, ì‚¬ìš© ë‚ ì§œ, ì‚¬ìš© ì„¼ì„œ, ìœˆë„ìš° ì •ë³´, ë²„í¼ ì •ë³´ê¹Œì§€ ê¸°ë¡
#   - ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì—‘ì…€ ì €ì¥ ê¸°ëŠ¥ í¬í•¨

import ee, pandas as pd, numpy as np, os, math, traceback, time
from datetime import datetime, timedelta
from tqdm import tqdm

# GEE ì´ˆê¸°í™”
try:
    ee.Initialize(project='matprocject11')
    print("GEE initialized")
except Exception as e:
    print("Init failed, authenticating...", repr(e))
    ee.Authenticate()
    ee.Initialize()
    print("GEE authenticated & initialized")

# ê²½ë¡œ ë° ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
INPUT_PATH   = "/content/TOTAL_DWI_CatForest_cleaned.xlsx"
OUTPUT_PATH  = "/content/TOTAL_DWI_with_NDVI11111.xlsx"
CHECKPOINT   = "/content/_ndvi_checkpoint.xlsx"
LOG_PATH     = "/content/_ndvi_log.txt"

DATE_COL, LAT_COL, LON_COL = "date", "lat", "lon"

# ì‚¬ìš©í•  GEE NDVI ì»¬ë ‰ì…˜
LANDSAT_ID = "LANDSAT/COMPOSITES/C02/T1_L2_8DAY_NDVI"
MODIS_ID   = "MODIS/061/MOD13Q1"
BAND       = "NDVI"

# ì‹œê°„ ì°½(ì¼ ë‹¨ìœ„)ê³¼ ê³µê°„ ë²„í¼(ë¯¸í„°) ë‹¨ê³„
WINDOW_STEPS = [16, 24, 32, 45, 60]
BUFFER_STEPS = [120, 300, 600, 1000]

SAVE_EVERY   = 100
PRINT_EVERY  = 50

# íƒ€ê¹ƒ ë‚ ì§œì™€ ê°€ì¥ ê°€ê¹Œìš´ ì˜ìƒ í•˜ë‚˜ë¥¼ ì„ íƒ
def _closest_image(col, target_date_str: str) -> ee.Image:
    d0 = ee.Date(target_date_str)
    def add_diff(img):
        t = ee.Date(img.get('system:time_start'))
        return img.set({
            'date_diff': t.difference(d0, 'day').abs(),
            'img_date': t.format('YYYY-MM-dd')
        })
    return ee.Image(col.map(add_diff).sort('date_diff').first())

# í•˜ë‚˜ì˜ GEE NDVI ë°ì´í„°ì…‹(LANDSAT ë˜ëŠ” MODIS)ì— ëŒ€í•´
# ì‹œê°„ ì°½ ë° ë²„í¼ë¥¼ ì ì§„ì ìœ¼ë¡œ ëŠ˜ë ¤ê°€ë©° NDVIë¥¼ ì¶”ì¶œ
def _try_dataset(lat: float, lon: float, date_str: str,
                 dataset_id: str, band: str, is_modis: bool):

    point = ee.Geometry.Point([float(lon), float(lat)])
    scale = 250 if dataset_id == MODIS_ID else 30
    sfac  = 0.0001 if is_modis else 1.0

    for win in WINDOW_STEPS:
        start = ee.Date((datetime.strptime(date_str, "%Y-%m-%d") - timedelta(days=win)).strftime("%Y-%m-%d"))
        end   = ee.Date((datetime.strptime(date_str, "%Y-%m-%d") + timedelta(days=win+1)).strftime("%Y-%m-%d"))
        col = ee.ImageCollection(dataset_id).filterDate(start, end).select(band)
        if col.size().getInfo() == 0:
            continue

        img = _closest_image(col, date_str)
        used_date = img.get('img_date').getInfo()

        # 1ì°¨: ë²„í¼ ì˜ì—­ì—ì„œ median NDVI ê³„ì‚°
        for buf in BUFFER_STEPS:
            geom = point.buffer(buf)
            try:
                d = img.reduceRegion(
                    reducer=ee.Reducer.median(),
                    geometry=geom, scale=scale,
                    bestEffort=True, maxPixels=1e8
                ).get(band).getInfo()
            except Exception:
                d = None
            if d is not None:
                v = max(-1.0, min(1.0, float(d) * sfac))
                return v, used_date, win, buf, dataset_id

        # 2ì°¨: sampleì„ ì´ìš©í•œ í¬ì¸íŠ¸ ìƒ˜í”Œë§
        try:
            fc = img.sample({
                'region': point, 'scale': scale,
                'numPixels': 1, 'geometries': False, 'tileScale': 2
            })
            d2 = None if fc.size().getInfo() == 0 else ee.Feature(fc.first()).get(band).getInfo()
        except Exception:
            d2 = None
        if d2 is not None:
            v2 = max(-1.0, min(1.0, float(d2) * sfac))
            return v2, used_date, win, 0, dataset_id

        # 3ì°¨: unmask + focal_meanìœ¼ë¡œ ë¹ˆ í”½ì…€ ë³´ì™„ í›„ ë‹¤ì‹œ ë²„í¼ median
        filled = img.select(band).unmask().focal_mean(radius=150, units='meters', iterations=1)
        for buf2 in BUFFER_STEPS:
            geom2 = point.buffer(buf2)
            try:
                d3 = filled.reduceRegion(
                    reducer=ee.Reducer.median(),
                    geometry=geom2, scale=scale,
                    bestEffort=True, maxPixels=1e8
                ).get(band).getInfo()
            except Exception:
                d3 = None
            if d3 is not None:
                v3 = max(-1.0, min(1.0, float(d3) * sfac))
                return v3, used_date, win, buf2, dataset_id

    return None, None, None, None, None

# LANDSAT â†’ MODIS ìˆœìœ¼ë¡œ NDVIë¥¼ ì‹œë„ í›„ ê°’ ë°˜í™˜
def get_ndvi(lat: float, lon: float, date_str: str):
    v, used, win, buf, src = _try_dataset(lat, lon, date_str, LANDSAT_ID, BAND, False)
    if v is not None:
        return v, used, os.path.basename(src), win, buf

    v, used, win, buf, src = _try_dataset(lat, lon, date_str, MODIS_ID, BAND, True)
    if v is not None:
        return v, used, os.path.basename(src), win, buf
    return np.nan, None, None, None, None

# ì…ë ¥ ì—‘ì…€ ë¡œë“œ ë° ë‚ ì§œ ì •ê·œí™”
df = pd.read_excel(INPUT_PATH)
df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors="coerce").dt.strftime("%Y-%m-%d")

# NDVI ê´€ë ¨ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìƒì„±
for c in ["NDVI", "NDVI_used_date", "NDVI_source", "NDVI_window", "NDVI_buffer"]:
    if c not in df.columns:
        df[c] = np.nan if c == "NDVI" else None

errors = []
filled = 0

# ê° í–‰ì— ëŒ€í•´ GEEì—ì„œ NDVI ì¶”ì¶œ
for i, row in tqdm(df.iterrows(), total=len(df)):
    lat, lon, d = row[LAT_COL], row[LON_COL], row[DATE_COL]

    if pd.isna(lat) or pd.isna(lon) or pd.isna(d):
        df.at[i, "NDVI"]            = np.nan
        df.at[i, "NDVI_used_date"]  = None
        df.at[i, "NDVI_source"]     = None
        df.at[i, "NDVI_window"]     = None
        df.at[i, "NDVI_buffer"]     = None
        continue

    try:
        ndvi, used_date, src, win, buf = get_ndvi(float(lat), float(lon), str(d))
        df.at[i, "NDVI"]            = ndvi
        df.at[i, "NDVI_used_date"]  = used_date
        df.at[i, "NDVI_source"]     = src
        df.at[i, "NDVI_window"]     = win
        df.at[i, "NDVI_buffer"]     = buf
        if not (pd.isna(ndvi) or (isinstance(ndvi, float) and math.isnan(ndvi))):
            filled += 1
    except Exception as e:
        errors.append(f"[row {i}] {type(e).__name__}: {repr(e)}")
        df.at[i, "NDVI"]            = np.nan
        df.at[i, "NDVI_used_date"]  = None
        df.at[i, "NDVI_source"]     = None
        df.at[i, "NDVI_window"]     = None
        df.at[i, "NDVI_buffer"]     = None

    # ì§„í–‰ ìƒí™© ì¶œë ¥ ë° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    if (i + 1) % PRINT_EVERY == 0:
        print(f"[{i+1}/{len(df)}] filled so far: {filled}")
    if (i + 1) % SAVE_EVERY == 0:
        df.to_excel(CHECKPOINT, index=False)
        print("Checkpoint saved ->", CHECKPOINT)

# ì¢Œí‘œë³„ ê·¸ë£¹ ë‚´ì—ì„œ NDVI ì„ í˜• ë³´ê°„ìœ¼ë¡œ ë¹ˆ êµ¬ê°„ ì±„ìš°ê¸°
df = df.sort_values([LAT_COL, LON_COL, DATE_COL])
def _fill_group(g):
    g["NDVI"] = g["NDVI"].astype(float)
    g["NDVI"] = g["NDVI"].interpolate(method="linear", limit_direction="both")
    return g
df = df.groupby([LAT_COL, LON_COL], group_keys=False).apply(_fill_group)

# ìµœì¢… ì €ì¥ ë° ì—ëŸ¬ ë¡œê·¸ ê¸°ë¡
df.to_excel(OUTPUT_PATH, index=False)
print("Saved ->", OUTPUT_PATH, "| rows:", len(df), "| filled:", int((~pd.isna(df['NDVI'])).sum()))

if errors:
    with open(LOG_PATH, "w", encoding="utf-8") as f:
        f.write("\n".join(errors))
    print(f"Logged {len(errors)} exceptions -> {LOG_PATH}")
else:
    print("No exceptions.")



### NDVI ì¶”ì¶œ 2ì°¨ (0ê°’ ë³´ì™„ìš©)

# NDVIê°€ 0ìœ¼ë¡œ ì±„ì›Œì§„ í–‰ì— ëŒ€í•´
# Landsat8, Sentinel-2, MODIS, ì›”ë³„ ê¸°í›„ê°’ ìˆœìœ¼ë¡œ ì‹œë„í•´ NDVIë¥¼ ë‹¤ì‹œ ì±„ìš°ëŠ” ìŠ¤í¬ë¦½íŠ¸
# ì…ë ¥:
#   - TOTAL_DWI_with_NDVI11111.xlsx (ê¸°ì¡´ NDVI í¬í•¨)
# ì¶œë ¥:
#   - TOTAL_DWI_with_NDVI11111_filled.xlsx

try:
    ee.Initialize(project='matprocject11')
    print("GEE initialized")
except Exception:
    ee.Authenticate()
    ee.Initialize()
    print("GEE authenticated & initialized")

INPUT_XLSX  = "/content/TOTAL_DWI_with_NDVI11111.xlsx"
OUTPUT_XLSX = "/content/TOTAL_DWI_with_NDVI11111_filled.xlsx"

DATE_COL = None
LON_COL  = None
LAT_COL  = None
NDVI_COL = None

BUF_METERS   = 120
LS_WIN_DAYS  = 32
S2_WIN_DAYS  = 20
CLIM_YEARS   = 5
S2_CLOUD_TH  = 40
REDUCER      = ee.Reducer.mean()

def _safe_date(s):
    if pd.isna(s): return None
    if isinstance(s, (pd.Timestamp, datetime)):
        return pd.to_datetime(s).strftime("%Y-%m-%d")
    try:
        return pd.to_datetime(str(s)).strftime("%Y-%m-%d")
    except Exception:
        return None

def _value_at(img, band, geom, scale, reducer=REDUCER):
    d = img.select(band).reduceRegion(
        reducer=reducer, geometry=geom, scale=scale,
        maxPixels=1e8, bestEffort=True
    )
    return d.get(band)

def _closest(col, target_date):
    t = ee.Date(target_date)
    col2 = col.map(lambda im: im.set('d', ee.Date(im.get('system:time_start')).difference(t,'day').abs()))
    return ee.Image(col2.sort('d').first())

# Landsat L2 ê¸°ë°˜ NDVI
def _landsat_ndvi(point, date_str):
    t  = ee.Date(date_str)
    st = t.advance(-LS_WIN_DAYS,'day')
    en = t.advance( LS_WIN_DAYS,'day').advance(1,'day')

    col = (ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')
           .merge(ee.ImageCollection('LANDSAT/LC09/C02/T1_L2'))
           .filterDate(st, en)
           .filterBounds(point))

    def _mask_map(img):
        qa = img.select('QA_PIXEL')
        cloud  = qa.bitwiseAnd(1<<3).neq(0)
        shadow = qa.bitwiseAnd(1<<4).neq(0)
        snow   = qa.bitwiseAnd(1<<5).neq(0)
        mask = cloud.Or(shadow).Or(snow).Not()
        sr = img.select(['SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7']).multiply(0.0000275).add(-0.2)
        nir = sr.select('SR_B5')
        red = sr.select('SR_B4')
        ndvi = nir.subtract(red).divide(nir.add(red)).rename('NDVI').updateMask(mask)
        return ndvi.copyProperties(img, img.propertyNames())

    ndvi_col = col.map(_mask_map)
    if ndvi_col.size().getInfo() == 0:
        return None, None
    best = _closest(ndvi_col, t)
    geom = point if BUF_METERS==0 else point.buffer(BUF_METERS)
    val  = _value_at(best, 'NDVI', geom, 30)
    return val, ee.Date(best.get('system:time_start')).format('YYYY-MM-dd')

# Sentinel-2 ê¸°ë°˜ NDVI
def _s2_ndvi(point, date_str):
    t  = ee.Date(date_str)
    st = t.advance(-S2_WIN_DAYS,'day')
    en = t.advance( S2_WIN_DAYS,'day').advance(1,'day')

    s2  = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
           .filterDate(st, en).filterBounds(point))

    prob = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY').filterDate(st, en).filterBounds(point)

    joined = ee.Join.saveFirst('cloud_prob').apply(
        primary=s2,
        secondary=prob,
        condition=ee.Filter.equals(leftField='system:index', rightField='system:index')
    )

    def _map(img):
        img = ee.Image(img)
        cp  = ee.Image(img.get('cloud_prob'))
        cp  = ee.Algorithms.If(cp, ee.Image(cp).select('probability'), ee.Image(0).rename('probability'))
        cp  = ee.Image(cp)

        scl = img.select('SCL')
        cloudMask = cp.gt(S2_CLOUD_TH) \
            .Or(scl.eq(3)).Or(scl.eq(8)).Or(scl.eq(9)).Or(scl.eq(10)).Or(scl.eq(11)).Not()

        b4 = img.select('B4').multiply(0.0001)
        b8 = img.select('B8').multiply(0.0001)
        ndvi = b8.subtract(b4).divide(b8.add(b4)).rename('NDVI').updateMask(cloudMask)
        return ndvi.copyProperties(img, img.propertyNames())

    ndvi_col = ee.ImageCollection(joined).map(_map)
    if ndvi_col.size().getInfo() == 0:
        return None, None
    best = _closest(ndvi_col, t)
    geom = point if BUF_METERS==0 else point.buffer(BUF_METERS)
    val  = _value_at(best, 'NDVI', geom, 10)
    return val, ee.Date(best.get('system:time_start')).format('YYYY-MM-dd')

# MODIS ê¸°ë°˜ NDVI
def _modis_ndvi(point, date_str):
    t = ee.Date(date_str)
    col = (ee.ImageCollection('MODIS/061/MOD13Q1')
           .filterDate(t.advance(-40,'day'), t.advance(40,'day'))
           .filterBounds(point)
           .select('NDVI'))

    if col.size().getInfo() == 0:
        return None, None
    best = _closest(col, t)
    ndvi = ee.Image(best).multiply(0.0001).rename('NDVI')
    geom = point if BUF_METERS==0 else point.buffer(BUF_METERS)
    val  = _value_at(ndvi, 'NDVI', geom, 250)
    return val, ee.Date(best.get('system:time_start')).format('YYYY-MM-dd')

# ìµœê·¼ 5ë…„ ì›”ë³„ MODIS í´ë¼ì´ë§ˆí† ë¡œ NDVI ê·¼ì‚¬
def _modis_monthly_clim(point, date_str):
    t = ee.Date(date_str)
    month = t.get('month')
    start = t.advance(-CLIM_YEARS, 'year')
    col = (ee.ImageCollection('MODIS/061/MOD13Q1')
           .filterDate(start, t)
           .filterBounds(point)
           .filter(ee.Filter.calendarRange(month, month, 'month'))
           .select('NDVI')
           .map(lambda im: ee.Image(im).multiply(0.0001).rename('NDVI')))
    if col.size().getInfo() == 0:
        return None, None
    clim = col.median()
    geom = point if BUF_METERS==0 else point.buffer(BUF_METERS)
    val  = _value_at(clim, 'NDVI', geom, 250)
    return val, ee.String('climatology(m=').cat(ee.Number(month).format()).cat(')')

# ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•˜ë©° NDVIë¥¼ ì–»ëŠ” í•¨ìˆ˜
def get_ndvi_with_fallback(lat, lon, date_str):
    pt = ee.Geometry.Point([float(lon), float(lat)])
    funcs = [_landsat_ndvi, _s2_ndvi, _modis_ndvi, _modis_monthly_clim]
    for f in funcs:
        try:
            val, when = f(pt, date_str)
            if val is None:
                continue
            is_null = ee.Algorithms.IsEqual(val, None).getInfo()
            if not is_null:
                return float(ee.Number(val).getInfo())
        except ee.EEException:
            time.sleep(0.5)
            continue
        except Exception:
            continue
    return None

# ì…ë ¥ ì—‘ì…€ ë¡œë“œ ë° ì¹¼ëŸ¼ ìë™ íƒì§€
df = pd.read_excel(INPUT_XLSX)
cols_lower = {c: c.lower() for c in df.columns}

def _guess(colnames, keys):
    for c in colnames:
        cl = c.lower()
        if any(k in cl for k in keys):
            return c
    return None

ndvi_col = NDVI_COL or _guess(df.columns, ['ndvi'])
date_col = DATE_COL or _guess(df.columns, ['date','ë‚ ì§œ','datetime'])
lon_col  = LON_COL  or _guess(df.columns, ['lon','ê²½ë„','x'])
lat_col  = LAT_COL  or _guess(df.columns, ['lat','ìœ„ë„','y'])

assert ndvi_col is not None, "NDVI ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. NDVI_COLì— ëª…ì‹œí•˜ì„¸ìš”."
assert date_col is not None, "date ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. DATE_COLì— ëª…ì‹œí•˜ì„¸ìš”."
assert lon_col  is not None and lat_col is not None, "lon/lat ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

print("Detected columns ->",
      f"NDVI:{ndvi_col}, date:{date_col}, lon:{lon_col}, lat:{lat_col}")

# NDVIê°€ 0ì¸ í–‰ë§Œ ëŒ€ìƒìœ¼ë¡œ GEEì—ì„œ ë‹¤ì‹œ ì¶”ì¶œ
mask = (df[ndvi_col].fillna(0) == 0) & df[lon_col].notna() & df[lat_col].notna() & df[date_col].notna()
rows_to_fill = df[mask].copy()
print(f"Rows to fill (NDVI==0): {len(rows_to_fill)} / {len(df)}")

filled_values = []
idx_list = rows_to_fill.index.tolist()

for i in tqdm(idx_list, desc="Filling NDVI"):
    lat = df.at[i, lat_col]
    lon = df.at[i, lon_col]
    date_str = _safe_date(df.at[i, date_col])
    if (lat is None) or (lon is None) or (date_str is None):
        filled_values.append((i, None))
        continue
    try:
        val = get_ndvi_with_fallback(lat, lon, date_str)
    except Exception:
        val = None
    filled_values.append((i, val))

# NDVI ê°’ì„ ë®ì–´ì“°ê¸° (Noneì€ NaN ì²˜ë¦¬)
for i, val in filled_values:
    if val is not None:
        df.at[i, ndvi_col] = val
    else:
        df.at[i, ndvi_col] = np.nan

df.to_excel(OUTPUT_XLSX, index=False)
print("Saved:", OUTPUT_XLSX)



### ê³ ì • ì¢Œí‘œ(15ê°œ) ê¸°ì¤€ NDVI ì¶”ì¶œ

# 15ê°œ ê³ ì • ì¢Œí‘œì™€ ë‚ ì§œì— ëŒ€í•´
# LANDSAT8 8ì¼ í•©ì„± NDVI â†’ MODIS MOD13Q1 ìˆœìœ¼ë¡œ NDVIë¥¼ ì¶”ì¶œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
# ì…ë ¥:
#   - wildfire_dataset.csv (lat, lon, date í¬í•¨)
# ì¶œë ¥:
#   - wildfire_dataset_with_NDVI.csv

# -*- coding: utf-8 -*-
import ee
import pandas as pd
import numpy as np
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# GEE ì´ˆê¸°í™”
try:
    ee.Initialize(project='matprocject11')
except:
    ee.Authenticate()
    ee.Initialize()

# ê²½ë¡œ ì„¤ì •
INPUT_CSV  = "/content/wildfire_dataset.csv"
OUTPUT_CSV = "/content/wildfire_dataset_with_NDVI.csv"

# ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬
df = pd.read_csv(INPUT_CSV)
df = df.rename(columns=str.lower)
df["date"] = pd.to_datetime(df["date"]).dt.strftime("%Y-%m-%d")

if "lat" not in df.columns or "lon" not in df.columns:
    raise ValueError("lat, lon, date ì»¬ëŸ¼ í•„ìš”")

# GEE NDVI ì»¬ë ‰ì…˜ ì°¸ì¡°
LAND8_ID = "LANDSAT/COMPOSITES/C02/T1_L2_8DAY_NDVI"
MODIS_ID = "MODIS/061/MOD13Q1"

LAND8_COL = ee.ImageCollection(LAND8_ID)
MODIS_COL = ee.ImageCollection(MODIS_ID)

LAND8_SCALE = 30
MODIS_SCALE = 250
SPATIAL_LAND8 = [120, 250]
SPATIAL_MODIS = [250, 500]

# ì§€ì •í•œ ë‚ ì§œì™€ ê°€ì¥ ê°€ê¹Œìš´ ì´ë¯¸ì§€ë¥¼ ì„ íƒ
def closest_image(col, date_str):
    d0 = ee.Date(date_str)
    def add_diff(im):
        t = ee.Date(im.get("system:time_start"))
        return im.set("d", t.difference(d0, "day").abs())
    return ee.Image(col.map(add_diff).sort("d").first())

# ë‹¨ì¼ ì¢Œí‘œì— ëŒ€í•´ NDVIë¥¼ ì¶”ì¶œ (Landsat8 â†’ MODIS ìˆœìœ¼ë¡œ ì‹œë„)
def extract_ndvi_for_one(lat, lon, date_str):
    point = ee.Geometry.Point([lon, lat])

    # Landsat8 8ì¼ í•©ì„± NDVI
    try:
        img_l8 = closest_image(LAND8_COL, date_str).select("NDVI")
        for buf in SPATIAL_LAND8:
            v = img_l8.reduceRegion(
                reducer=ee.Reducer.median(),
                geometry=point.buffer(buf),
                scale=LAND8_SCALE,
                bestEffort=True
            ).get("NDVI").getInfo()
            if v is not None:
                return float(v)
    except:
        pass

    # MODIS NDVI
    try:
        img_m = closest_image(MODIS_COL, date_str).select("NDVI").multiply(0.0001)
        for buf in SPATIAL_MODIS:
            v = img_m.reduceRegion(
                reducer=ee.Reducer.median(),
                geometry=point.buffer(buf),
                scale=MODIS_SCALE,
                bestEffort=True
            ).get("NDVI").getInfo()
            if v is not None:
                return float(v)
    except:
        pass

    return None

# ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì›Œì»¤ í•¨ìˆ˜
def worker(row):
    try:
        return extract_ndvi_for_one(row["lat"], row["lon"], row["date"])
    except:
        return None

rows = df.to_dict("records")
results = []

print("Extracting NDVI ...")

# ThreadPoolExecutorë¡œ NDVI ë³‘ë ¬ ê³„ì‚°
with ThreadPoolExecutor(max_workers=6) as ex:
    futures = {ex.submit(worker, r): i for i, r in enumerate(rows)}
    for f in tqdm(as_completed(futures), total=len(futures)):
        results.append(f.result())

# ê²°ê³¼ ì €ì¥
df["NDVI"] = results
df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

print("Saved:", OUTPUT_CSV)
print("Shape:", df.shape)
print("NDVI non-null:", df["NDVI"].notnull().sum())



### ìµœì¢… ì„ ë³„ ì¢Œí‘œìš© DEM/TMI ë“± ì§€í˜• ì§€í‘œ ì¶”ì¶œ

# ì…ë ¥ ì¢Œí‘œ(lat, lon)ì— ëŒ€í•´
# SRTM DEMìœ¼ë¡œë¶€í„° ê³ ë„, ì‚¬ë©´ ë°©í–¥, ìƒëŒ€ê³ , slope_pos, TMI ë“±ì„ ê³„ì‚°í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
# ì…ë ¥:
#   - CSV (lat, lon í•„ìˆ˜, ì¶”ê°€ ì»¬ëŸ¼ì€ ëª¨ë‘ ìœ ì§€)
# ì¶œë ¥:
#   - input_with_TMI.csv (ê¸°ì¡´ ì»¬ëŸ¼ + elev_gee, aspect_deg, rel_h, slope_pos, TMI ë“±)

# -*- coding: utf-8 -*-
import ee, pandas as pd, np

# Colab í™˜ê²½ì—ì„œ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìœ„í•œ ì¤€ë¹„
try:
    from google.colab import files
except ImportError:
    files = None

# GEE ì´ˆê¸°í™”
PROJECT_ID = "solid-time-472606-u0"

try:
    ee.Initialize(project=PROJECT_ID)
except Exception:
    ee.Authenticate(project=PROJECT_ID)
    ee.Initialize(project=PROJECT_ID)

print("[GEE] initialized")

# ì…ë ¥ CSV ë¡œë“œ (colabì´ë©´ ì—…ë¡œë“œ, ë¡œì»¬ì´ë©´ ê²½ë¡œ ì´ìš©)
if files is not None:
    uploaded = files.upload()
    fname = list(uploaded.keys())[0]
    print(f"[INFO] ì—…ë¡œë“œëœ CSV: {fname}")
    df = pd.read_csv(fname)
else:
    df = pd.read_csv(r"C:\path\to\your\input.csv")

# ê¸°ë³¸ í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
if "lat" not in df.columns or "lon" not in df.columns:
    raise ValueError("ì…ë ¥ CSVì— 'lat', 'lon' ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")

# ë³‘í•©ìš© id(pid) ìƒì„±
df = df.reset_index(drop=True)
df["pid"] = df.index

print("[INPUT COLUMNS]", df.columns.tolist())
print(df.head())

# pandas DataFrame â†’ GEE FeatureCollection(pid + geometryë§Œ ì‚¬ìš©)
def df_to_fc_with_pid(df_in, lat_col="lat", lon_col="lon"):
    feats = []
    for _, row in df_in.iterrows():
        lat = float(row[lat_col])
        lon = float(row[lon_col])
        geom = ee.Geometry.Point([lon, lat])
        props = {"pid": int(row["pid"])}
        feats.append(ee.Feature(geom, props))
    return ee.FeatureCollection(feats)

fc_points = df_to_fc_with_pid(df, "lat", "lon")

# DEM, Aspect, ì£¼ë³€ min/max ê³ ë„, ìƒëŒ€ê³ (rel_h) ê³„ì‚°ìš© ì´ë¯¸ì§€ ì¤€ë¹„
dem = ee.Image("USGS/SRTMGL1_003").select("elevation")
aspect = ee.Terrain.aspect(dem).rename("aspect_deg")

kernel = ee.Kernel.circle(radius=500, units="meters")
dem_min = dem.reduceNeighborhood(reducer=ee.Reducer.min(), kernel=kernel).rename("dem_min")
dem_max = dem.reduceNeighborhood(reducer=ee.Reducer.max(), kernel=kernel).rename("dem_max")

# ìƒëŒ€ê³ : (í˜„ì¬ ê³ ë„ - ì£¼ë³€ ìµœì†Œ) / (ì£¼ë³€ ìµœëŒ€ - ì£¼ë³€ ìµœì†Œ)
rel_h = dem.subtract(dem_min).divide(dem_max.subtract(dem_min)).rename("rel_h")

terrain_img = dem.addBands([aspect, dem_min, dem_max, rel_h])

# ì¢Œí‘œë³„ë¡œ DEM ë° ì§€í˜• ì§€í‘œë¥¼ ìƒ˜í”Œë§
sampled = terrain_img.sampleRegions(
    collection=fc_points,
    scale=30,
    geometries=False
)

sampled_dict = sampled.getInfo()

rows = []
for f in sampled_dict["features"]:
    props = f["properties"]
    rows.append(props)

terrain_df = pd.DataFrame(rows)
print("[TERRAIN DF COLUMNS]", terrain_df.columns.tolist())
print(terrain_df.head())

terrain_df = terrain_df.rename(columns={"elevation": "elev_gee"})

# ìƒëŒ€ê³ (rel_h)ë¥¼ ë°”íƒ•ìœ¼ë¡œ slope_pos ë²”ì£¼ ë¶„ë¥˜
def classify_slope_pos(rel_h_val):
    if pd.isna(rel_h_val):
        return np.nan
    if rel_h_val < 0.2:
        return "ì‚°ë¡í•˜ë¶€"
    elif rel_h_val < 0.4:
        return "ì‚°ë¡ìƒë¶€"
    elif rel_h_val < 0.6:
        return "ì‚°ë³µí•˜ë¶€"
    elif rel_h_val < 0.8:
        return "ì‚°ë³µìƒë¶€"
    else:
        return "ì‚°ì •í•˜ë¶€"

terrain_df["slope_pos"] = terrain_df["rel_h"].apply(classify_slope_pos)

print("[AFTER slope_pos]")
print(terrain_df[["pid", "elev_gee", "aspect_deg", "rel_h", "slope_pos"]].head())

# ì›ë³¸ dfì™€ pid ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
merged = df.merge(
    terrain_df[["pid", "elev_gee", "aspect_deg", "rel_h", "slope_pos"]],
    on="pid",
    how="left"
)

print("[MERGED COLUMNS]", merged.columns.tolist())
print(merged.head())

# TMI ê³„ìˆ˜ í•¨ìˆ˜ë“¤ (ì‚¬ë©´ ë°©í–¥, ìƒëŒ€ê³  ìœ„ì¹˜, ê³ ë„ êµ¬ê°„)
def tmi_aspect(aspect_deg):
    a = aspect_deg % 360
    if 67.5 <= a < 112.5:
        return 1.5
    elif (292.5 <= a < 360) or (0 <= a < 22.5) or (247.5 <= a < 292.5):
        return 2.5
    elif 112.5 <= a < 202.5:
        return 4
    elif (22.5 <= a < 67.5) or (292.5 <= a < 337.5):
        return 4.5
    elif 202.5 <= a < 247.5:
        return 5
    return np.nan

def tmi_position(pos):
    mapping = {
        "ì‚°ì •í•˜ë¶€": 0.5,
        "ì‚°ë³µìƒë¶€": 0.5,
        "ì‚°ë³µí•˜ë¶€": 1,
        "ì‚°ë¡ìƒë¶€": 1.5,
        "ì‚°ë¡í•˜ë¶€": 5
    }
    return mapping.get(pos, np.nan)

def tmi_elevation(elev):
    if elev >= 876:
        return 1
    elif 628 <= elev < 876:
        return 2
    elif 380 <= elev < 628:
        return 3
    elif 132 <= elev < 380:
        return 4
    elif elev < 132:
        return 5
    return np.nan

def calc_tmi(aspect_deg, pos, elev):
    return tmi_aspect(aspect_deg) + tmi_position(pos) + tmi_elevation(elev)

# TMI êµ¬ì„± ìš”ì†Œ ë° ìµœì¢… TMI ê³„ì‚°
merged["TMI_aspect"] = merged["aspect_deg"].apply(tmi_aspect)
merged["TMI_pos"]    = merged["slope_pos"].apply(tmi_position)
merged["TMI_elev"]   = merged["elev_gee"].apply(tmi_elevation)
merged["TMI"] = merged["TMI_aspect"] + merged["TMI_pos"] + merged["TMI_elev"]

print(merged[["lat","lon","elev_gee","aspect_deg","slope_pos","TMI"]].head())

# ìµœì¢… CSV ì €ì¥
OUT_PATH = "input_with_TMI.csv"
merged.to_csv(OUT_PATH, index=False, encoding="utf-8-sig")
print(f"[SAVED] {OUT_PATH}")

if files is not None:
    files.download(OUT_PATH)



### ìµœì¢… ë°ì´í„°ì…‹ ê³„ì ˆë³„ FFDRI ë¶„í¬ í™•ì¸ ë° LSTM 1ì°¨ ëª¨ë¸

# ìµœì¢… LSTM í•™ìŠµìš© ë°ì´í„°ì…‹ì—ì„œ
# FFDRIì˜ ì›”ë³„ í†µê³„ì™€ ê³„ì ˆëŒ€ë³„ ë¶„í¬ë¥¼ í™•ì¸í•˜ê³ 
# ë´„/ê°€ì„ê²¨ìš¸ ê³„ì ˆì— ëŒ€í•´ LSTM 1ì°¨ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ì½”ë“œ

import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score

import tensorflow as tf
from tensorflow.keras import layers, models

# LSTMìš© ì‹œê³„ì—´ ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´ ì¶œë ¥
df = pd.read_csv("/content/lstm_dataset.csv")
df["date"] = pd.to_datetime(df["date"])
df = df.sort_values(["pid", "date"]).reset_index(drop=True)

print(df.head())
print(df.columns)
print(df["pid"].nunique(), df["date"].min(), df["date"].max())

import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("/content/lstm_dataset.csv")
df["date"] = pd.to_datetime(df["date"])
df["Month"] = df["date"].dt.month

# ì›”ë³„ FFDRI ê¸°ì´ˆ í†µê³„ëŸ‰ ê³„ì‚°
monthly_stats = df.groupby("Month")["FFDRI"].agg([
    ("mean_FFDRI", "mean"),
    ("std_FFDRI", "std"),
    ("min_FFDRI", "min"),
    ("max_FFDRI", "max"),
    ("iqr_FFDRI", lambda x: x.quantile(0.75) - x.quantile(0.25)),
])

# Range(ìµœëŒ€-ìµœì†Œ) ì¶”ê°€
monthly_stats["range_FFDRI"] = monthly_stats["max_FFDRI"] - monthly_stats["min_FFDRI"]

monthly_stats

# ê³„ì ˆëŒ€ë³„ FFDRI KDE ë¹„êµ í”Œë¡¯
plt.figure(figsize=(10,6))
sns.kdeplot(df[df["Month"].isin([2,3,4])]["FFDRI"], label="ë´„(2â€“4ì›”)", linewidth=2)
sns.kdeplot(df[df["Month"].isin([10,11,12])]["FFDRI"], label="10â€“12ì›”", linewidth=2)
sns.kdeplot(df[df["Month"].isin([5,6,7,8,9])]["FFDRI"], label="5â€“9ì›”", linewidth=2)
plt.title("ê³„ì ˆëŒ€ë³„ FFDRI KDE ë¹„êµ")
plt.legend()
plt.show()

# LSTM í•™ìŠµì„ ìœ„í•œ ê³„ì ˆ êµ¬ë¶„ ê°’ ìƒì„±
df["Month"] = df["date"].dt.month

def season_category(m):
    if m in [2, 3, 4]:
        return 1  # ë´„
    elif m in [10, 11, 12]:
        return 2  # ê°€ì„Â·ê²¨ìš¸
    else:
        return 0  # ê¸°íƒ€ ê³„ì ˆ

df["season_cat"] = df["Month"].apply(season_category)
df["season_cat"].value_counts()

import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score

# pidë³„ë¡œ ì—°ì†ëœ ë‚ ì§œ(seq_lenì¼)ë¥¼ ë§Œì¡±í•˜ëŠ” ì‹œê³„ì—´ ìœˆë„ìš° ìƒì„±
def make_sequences_by_pid(df_sub, feature_cols, target_col, seq_len=14):
    X_list, y_list = [], []

    for pid, g in df_sub.groupby("pid"):
        g = g.sort_values("date")
        values = g[feature_cols + [target_col]].values
        dates = g["date"].values

        if len(g) <= seq_len:
            continue

        for i in range(len(g) - seq_len):
            window_dates = dates[i:i+seq_len]
            diffs = (window_dates[1:] - window_dates[:-1]).astype("timedelta64[D]").astype(int)
            if not np.all(diffs == 1):
                continue

            X_list.append(values[i:i+seq_len, :-1])
            y_list.append(values[i+seq_len, -1])

    if len(X_list) == 0:
        return None, None
    return np.array(X_list), np.array(y_list)

# ê°€ë²¼ìš´ LSTM ëª¨ë¸ ì •ì˜
def build_light_lstm(input_shape):
    model = models.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(32),
        layers.Dense(8, activation="relu"),
        layers.Dense(1)
    ])
    model.compile(
        loss="mse",
        optimizer=tf.keras.optimizers.Adam(1e-3),
        metrics=["mae"]
    )
    return model

# ì‚¬ìš©í•  í”¼ì²˜/íƒ€ê¹ƒ/ì‹œí€€ìŠ¤ ê¸¸ì´ ë° í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬ ê¸°ì¤€ì¼
feature_cols = ["Tmean", "RH", "WSPD", "TP_mm", "sunlight_era5", "NDVI"]
target_col = "FFDRI"
seq_len = 14
split_date = pd.Timestamp("2024-01-01")
season_list = [1, 2]

quick_results = []

# ë´„(1), ê°€ì„Â·ê²¨ìš¸(2) ê³„ì ˆë³„ë¡œ LSTM í•™ìŠµ ë° í‰ê°€
for target_season in season_list:
    print("=" * 60)
    print("SEASON", target_season)
    print("=" * 60)

    df_season = df[df["season_cat"] == target_season].copy()
    df_season["date"] = pd.to_datetime(df_season["date"])
    df_season = df_season.sort_values(["pid", "date"])

    train_df = df_season[df_season["date"] < split_date].copy()
    test_df = df_season[df_season["date"] >= split_date].copy()

    if len(train_df) < seq_len + 30 or len(test_df) < seq_len + 10:
        print("skip season", target_season)
        continue

    # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])
    test_df[feature_cols] = scaler.transform(test_df[feature_cols])

    # LSTM ì…ë ¥ ì‹œí€€ìŠ¤ ìƒì„±
    X_train, y_train = make_sequences_by_pid(train_df, feature_cols, target_col, seq_len)
    X_test, y_test = make_sequences_by_pid(test_df, feature_cols, target_col, seq_len)

    if X_train is None or X_test is None:
        print("sequence insufficient")
        continue

    print("train:", X_train.shape, "test:", X_test.shape)

    model = build_light_lstm((X_train.shape[1], X_train.shape[2]))
    es = tf.keras.callbacks.EarlyStopping(
        monitor="val_loss", patience=3, restore_best_weights=True
    )

    model.fit(
        X_train, y_train,
        validation_split=0.2,
        epochs=12,
        batch_size=64,
        callbacks=[es],
        verbose=1
    )

    y_pred = model.predict(X_test).ravel()
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print("MAE:", round(mae, 3), "R2:", round(r2, 3))

    quick_results.append({
        "season": target_season,
        "train_seq": len(X_train),
        "test_seq": len(X_test),
        "MAE": mae,
        "R2": r2
    })

quick_results_df = pd.DataFrame(quick_results)
quick_results_df


### LSTM 2ì°¨ëª¨ë¸(DWI ê¸°ë°˜ ì˜ˆì¸¡)

import pandas as pd
import numpy as np

# 1) ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬
df = pd.read_csv("/content/lstm_dataset_with_type.csv")
df["date"] = pd.to_datetime(df["date"])

print(df.columns)
print(df.head())

# 2) ì›”, ê³„ì ˆ ë²”ì£¼ ìƒì„± (ë´„=1, ê°€ì„Â·ê²¨ìš¸=2, ë‚˜ë¨¸ì§€=0)
df["Month"] = df["date"].dt.month

def season_category(m):
    if m in [2, 3, 4]:
        return 1
    elif m in [10, 11, 12]:
        return 2
    else:
        return 0

df["season_cat"] = df["Month"].apply(season_category)

# 3) LSTM ì…ë ¥ í”¼ì²˜ ë° íƒ€ê¹ƒ, ë³´ì¡° ë³€ìˆ˜ ì •ì˜
#    - feature_cols: ê¸°ìƒ + DWI + ê³„ì ˆ
#    - target_col: ë‹¤ìŒë‚  DWI
#    - aux_cols: ì´í›„ FFDRI ê³„ì‚°ìš© FMI, TMI, day_weight
feature_cols = [
    "Tmean", "RH", "WSPD", "TP_mm", "sunlight_era5",
    "DWI",
    "season_cat"
]

target_col = "DWI"
aux_cols   = ["FMI", "TMI", "day_weight"]

def make_sequences(df_sub, feature_cols, target_col, aux_cols, seq_len=60):
    """
    ì—¬ëŸ¬ pidë¥¼ í¬í•¨í•œ DataFrameì„ ë°›ì•„
      X: (N, seq_len, num_features)
      y: (N,)
      aux: (N, len(aux_cols))
    í˜•íƒœë¡œ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±.
    ë‚ ì§œê°€ í•˜ë£¨ì”© ì—°ì†ì¸ êµ¬ê°„ë§Œ ì‚¬ìš©.
    """
    X_list, y_list, aux_list = [], [], []

    for pid, g in df_sub.groupby("pid"):
        g = g.sort_values("date")

        feat_vals   = g[feature_cols].values
        target_vals = g[target_col].values
        aux_vals    = g[aux_cols].values
        dates       = g["date"].values

        if len(g) <= seq_len:
            continue

        for i in range(len(g) - seq_len):
            window_dates = dates[i:i+seq_len+1]
            diffs = (window_dates[1:] - window_dates[:-1]).astype("timedelta64[D]").astype(int)

            # ë‚ ì§œê°€ 1ì¼ ë‹¨ìœ„ë¡œ ì—°ì†ì¸ êµ¬ê°„ë§Œ ì‚¬ìš©
            if not np.all(diffs == 1):
                continue

            X_list.append(feat_vals[i:i+seq_len])
            y_list.append(target_vals[i+seq_len])
            aux_list.append(aux_vals[i+seq_len])

    if len(X_list) == 0:
        return None, None, None

    return np.array(X_list), np.array(y_list), np.array(aux_list)

from sklearn.preprocessing import StandardScaler

# 4) í•™ìŠµ/í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§
seq_len    = 60
split_date = pd.Timestamp("2023-01-01")

train_df = df[df["date"] < split_date].copy()
test_df  = df[df["date"] >= split_date].copy()

x_scaler = StandardScaler()
train_df[feature_cols] = x_scaler.fit_transform(train_df[feature_cols])
test_df[feature_cols]  = x_scaler.transform(test_df[feature_cols])

# 5) ì‹œí€€ìŠ¤ ìƒì„±
X_train, y_train, aux_train = make_sequences(train_df, feature_cols, target_col, aux_cols, seq_len)
X_test,  y_test,  aux_test  = make_sequences(test_df,  feature_cols, target_col, aux_cols, seq_len)

print("X_train:", X_train.shape, "X_test:", X_test.shape)

import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.metrics import mean_absolute_error, r2_score

def build_lstm_model(input_shape):
    """
    2ì¸µ LSTM ê¸°ë°˜ DWI ì˜ˆì¸¡ ëª¨ë¸
      ì…ë ¥: (seq_len, num_features)
      ì¶œë ¥: ë‹¤ìŒë‚  DWI ìŠ¤ì¹¼ë¼
    """
    model = models.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(64, return_sequences=True),
        layers.LSTM(32),
        layers.Dense(16, activation="relu"),
        layers.Dense(1)
    ])

    model.compile(
        loss="mse",
        optimizer=tf.keras.optimizers.Adam(1e-3),
        metrics=["mae"]
    )
    return model

# 6) LSTM í•™ìŠµ
input_shape = (X_train.shape[1], X_train.shape[2])
model = build_lstm_model(input_shape)

es = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=5,
    restore_best_weights=True
)

history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=40,
    batch_size=32,
    callbacks=[es],
    verbose=1
)

from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

# 7) DWI ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
y_pred_dwi = model.predict(X_test).ravel()

y_true_dwi      = y_test.copy()
y_pred_dwi_true = y_pred_dwi.copy()

assert len(y_true_dwi) == len(y_pred_dwi_true) == aux_test.shape[0], \
    f"length mismatch: y_true={len(y_true_dwi)}, y_pred={len(y_pred_dwi_true)}, aux={aux_test.shape[0]}"

print("=== DWI ì˜ˆì¸¡ ì„±ëŠ¥ ===")
print("MAE (DWI):", mean_absolute_error(y_true_dwi, y_pred_dwi_true))
print("R2  (DWI):", r2_score(y_true_dwi, y_pred_dwi_true))

# 8) ì˜ˆì¸¡ DWI ê¸°ë°˜ FFDRI ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€
FMI_test        = aux_test[:, 0]
TMI_test        = aux_test[:, 1]
day_weight_test = aux_test[:, 2]

FFDRI_true = (y_true_dwi      + FMI_test + TMI_test) * day_weight_test
FFDRI_pred = (y_pred_dwi_true + FMI_test + TMI_test) * day_weight_test

print("\n=== FFDRI ì˜ˆì¸¡ ì„±ëŠ¥ (DWI ì˜ˆì¸¡ ê¸°ë°˜) ===")
print("MAE (FFDRI):", mean_absolute_error(FFDRI_true, FFDRI_pred))
print("R2  (FFDRI):", r2_score(FFDRI_true, FFDRI_pred))


### ë”¥ëŸ¬ë‹ ë°ì´í„°ì…‹ í™•ë³´ (ERA5 ê¸°ìƒë³€ìˆ˜ ì¶”ì¶œ)

!pip -q install earthengine-api pandas tqdm

import ee, pandas as pd, datetime as dt, time
from google.colab import files
from tqdm import tqdm

# 1) GEE í”„ë¡œì íŠ¸ ë° ì…ì¶œë ¥ ê²½ë¡œ ì„¤ì •
PROJECT_ID = "solid-time-472606-u0"
CSV_PATH   = "/content/sites_by_type.csv"
OUT_CSV    = "/content/fri_inputs_by_row.csv"

# ERA5 í•´ìƒë„ ê¸°ì¤€ ì¶”ì¶œ ì„¤ì •
SAMPLE_RADIUS_M = 9000
SAMPLE_SCALE_M  = 9000
TILESCALE       = 4

# ë‚ ì§œ êµ¬ê°„ ì„¤ì •
DATE_START = dt.date(2019, 1, 1)
DATE_END   = dt.date(2024, 12, 31)

# íƒ€ì„ì¡´ ê²½ê³„ ë³´ì •(ì¼ ë‹¨ìœ„)
DATE_SHIFT_DAYS = 0

# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°(ì¼ ë‹¨ìœ„)
CHECKPOINT_EVERY = 50

# getInfo ì¬ì‹œë„ ì„¤ì •
MAX_RETRY   = 6
BACKOFF_BASE= 1.5

# 2) GEE ì´ˆê¸°í™”
try:
    ee.Initialize(project=PROJECT_ID)
except Exception:
    ee.Authenticate(project=PROJECT_ID)
    ee.Initialize(project=PROJECT_ID)
print("[GEE] initialized")
ee.data.setDeadline(120000)

# 3) ì¢Œí‘œ CSV ë¡œë“œ ë˜ëŠ” ì—…ë¡œë“œ
try:
    _ = open(CSV_PATH, "r")
except FileNotFoundError:
    print("[UPLOAD] ì„ íƒì°½ì—ì„œ sites_by_type.csv ì—…ë¡œë“œ")
    uploaded = files.upload()
    if "sites_by_type.csv" not in uploaded:
        name = next(iter(uploaded))
        with open(CSV_PATH, "wb") as f:
            f.write(uploaded[name])
        print(f"[INFO] ì—…ë¡œë“œ íŒŒì¼ì„ sites_by_type.csvë¡œ ì €ì¥: {name} â†’ sites_by_type.csv")
    else:
        print("[INFO] sites_by_type.csv ì—…ë¡œë“œ ì™„ë£Œ")

df_sites = pd.read_csv(CSV_PATH, encoding="utf-8-sig", engine="python")
df_sites.columns = [c.strip().lower().replace("\ufeff", "") for c in df_sites.columns]

need = {"lat", "lon"}
if not (need <= set(df_sites.columns)):
    raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½. í˜„ì¬ í—¤ë”: {list(df_sites.columns)} (í•„ìš”: {sorted(need)})")

df_sites["lon"] = pd.to_numeric(df_sites["lon"], errors="coerce")
df_sites["lat"] = pd.to_numeric(df_sites["lat"], errors="coerce")
df_sites = df_sites.dropna(subset=["lon", "lat"]).drop_duplicates(subset=["lon", "lat"]).reset_index(drop=True)
df_sites["pid"] = df_sites.index.astype(int)

print(f"[INFO] sites: {len(df_sites)} rows")
print(df_sites.head())

# 4) ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
def daterange(d0, d1):
    cur = d0
    while cur <= d1:
        yield cur
        cur = cur + dt.timedelta(days=1)

unique_dates = [d for d in daterange(DATE_START, DATE_END)]
if DATE_SHIFT_DAYS != 0:
    unique_dates = [d + dt.timedelta(days=DATE_SHIFT_DAYS) for d in unique_dates]
unique_dates_str = [d.isoformat() for d in unique_dates]
print(f"[INFO] date range: {unique_dates_str[0]} ~ {unique_dates_str[-1]}  (days={len(unique_dates_str)})")

# 5) ERA5 ImageCollection ë° ì¼ë‹¨ìœ„ ë³€ìˆ˜ ë³€í™˜ í•¨ìˆ˜
ERA5_LAND   = ee.ImageCollection("ECMWF/ERA5_LAND/HOURLY")
ERA5_GLOBAL = ee.ImageCollection("ECMWF/ERA5/HOURLY")

def _per_hour_to_vars(im):
    # ì‹œê°„ë³„ ì˜¨ë„, ì´ìŠ¬ì , í’ì†, ê°•ìˆ˜ëŸ‰ì„ Tmean, RH, WSPD, TP_mmë¡œ ë³€í™˜
    T  = im.select("temperature_2m").subtract(273.15).rename("Tmean")
    Td = im.select("dewpoint_temperature_2m").subtract(273.15)
    a, b = 17.625, 243.04
    RH = Td.expression(
        "100*exp(a*Td/(b+Td) - a*T/(b+T))",
        {"a": a, "b": b, "Td": Td, "T": T}
    ).rename("RH")
    U = im.select("u_component_of_wind_10m")
    V = im.select("v_component_of_wind_10m")
    WSPD = U.pow(2).add(V.pow(2)).sqrt().rename("WSPD")
    TP = im.select("total_precipitation").multiply(1000).rename("TP_mm")
    return T.addBands([RH, WSPD, TP])

def _daily_from(ic, date_str):
    d0 = ee.Date(date_str)
    d1 = d0.advance(1, "day")
    hourly = ic.filterDate(d0, d1).map(_per_hour_to_vars)
    Tmean = hourly.select("Tmean").mean()
    RH    = hourly.select("RH").mean()
    WSPD  = hourly.select("WSPD").mean()
    TP    = hourly.select("TP_mm").sum()
    return Tmean.addBands([RH, WSPD, TP])

def daily_era5(date_str):
    # land ìš°ì„ , ëˆ„ë½ ì‹œ globalë¡œ ë³´ì™„
    land  = _daily_from(ERA5_LAND,   date_str)
    globe = _daily_from(ERA5_GLOBAL, date_str)
    fused = land.unmask(globe)
    return fused.set({"date": date_str})

def safe_getInfo(ee_obj):
    # getInfo í˜¸ì¶œ ì‹œ ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„
    for k in range(MAX_RETRY):
        try:
            return ee_obj.getInfo()
        except Exception as e:
            wait = BACKOFF_BASE**k
            print(f"[WARN] getInfo retry {k+1}/{MAX_RETRY} in {wait:.2f}s -> {e}")
            time.sleep(wait)
    return ee_obj.getInfo()

# 6) ì´ì–´ë‹¬ë¦¬ê¸°ìš© ì²´í¬í¬ì¸íŠ¸ ë³µì›
try:
    df_out = pd.read_csv(OUT_CSV)
    done_keys = set(zip(df_out["date"].astype(str), df_out["pid"].astype(int)))
    rows_out = df_out.to_dict("records")
    print(f"[RESUME] existing rows: {len(rows_out)}")
except Exception:
    rows_out = []
    done_keys = set()

# 7) ë‚ ì§œ Ã— ì¢Œí‘œ ë£¨í”„ë¥¼ ëŒë©° ì¼ í‰ê·  ë³€ìˆ˜ ì¶”ì¶œ
total_dates = len(unique_dates_str)
print(f"[INFO] sampling by date Ã— {len(df_sites)} sites  (dates={total_dates})")

processed_since_cp = 0

for idx, d in enumerate(tqdm(unique_dates_str, desc="[sampling by date]"), 1):
    if all((d, int(pid)) in done_keys for pid in df_sites["pid"].tolist()):
        continue

    fc = ee.FeatureCollection([
        ee.Feature(
            ee.Geometry.Point([float(r["lon"]), float(r["lat"])]).buffer(SAMPLE_RADIUS_M),
            {"pid": int(r["pid"]), "lon": float(r["lon"]), "lat": float(r["lat"])}
        )
        for _, r in df_sites.iterrows()
    ])

    img = daily_era5(d)

    reduced = img.reduceRegions(
        collection=fc,
        reducer=ee.Reducer.mean(),
        scale=SAMPLE_SCALE_M,
        tileScale=TILESCALE
    ).map(lambda f: f.set({"date": d}))

    data  = safe_getInfo(reduced)
    feats = data.get("features", []) if data else []

    for f in feats:
        p = f.get("properties", {})
        key = (str(p.get("date")), int(p.get("pid")))
        if key in done_keys:
            continue
        rows_out.append({
            "date":  p.get("date"),
            "pid":   p.get("pid"),
            "lon":   p.get("lon"),
            "lat":   p.get("lat"),
            "Tmean": p.get("Tmean_mean", p.get("Tmean")),
            "RH":    p.get("RH_mean",    p.get("RH")),
            "WSPD":  p.get("WSPD_mean",  p.get("WSPD")),
            "TP_mm": p.get("TP_mm_mean", p.get("TP_mm")),
        })
        done_keys.add(key)

    processed_since_cp += 1
    if processed_since_cp >= CHECKPOINT_EVERY or idx == total_dates:
        tmp = pd.DataFrame(rows_out).sort_values(["date", "pid"]).reset_index(drop=True)
        tmp.to_csv(OUT_CSV, index=False, encoding="utf-8-sig")
        print(f"[CP] saved {len(tmp)} rows at {d}  ({idx}/{total_dates})")
        processed_since_cp = 0

# 8) ìµœì¢… ì €ì¥ ë° í’ˆì§ˆ ì ê²€
df_out = pd.DataFrame(rows_out).sort_values(["date", "pid"]).reset_index(drop=True)
df_out.to_csv(OUT_CSV, index=False, encoding="utf-8-sig")

num_total   = len(df_out)
num_dates   = df_out["date"].nunique()
num_sites   = df_out["pid"].nunique()
num_all_nan = df_out[["Tmean", "RH", "WSPD", "TP_mm"]].isna().all(axis=1).sum()
num_any_nan = df_out[["Tmean", "RH", "WSPD", "TP_mm"]].isna().any(axis=1).sum()

print(f"[SAVED] {OUT_CSV}  rows={num_total}  (sites={num_sites}, dates={num_dates})")
print(f"[QC] all-NaN rows = {num_all_nan} / any-NaN rows = {num_any_nan}")

bad = df_sites[
    (df_sites["lat"] < 33) | (df_sites["lat"] > 39) |
    (df_sites["lon"] < 124) | (df_sites["lon"] > 132)
]
if len(bad):
    print("[WARN] KR bounds outliers (first 5):")
    print(bad[["pid", "lat", "lon"]].head())

files.download(OUT_CSV)


### ì‚°ë¶ˆ í”¼í•´ê·œëª¨ ê¸°ë°˜ FFDRI_new ì˜ˆì¸¡ ì‹œë„ (RandomForest, ElasticNet)

import pandas as pd
import numpy as np

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import spearmanr

import matplotlib.pyplot as plt

# 1) ì‚°ë¶ˆ í”¼í•´ê·œëª¨ ë°ì´í„° ë¡œë“œ
DATA_PATH = "/content/wildfire_dataset_GBDG_FFDRI.csv"
df = pd.read_csv(DATA_PATH)

print("Shape:", df.shape)
df.head()

df["date"] = pd.to_datetime(df["date"])
df = df.sort_values("date").reset_index(drop=True)

# 2) ë¡œê·¸ ìŠ¤ì¼€ì¼ íƒ€ê¹ƒ ìƒì„±
target_col = "í”¼í•´ë©´ì _í•©ê³„"
df["target_log"] = np.log(df[target_col] + 0.01)

# 3) ì…ë ¥ í”¼ì²˜ ì„¤ì •
feature_cols = [
    "tmean", "rh", "eh", "wspd", "tp_mm", "rne",
    "sunlight_era5", "NDVI", "pdwi", "dwi",
    "fmi", "tmi", "ffdri"
]

X = df[feature_cols]
y = df["target_log"]

print("ì…ë ¥ë³€ìˆ˜:", feature_cols)
print("X shape:", X.shape, "y shape:", y.shape)

from sklearn.model_selection import train_test_split

# 4) í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42, shuffle=True
)

X_train, X_valid, y_train, y_valid = train_test_split(
    X_temp, y_temp, test_size=0.1765, random_state=42, shuffle=True
)

print("Train:", X_train.shape)
print("Valid:", X_valid.shape)
print("Test:", X_test.shape)

# 5) RandomForest íšŒê·€ ëª¨ë¸ í•™ìŠµ
rf = RandomForestRegressor(
    n_estimators=300,
    max_depth=6,
    min_samples_leaf=5,
    random_state=42,
    n_jobs=-1
)

rf.fit(X_train, y_train)

def evaluate(model, X, y_true, name=""):
    """
    í”¼íŒ…ëœ ëª¨ë¸ê³¼ X, y_trueë¥¼ ë°›ì•„
    ë¡œê·¸ ìŠ¤ì¼€ì¼ê³¼ ì› ìŠ¤ì¼€ì¼ì—ì„œ ì„±ëŠ¥ í‰ê°€.
    """
    y_pred_log = model.predict(X)

    mae_log = mean_absolute_error(y_true, y_pred_log)
    rmse_log = np.sqrt(mean_squared_error(y_true, y_pred_log))
    r2_log   = r2_score(y_true, y_pred_log)
    spear_log, _ = spearmanr(y_true, y_pred_log)

    y_true_raw = np.exp(y_true) - 0.01
    y_pred_raw = np.exp(y_pred_log) - 0.01

    mae = mean_absolute_error(y_true_raw, y_pred_raw)
    rmse = np.sqrt(mean_squared_error(y_true_raw, y_pred_raw))
    r2   = r2_score(y_true_raw, y_pred_raw)
    spear, _ = spearmanr(y_true_raw, y_pred_raw)

    print(f"\n=== {name} í‰ê°€ ===")
    print(f"[Log]  MAE={mae_log:.4f}, RMSE={rmse_log:.4f}, R2={r2_log:.4f}, Spearman={spear_log:.4f}")
    print(f"[Raw]  MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}, Spearman={spear:.4f}")

    return y_pred_raw, y_pred_log

# 6) RandomForest ì„±ëŠ¥ í‰ê°€
train_pred, _ = evaluate(rf, X_train, y_train, "Train")
valid_pred, _ = evaluate(rf, X_valid, y_valid, "Valid")
test_pred,  _ = evaluate(rf, X_test,  y_test,  "Test")

# 7) ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ìœ„í—˜ë„ ì¸ë±ìŠ¤ ìƒì„±
all_pred_log = rf.predict(X)
all_pred_raw = np.exp(all_pred_log) - 0.01

df["pred_area"] = all_pred_raw

min_val, max_val = df["pred_area"].min(), df["pred_area"].max()
df["danger_index"] = 100 * (df["pred_area"] - min_val) / (max_val - min_val + 1e-9)

print(df[["date", target_col, "pred_area", "danger_index"]].head())

plt.hist(df["danger_index"], bins=30)
plt.title("Danger Index Distribution (0~100)")
plt.xlabel("Index")
plt.ylabel("Count")
plt.show()

# 8) ElasticNet ê¸°ë°˜ ëŒ€ì•ˆ ëª¨ë¸ í•™ìŠµ
DATA_PATH = "/content/wildfire_dataset_GBDG_FFDRI.csv"
df = pd.read_csv(DATA_PATH)

print("Shape:", df.shape)
df.head()

X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42, shuffle=True
)

X_train, X_valid, y_train, y_valid = train_test_split(
    X_temp, y_temp, test_size=0.1765, random_state=42, shuffle=True
)

print("Train:", X_train.shape)
print("Valid:", X_valid.shape)
print("Test:", X_test.shape)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_valid_scaled = scaler.transform(X_valid)
X_test_scaled  = scaler.transform(X_test)
X_all_scaled   = scaler.transform(X)

from sklearn.linear_model import ElasticNetCV

enet = ElasticNetCV(
    l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9],
    alphas=np.logspace(-4, 2, 50),
    cv=5,
    random_state=42
)

enet.fit(X_train_scaled, y_train)

print("Best alpha:", enet.alpha_)
print("Best l1_ratio:", enet.l1_ratio_)

def eval_split(model, Xs, y_true, name=""):
    """
    ElasticNet ëª¨ë¸ì˜ ë¡œê·¸/ì› ìŠ¤ì¼€ì¼ í‰ê°€.
    """
    y_pred_log = model.predict(Xs)

    mae_log = mean_absolute_error(y_true, y_pred_log)
    rmse_log = np.sqrt(mean_squared_error(y_true, y_pred_log))
    r2_log   = r2_score(y_true, y_pred_log)
    spear_log, _ = spearmanr(y_true, y_pred_log)

    y_true_raw = np.exp(y_true) - 0.01
    y_pred_raw = np.exp(y_pred_log) - 0.01

    mae = mean_absolute_error(y_true_raw, y_pred_raw)
    rmse = np.sqrt(mean_squared_error(y_true_raw, y_pred_raw))
    r2   = r2_score(y_true_raw, y_pred_raw)
    spear, _ = spearmanr(y_true_raw, y_pred_raw)

    print(f"\n=== {name} í‰ê°€ ===")
    print(f"[Log] MAE={mae_log:.4f}, RMSE={rmse_log:.4f}, R2={r2_log:.4f}, Spearman={spear_log:.4f}")
    print(f"[Raw] MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}, Spearman={spear:.4f}")

eval_split(enet, X_train_scaled, y_train, "Train")
eval_split(enet, X_valid_scaled, y_valid, "Valid")
eval_split(enet, X_test_scaled,  y_test,  "Test")

df["pred_log"]  = enet.predict(X_all_scaled)
df["pred_area"] = np.exp(df["pred_log"]) - 0.01

min_v = df["pred_area"].min()
max_v = df["pred_area"].max()
df["danger_index"] = 100 * (df["pred_area"] - min_v) / (max_v - min_v + 1e-9)

df[["date", target_col, "pred_area", "danger_index"]].head()


### FFDRI_new ì‹ ì¶”ì¶œ (íšŒê·€ê³„ìˆ˜ ê¸°ë°˜ ìƒˆ ì§€ìˆ˜ ì •ì˜)

import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.metrics import r2_score, mean_squared_error

# 1) CSV ì—…ë¡œë“œ ë° ë¡œë“œ
CSV_PATH = None

try:
    from google.colab import files
    uploaded = files.upload()
    CSV_PATH = list(uploaded.keys())[0]
    print("ì—…ë¡œë“œí•œ íŒŒì¼ëª…:", CSV_PATH)
except Exception:
    print("Colabì´ ì•„ë‹ˆë©´ CSV_PATHë¥¼ ì§ì ‘ ì§€ì •í•˜ì„¸ìš”.")
    CSV_PATH = "lstm_dataset.csv"

df = pd.read_csv(CSV_PATH)

print("\nCSV ì»¬ëŸ¼ ëª©ë¡:")
print(df.columns.tolist())

# 2) FFDRI_new êµ¬ì„±ì— ì‚¬ìš©í•  í”¼ì²˜ì™€ íƒ€ê¹ƒ ì§€ì •
feature_cols = [
    "DWI",
    "FMI",
    "TMI",
    "sunlight_era5",
    "NDVI"
]

target_col = "FFDRI"

missing = [c for c in feature_cols + [target_col] if c not in df.columns]
if len(missing) > 0:
    raise ValueError(f"CSVì— ì—†ëŠ” ì»¬ëŸ¼ì´ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì´ë¦„ì„ CSVì— ë§ê²Œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤: {missing}")

# 3) ê²°ì¸¡ ì œê±° ë° í•™ìŠµ ë°ì´í„° êµ¬ì„±
df_clean = df.dropna(subset=feature_cols + [target_col]).copy()

X = df_clean[feature_cols].copy()
y = df_clean[target_col].values

print(f"\nì‚¬ìš© ë°ì´í„° ê°œìˆ˜: {len(df_clean)} í–‰")

# 4) í‘œì¤€í™” í›„ Ridge íšŒê·€ ëª¨ë¸ í•™ìŠµ
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

reg = Ridge(alpha=1.0, random_state=42)
reg.fit(X_std, y)

y_pred = reg.predict(X_std)
r2   = r2_score(y, y_pred)
mse  = mean_squared_error(y, y_pred)
rmse = np.sqrt(mse)

print("\nëª¨ë¸ ì„±ëŠ¥ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)")
print(f"RÂ²  : {r2:.4f}")
print(f"RMSE: {rmse:.4f}")

# 5) í‘œì¤€í™” ê³µê°„ ê³„ìˆ˜ë¥¼ ì› ë‹¨ìœ„ ê³„ìˆ˜ë¡œ ë³€í™˜
beta0      = reg.intercept_
betas_std  = reg.coef_
means      = scaler.mean_
scales     = scaler.scale_

coeff_orig     = betas_std / scales
intercept_orig = beta0 - np.sum(betas_std * means / scales)

# 6) ìµœì¢… FFDRI_new ì„ í˜•ì‹ ì¶œë ¥
print("\nì§€ì—­íŠ¹í™” FFDRI_new ê³µì‹ (ì› ë‹¨ìœ„)")
formula = f"FFDRI_new = {intercept_orig:.4f}"
for name, c in zip(feature_cols, coeff_orig):
    sign = " + " if c >= 0 else " - "
    formula += f"{sign}{abs(c):.4f} * {name}"
print(formula)

coef_table = pd.DataFrame({
    "feature": feature_cols,
    "coef_original_space": coeff_orig,
    "coef_standard_space": betas_std
})
print("\nê³„ìˆ˜ ìƒì„¸í‘œ")
print(coef_table)

print("\nê°„ë‹¨ í•´ì„")
for name, c in zip(feature_cols, coeff_orig):
    print(f"{name}: ê³„ìˆ˜ {c:.4f}")

# 7) ìƒˆ FFDRI_new ê°’ ê³„ì‚° í›„ ì›ë³¸ dfì— ë³‘í•©
ffdri_new = intercept_orig
for name, c in zip(feature_cols, coeff_orig):
    ffdri_new += c * df_clean[name].values

df_clean["FFDRI_new"] = ffdri_new

df_out = df.copy()
df_out = df_out.merge(
    df_clean[["FFDRI_new"]],
    left_index=True,
    right_index=True,
    how="left"
)

output_name = "ffdri_with_new_index.csv"
df_out.to_csv(output_name, index=False)
print("\nìƒˆ ì§€ìˆ˜(FFDRI_new)ë¥¼ í¬í•¨í•œ CSV ì €ì¥ ì™„ë£Œ:", output_name)

try:
    from google.colab import files
    files.download(output_name)
except Exception:
    pass


### LSTM vs GRU ëª¨ë¸ í•™ìŠµ ë° ê³„ì ˆë³„ ë¹„êµ (FFDRI_new ì˜ˆì¸¡)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

try:
    from google.colab import files
    IN_COLAB = True
except ImportError:
    IN_COLAB = False

print("TensorFlow version:", tf.__version__)

# 1) FFDRI_newê°€ í¬í•¨ëœ CSV ë¡œë“œ
if IN_COLAB:
    print("FFDRI_newê°€ í¬í•¨ëœ lstm_dataset.csvë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    uploaded = files.upload()
    CSV_PATH = list(uploaded.keys())[0]
else:
    CSV_PATH = "lstm_dataset.csv"

df = pd.read_csv(CSV_PATH)
print("ì›ë³¸ ë°ì´í„° shape:", df.shape)

if "date" not in df.columns:
    raise ValueError("CSVì— 'date' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

df["date"] = pd.to_datetime(df["date"])

# ì¢Œí‘œ ID ì»¬ëŸ¼ ê²°ì •
if "pid" in df.columns:
    id_col = "pid"
elif "site_id" in df.columns:
    id_col = "site_id"
else:
    id_col = "pid"
    df[id_col] = 0

# season ì»¬ëŸ¼ ì—†ìœ¼ë©´ monthì—ì„œ ìƒì„±
if "season" not in df.columns:
    df["month"] = df["date"].dt.month

    def season_from_month(m):
        if m in [2, 3, 4]:
            return "spring"
        elif m in [10, 11, 12]:
            return "fall"
        else:
            return "other"

    df["season"] = df["month"].apply(season_from_month)

print("season ë¶„í¬:")
print(df["season"].value_counts())

# FFDRI_new íƒ€ê¹ƒ í™•ì¸
TARGET_COL = "FFDRI_new"
if TARGET_COL not in df.columns:
    raise ValueError("CSVì— 'FFDRI_new' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

# 2) ë™ì  ì…ë ¥ feature í›„ë³´ ì„¤ì • (íƒ€ê¹ƒì€ ì œì™¸)
candidate_cols = [
    "Tmean", "RH", "WSPD", "TP_mm",
    "sunlight_era5", "DWI", "FMI", "TMI",
    "NDVI", "day_weight", "FFDRI"
]

dynamic_features = []
for c in candidate_cols:
    if c in df.columns and c != TARGET_COL:
        dynamic_features.append(c)

print("ì‚¬ìš©í•  ì…ë ¥ feature:", dynamic_features)
n_features = len(dynamic_features)
if n_features == 0:
    raise ValueError("dynamic_featuresê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

# 3) ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„± í•¨ìˆ˜
LOOKBACK = 14
HORIZON  = 7

def create_sequences(sub_df, lookback, horizon,
                     feature_cols, id_col, target_col=TARGET_COL):
    """
    idë³„ë¡œ ì •ë ¬ëœ ì‹œê³„ì—´ì—ì„œ
      X: (N, lookback, n_features)
      y: (N, horizon)
      meta: pid, base_date, lat, lon
    ìƒì„±.
    """
    X_list, y_list, meta_list = [], [], []

    for pid, g in sub_df.groupby(id_col):
        g = g.sort_values("date").reset_index(drop=True)

        lat_val = g["lat"].iloc[0] if "lat" in g.columns else np.nan
        lon_val = g["lon"].iloc[0] if "lon" in g.columns else np.nan

        feats  = g[feature_cols].values
        target = g[target_col].values

        if len(g) < lookback + horizon:
            continue

        for i in range(len(g) - lookback - horizon + 1):
            X_list.append(feats[i:i+lookback])
            y_list.append(target[i+lookback:i+lookback+horizon])

            base_idx  = i + lookback - 1
            base_date = g.loc[base_idx, "date"]

            meta_list.append({
                "pid": pid,
                "base_date": base_date,
                "lat": lat_val,
                "lon": lon_val
            })

    if not X_list:
        X = np.empty((0, lookback, len(feature_cols)))
        y = np.empty((0, horizon))
        meta_df = pd.DataFrame(columns=["pid", "base_date", "lat", "lon"])
    else:
        X = np.array(X_list, dtype="float32")
        y = np.array(y_list, dtype="float32")
        meta_df = pd.DataFrame(meta_list)

    return X, y, meta_df

# 4) ê³„ì ˆë³„ ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§
def build_splits_for_season(df_all, season_name,
                            feature_cols, id_col):
    """
    íŠ¹ì • seasonì— ëŒ€í•´ train/val/test ë¶„í• ,
    ì‹œí€€ìŠ¤ ìƒì„± ë° ìŠ¤ì¼€ì¼ë§ê¹Œì§€ ìˆ˜í–‰.
    """
    sub = df_all[df_all["season"] == season_name].copy()
    if sub.empty:
        print(f"[{season_name}] season ë°ì´í„° ì—†ìŒ")
        return None

    sub = sub.sort_values(["date", id_col])

    train_df = sub[sub["date"] < pd.Timestamp("2023-01-01")]
    val_df   = sub[
        (sub["date"] >= pd.Timestamp("2023-01-01")) &
        (sub["date"] <  pd.Timestamp("2024-01-01"))
    ]
    test_df  = sub[sub["date"] >= pd.Timestamp("2024-01-01")]

    X_train, y_train, meta_train = create_sequences(
        train_df, LOOKBACK, HORIZON, feature_cols, id_col
    )
    X_val,   y_val,   meta_val   = create_sequences(
        val_df,   LOOKBACK, HORIZON, feature_cols, id_col
    )
    X_test,  y_test,  meta_test  = create_sequences(
        test_df,  LOOKBACK, HORIZON, feature_cols, id_col
    )

    print(f"[{season_name}] samples: train {X_train.shape[0]} / val {X_val.shape[0]} / test {X_test.shape[0]}")

    scaler = StandardScaler()
    n_feat = len(feature_cols)

    if X_train.shape[0] > 0:
        Xtr_2d = X_train.reshape(-1, n_feat)
        X_train_scaled = scaler.fit_transform(Xtr_2d).reshape(X_train.shape)
    else:
        X_train_scaled = X_train

    def transform_X(X):
        if X.shape[0] == 0:
            return X
        X2d = X.reshape(-1, n_feat)
        return scaler.transform(X2d).reshape(X.shape)

    X_val_scaled  = transform_X(X_val)
    X_test_scaled = transform_X(X_test)

    return {
        "X_train": X_train_scaled, "y_train": y_train, "meta_train": meta_train,
        "X_val":   X_val_scaled,   "y_val":   y_val,   "meta_val":   meta_val,
        "X_test":  X_test_scaled,  "y_test":  y_test,  "meta_test":  meta_test,
        "scaler":  scaler
    }

# 5) LSTM/GRU ëª¨ë¸ ì •ì˜
def build_lstm_model(timesteps, n_features, horizon):
    model = Sequential([
        LSTM(64, return_sequences=True, input_shape=(timesteps, n_features)),
        LSTM(32),
        Dropout(0.2),
        Dense(horizon)
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(1e-3),
        loss="mse",
        metrics=["mae"]
    )
    return model

def build_gru_model(timesteps, n_features, horizon):
    model = Sequential([
        GRU(64, return_sequences=True, input_shape=(timesteps, n_features)),
        GRU(32),
        Dropout(0.2),
        Dense(horizon)
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(1e-3),
        loss="mse",
        metrics=["mae"]
    )
    return model

# 6) í•™ìŠµ ì´ë ¥ ì‹œê°í™”
def plot_history(history, season, model_name):
    h = history.history
    plt.figure(figsize=(7, 4))
    plt.plot(h["loss"], label="train_loss")
    if "val_loss" in h:
        plt.plot(h["val_loss"], label="val_loss")
    plt.title(f"{season} - {model_name} Training History")
    plt.xlabel("Epoch")
    plt.ylabel("MSE loss")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

# 7) í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ í‰ê°€
def evaluate_on_test(model, X_test, y_test, season, model_name):
    """
    t+1, t+1~t+7 êµ¬ê°„ì—ì„œ RMSE, MAE, ìƒê´€ê³„ìˆ˜ ê³„ì‚°.
    """
    if X_test.shape[0] == 0:
        print(f"[{season}][{model_name}] í…ŒìŠ¤íŠ¸ì…‹ ì—†ìŒ")
        return {}

    y_pred = model.predict(X_test, verbose=0)
    assert y_pred.shape == y_test.shape

    y_true_1 = y_test[:, 0]
    y_pred_1 = y_pred[:, 0]
    rmse_1 = np.sqrt(mean_squared_error(y_true_1, y_pred_1))
    mae_1  = mean_absolute_error(y_true_1, y_pred_1)
    corr_1 = np.corrcoef(y_true_1, y_pred_1)[0, 1]

    rmse_all = np.sqrt(mean_squared_error(y_test.reshape(-1), y_pred.reshape(-1)))
    mae_all  = mean_absolute_error(y_test.reshape(-1), y_pred.reshape(-1))

    print(f"\n[{season}][{model_name}] í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥")
    print("  (t+1)  RMSE:", rmse_1, " MAE:", mae_1, " Corr:", corr_1)
    print("  (1~7) RMSE:", rmse_all, " MAE:", mae_all)

    return {
        "rmse_1": rmse_1, "mae_1": mae_1, "corr_1": corr_1,
        "rmse_all": rmse_all, "mae_all": mae_all
    }

# 8) ê³„ì ˆë³„ LSTM vs GRU ë¹„êµ ë£¨í”„
EPOCHS = 100
BATCH  = 64

summary_rows = []

for season_name in ["spring", "fall"]:
    print(f"\nSeason: {season_name}")

    splits = build_splits_for_season(df, season_name, dynamic_features, id_col)
    if splits is None:
        continue

    X_train = splits["X_train"]
    y_train = splits["y_train"]
    X_val   = splits["X_val"]
    y_val   = splits["y_val"]
    X_test  = splits["X_test"]
    y_test  = splits["y_test"]

    if X_train.shape[0] == 0:
        print(f"[{season_name}] í•™ìŠµ ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ ì—†ìŒ")
        continue

    for model_name, builder in [("LSTM", build_lstm_model), ("GRU", build_gru_model)]:
        print(f"\n{season_name} / {model_name} í•™ìŠµ")

        model = builder(LOOKBACK, n_features, HORIZON)
        model.summary()

        callbacks = [
            EarlyStopping(
                monitor="val_loss",
                patience=10,
                restore_best_weights=True
            )
        ]

        if X_val.shape[0] > 0:
            history = model.fit(
                X_train, y_train,
                epochs=EPOCHS,
                batch_size=BATCH,
                validation_data=(X_val, y_val),
                callbacks=callbacks,
                verbose=1
            )
        else:
            history = model.fit(
                X_train, y_train,
                epochs=EPOCHS,
                batch_size=BATCH,
                callbacks=callbacks,
                verbose=1
            )

        plot_history(history, season_name, model_name)

        h = history.history
        train_loss = h["loss"][-1]
        train_mae  = h["mae"][-1]
        if "val_loss" in h:
            val_loss = h["val_loss"][-1]
            val_mae  = h["val_mae"][-1]
        else:
            val_loss = np.nan
            val_mae  = np.nan

        metrics_test = evaluate_on_test(model, X_test, y_test, season_name, model_name)

        summary_rows.append({
            "season": season_name,
            "model": model_name,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "train_mae": train_mae,
            "val_mae": val_mae,
            **metrics_test
        })

# 9) LSTM vs GRU ìš”ì•½ í…Œì´ë¸”
if summary_rows:
    summary_df = pd.DataFrame(summary_rows)
    print("\nLSTM vs GRU í•™ìŠµ/ê²€ì • ë¹„êµ ìš”ì•½")
    print(summary_df)

    if IN_COLAB:
        summary_df.to_csv("lstm_gru_ffdri_new_summary.csv",
                          index=False, encoding="utf-8-sig")
        files.download("lstm_gru_ffdri_new_summary.csv")
else:
    print("ìš”ì•½í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")


### 2025ë…„ 4ì›” 28ì¼ ëŒ€êµ¬ í•¨ì§€ì‚° ëŒ€í˜•ì‚°ë¶ˆì¼ì ëª¨ë¸ í…ŒìŠ¤íŠ¸ (GEE ê¸°ë°˜ GRU ì˜ˆì¸¡)

import io
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

from sklearn.preprocessing import StandardScaler

try:
    from google.colab import files
    IN_COLAB = True
except ImportError:
    IN_COLAB = False
    files = None

import tensorflow as tf
from tensorflow.keras.models import load_model

import ee

# 0) GEE ì´ˆê¸°í™”
PROJECT_ID = "solid-time-472606-u0"

try:
    ee.Initialize(project=PROJECT_ID)
except Exception:
    ee.Authenticate()
    ee.Initialize(project=PROJECT_ID)

print("[GEE] initialized")

# 1) ì‚¬ìš©ì ì…ë ¥ (ì˜ˆì¸¡ ë‚ ì§œ, ì¢Œí‘œ, ì‹œì¦Œ ë“±)
FORECAST_DATE = "2025-04-28"
FORECAST_DATE = datetime.strptime(FORECAST_DATE, "%Y-%m-%d").date()

NEW_LAT = 35.9181780434288
NEW_LON = 128.566700685493
NEIGHBOR_KM = 5

SEASON   = "spring"
TIMEZONE = "Asia/Seoul"

LOOKBACK = 14
HORIZON  = 7

OUTPUT_CSV = f"ffdri_new_forecast_NEWPOINT_{SEASON}_{FORECAST_DATE}_7d.csv"

print("ì…ë ¥ ì¢Œí‘œ:", NEW_LAT, NEW_LON)
print("ì„ íƒ ì‹œì¦Œ:", SEASON)
print("ì˜ˆì¸¡ ê¸°ì¤€ì¼:", FORECAST_DATE)

# 2) lstm_dataset.csv ë¡œë“œ
if IN_COLAB:
    print("lstm_dataset.csv ì—…ë¡œë“œ")
    uploaded = files.upload()
    csv_name = list(uploaded.keys())[0]
else:
    csv_name = "lstm_dataset.csv"

df = pd.read_csv(csv_name, parse_dates=["date"])
df = df.sort_values(["pid", "date"]).reset_index(drop=True)

# season ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì›” ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
if "season" not in df.columns:
    df["month"] = df["date"].dt.month
    df["season"] = df["month"].apply(
        lambda m: "spring" if m in [2, 3, 4]
        else ("fall" if m in [10, 11, 12] else "other")
    )

df = df[df["season"] == SEASON].copy()
if df.empty:
    raise ValueError(f"{SEASON} ì‹œì¦Œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

df["doy"] = df["date"].dt.dayofyear

# 3) Feature / Target ì •ì˜
TARGET = "FFDRI_new"
if TARGET not in df.columns:
    raise ValueError("FFDRI_new ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

candidate_features = [
    "Tmean", "RH", "WSPD", "TP_mm",
    "sunlight_era5", "DWI", "FMI", "TMI",
    "NDVI", "day_weight", "FFDRI"
]

FEATURES = [c for c in candidate_features if c in df.columns and c != TARGET]
n_features = len(FEATURES)

if n_features == 0:
    raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ Featureê°€ ì—†ìŠµë‹ˆë‹¤. candidate_featuresë¥¼ í™•ì¸í•˜ì„¸ìš”.")

print("ì‚¬ìš© Features:", FEATURES)
print("n_features:", n_features)

# 4) GRU í•™ìŠµê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ êµ¬ì„±
def make_sequences(sub_df):
    X_list, y_list = [], []

    for pid, g in sub_df.groupby("pid"):
        g = g.sort_values("date").reset_index(drop=True)

        X_raw = g[FEATURES].values
        y_raw = g[TARGET].values

        if len(g) < LOOKBACK + HORIZON:
            continue

        for i in range(LOOKBACK, len(g) - HORIZON + 1):
            X_list.append(X_raw[i-LOOKBACK:i])
            y_list.append(y_raw[i:i+HORIZON])

    if len(X_list) == 0:
        return None, None

    return np.array(X_list, dtype="float32"), np.array(y_list, dtype="float32")

X_all, y_all = make_sequences(df)
if X_all is None:
    raise ValueError("ë°ì´í„°ê°€ ë¶€ì¡±í•´ ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

N = X_all.shape[0]
idx = int(N * 0.8)
X_train = X_all[:idx]

scaler = StandardScaler()
X_train_2d = X_train.reshape(-1, n_features)
scaler.fit(X_train_2d)

def transform_X(z):
    z2 = z.reshape(-1, n_features)
    z2 = scaler.transform(z2)
    return z2.reshape(z.shape)

# 5) ì£¼ë³€ í•™ìŠµ ì¢Œí‘œ íƒìƒ‰ (NEIGHBOR_KM ì´ë‚´ pid ì„ íƒ)
pid_locs = df.groupby("pid")[["lat", "lon"]].first().reset_index()

def haversine(lat1, lon1, lat2, lon2):
    R = 6371.0
    dlat = np.radians(lat2 - lat1)
    dlon = np.radians(lon2 - lon1)
    a = np.sin(dlat/2)**2 + np.cos(np.radians(lat1))*np.cos(np.radians(lat2))*np.sin(dlon/2)**2
    return 2 * R * np.arcsin(np.sqrt(a))

pid_locs["dist_km"] = pid_locs.apply(
    lambda r: haversine(NEW_LAT, NEW_LON, r["lat"], r["lon"]),
    axis=1
)

near = pid_locs[pid_locs["dist_km"] <= NEIGHBOR_KM]

if near.empty:
    print("ì£¼ë³€ ì¢Œí‘œ ì—†ìŒ â†’ ì „ì²´ pid ì‚¬ìš©")
    near_pids = pid_locs["pid"].tolist()
else:
    print(len(near), "ê°œ ì¢Œí‘œ ì‚¬ìš©")
    near_pids = near["pid"].tolist()

# 6) DOY ê¸°ë°˜ ê¸°í›„ í´ë¼ì´ëª¨(í‰ë…„ê°’) ê³„ì‚° (pid, doy ê¸°ì¤€)
clim_features = df.groupby(["pid", "doy"])[FEATURES].mean()

def get_climo_vector(date_obj):
    doy = date_obj.timetuple().tm_yday
    rows = []

    for pid in near_pids:
        key = (pid, doy)
        if key in clim_features.index:
            rows.append(clim_features.loc[key].values)

    if rows:
        return np.mean(rows, axis=0).astype("float32")
    return clim_features.mean().values.astype("float32")

# 7) GEE ERA5-Landì—ì„œ í•˜ë£¨ í‰ê·  ê¸°ìƒìš”ì†Œ ê°€ì ¸ì˜¤ê¸°
def fetch_weather_by_date_gee(lat, lon, date_obj):
    start = ee.Date(date_obj.strftime("%Y-%m-%d"))
    end   = start.advance(1, "day")

    col = ee.ImageCollection("ECMWF/ERA5_LAND/HOURLY").filterDate(start, end)

    img_mean = col.select([
        "temperature_2m",
        "dewpoint_temperature_2m",
        "u_component_of_wind_10m",
        "v_component_of_wind_10m"
    ]).mean()

    img_sum_tp = col.select("total_precipitation").sum().rename("total_precipitation_sum")

    img_agg = img_mean.addBands(img_sum_tp)

    pt = ee.Geometry.Point(lon, lat)
    vals = img_agg.sample(pt, scale=9000).first().getInfo()["properties"]

    T_K  = vals["temperature_2m"]
    Td_K = vals["dewpoint_temperature_2m"]
    u10  = vals["u_component_of_wind_10m"]
    v10  = vals["v_component_of_wind_10m"]
    tp_m = vals["total_precipitation_sum"]

    T_C  = T_K  - 273.15
    Td_C = Td_K - 273.15

    wspd = float((u10**2 + v10**2) ** 0.5)
    tp_mm = float(tp_m * 1000.0)

    import math
    def esat(T):
        return 6.112 * math.exp((17.67*T)/(T+243.5))

    e  = esat(Td_C)
    es = esat(T_C)
    rh = float(100.0 * e / es) if es > 0 else np.nan

    result = {
        "T_mean":   float(T_C),
        "T_max":    float(T_C),
        "RH_mean":  rh,
        "WSPD_mean": wspd,
        "TP_sum":   tp_mm,
    }
    return result

# 8) DWI ê³„ì‚° í•¨ìˆ˜ (ê³„ì ˆë³„ ì‹)
def pre_spring(T, RH, W):
    return 1/(1+np.exp(2.706 + 0.088*T - 0.055*RH - 0.023*RH - 0.104*W))

def pre_fall(T, RH, W):
    return 1/(1+np.exp(1.099 + 0.117*T - 0.069*RH - 0.182*W))

def compute_DWI(weather):
    if SEASON == "spring":
        pre = pre_spring(weather["T_max"], weather["RH_mean"], weather["WSPD_mean"])
    else:
        pre = pre_fall(weather["T_max"], weather["RH_mean"], weather["WSPD_mean"])

    tp = weather["TP_sum"]
    if tp >= 10:
        rne = 0.3
    elif tp >= 1:
        rne = 0.7
    else:
        rne = 1.0
    return pre * rne

# 9) FFDRI_new ê³„ì‚°ì‹ (íšŒê·€ ê¸°ë°˜ ê³µì‹)
def compute_FFDRI_new(dwi, fmi, tmi, sunlight, ndvi):
    return (
        5.3012
        + 6.6232*dwi
        + 0.7657*fmi
        + 1.0994*tmi
        + 0.0906*sunlight
        - 10.6796*ndvi
    )

# 10) LOOKBACK ì‹œí€€ìŠ¤ êµ¬ì„± (FORECAST_DATE ê¸°ì¤€)
feat_index = {name: i for i, name in enumerate(FEATURES)}
seq_rows = []

for offset in range(LOOKBACK-1, 0, -1):
    d = FORECAST_DATE - timedelta(days=offset)
    vec = get_climo_vector(d)
    seq_rows.append(vec)

today_vec = get_climo_vector(FORECAST_DATE)

weather_input = fetch_weather_by_date_gee(NEW_LAT, NEW_LON, FORECAST_DATE)
dwi_today = compute_DWI(weather_input)

if "Tmean" in feat_index:
    today_vec[feat_index["Tmean"]] = weather_input["T_mean"]
if "RH" in feat_index:
    today_vec[feat_index["RH"]] = weather_input["RH_mean"]
if "WSPD" in feat_index:
    today_vec[feat_index["WSPD"]] = weather_input["WSPD_mean"]
if "TP_mm" in feat_index:
    today_vec[feat_index["TP_mm"]] = weather_input["TP_sum"]
if "DWI" in feat_index:
    today_vec[feat_index["DWI"]] = dwi_today

seq_rows.append(today_vec)

seq_feat = np.stack(seq_rows, axis=0)
seq_feat = seq_feat.reshape(1, LOOKBACK, n_features)
seq_feat_s = transform_X(seq_feat)

# 11) GRU ëª¨ë¸ ë¡œë“œ ë° 7ì¼ ì˜ˆì¸¡ ìˆ˜í–‰
if IN_COLAB:
    print("gru ëª¨ë¸ ì—…ë¡œë“œ (ì˜ˆ: gru_spring.h5)")
    uploaded_model = files.upload()
    model_name = list(uploaded_model.keys())[0]
else:
    model_name = f"gru_{SEASON}.h5"

model = load_model(model_name, compile=False)

print("model.input_shape:", model.input_shape)
print("ì…ë ¥ seq_feat_s.shape:", seq_feat_s.shape)

y_pred = model.predict(seq_feat_s)[0]

# 12) ì˜¤ëŠ˜ FFDRI_new ê³„ì‚° (ë³´ê³ ìš©)
FMI_  = today_vec[feat_index["FMI"]]           if "FMI" in feat_index else np.nan
TMI_  = today_vec[feat_index["TMI"]]           if "TMI" in feat_index else np.nan
NDVI_ = today_vec[feat_index["NDVI"]]          if "NDVI" in feat_index else np.nan
SUN_  = today_vec[feat_index["sunlight_era5"]] if "sunlight_era5" in feat_index else np.nan

ff_today = compute_FFDRI_new(dwi_today, FMI_, TMI_, SUN_, NDVI_)

# 13) ìœ„í—˜ë“±ê¸‰ êµ¬ê°„ ì •ì˜ (ë¶„ìœ„ìˆ˜ ê¸°ë°˜)
q25 = df["FFDRI_new"].quantile(0.25)
q50 = df["FFDRI_new"].quantile(0.50)
q75 = df["FFDRI_new"].quantile(0.75)
q90 = df["FFDRI_new"].quantile(0.90)

print("FFDRI_new quantiles (season =", SEASON, ")")
print("Q25:", q25, " Q50:", q50, " Q75:", q75, " Q90:", q90)

def risk(v):
    if v < q25:
        return "low"
    elif v < q50:
        return "moderate"
    elif v < q75:
        return "high"
    elif v < q90:
        return "very_high"
    else:
        return "extreme"

# 14) ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
print("\nì˜ˆì¸¡ ê²°ê³¼ (GEE ê¸°ë°˜)")
print("ì˜ˆì¸¡ ê¸°ì¤€ì¼:", FORECAST_DATE)
print("ì…ë ¥ ì¢Œí‘œ:", NEW_LAT, NEW_LON)
print("GEE(ERA5-Land) ì…ë ¥:", weather_input)
print("DWI:", dwi_today)
print("ì˜¤ëŠ˜ FFDRI_new ê³„ì‚°ê°’:", ff_today)

for i in range(HORIZON):
    print(f"D+{i+1}: {y_pred[i]:.3f} | ìœ„í—˜ë“±ê¸‰: {risk(y_pred[i]+5.5)}")

# 15) CSV ì €ì¥
out = pd.DataFrame({
    "base_date": [FORECAST_DATE],
    "lat": [NEW_LAT],
    "lon": [NEW_LON],
    "FFDRI_new_today": [ff_today],
})

for i in range(HORIZON):
    out[f"FFDRI_new_model_D{i+1}"] = [y_pred[i]]
    out[f"risk_D{i+1}"] = [risk(y_pred[i])]

out.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

if IN_COLAB:
    files.download(OUTPUT_CSV)



### ê³„ì ˆ ë¶„ë¦¬ ì ìš© í…ŒìŠ¤íŠ¸ (Open-Meteo + GRU)

import io
import requests
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

from sklearn.preprocessing import StandardScaler

try:
    from google.colab import files
    IN_COLAB = True
except ImportError:
    IN_COLAB = False
    files = None

import tensorflow as tf
from tensorflow.keras.models import load_model

# 0) ì‚¬ìš©ì ì…ë ¥
NEW_LAT = 35.9181780434288
NEW_LON = 128.566700685493
NEIGHBOR_KM = 5

SEASON   = "spring"
TIMEZONE = "Asia/Seoul"

LOOKBACK = 14
HORIZON  = 7

today = datetime.now().date()
OUTPUT_CSV = f"ffdri_new_forecast_NEWPOINT_{SEASON}_{today}_7d.csv"

print("ì…ë ¥ ì¢Œí‘œ:", NEW_LAT, NEW_LON)
print("ì„ íƒ ì‹œì¦Œ:", SEASON)

# 1) lstm_dataset.csv ë¡œë“œ
if IN_COLAB:
    print("lstm_dataset.csv ì—…ë¡œë“œ")
    uploaded = files.upload()
    csv_name = list(uploaded.keys())[0]
else:
    csv_name = "lstm_dataset.csv"

df = pd.read_csv(csv_name, parse_dates=["date"])
df = df.sort_values(["pid", "date"]).reset_index(drop=True)

if "season" not in df.columns:
    df["month"] = df["date"].dt.month
    df["season"] = df["month"].apply(
        lambda m: "spring" if m in [2, 3, 4]
        else ("fall" if m in [10, 11, 12] else "other")
    )

df = df[df["season"] == SEASON].copy()
if df.empty:
    raise ValueError(f"{SEASON} ì‹œì¦Œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

df["doy"] = df["date"].dt.dayofyear

# 2) Feature / Target ì •ì˜
TARGET = "FFDRI_new"
if TARGET not in df.columns:
    raise ValueError("FFDRI_new ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤. (í•™ìŠµ ë•Œì™€ ë™ì¼ CSV)")

candidate_features = [
    "Tmean", "RH", "WSPD", "TP_mm",
    "sunlight_era5", "DWI", "FMI", "TMI",
    "NDVI", "day_weight", "FFDRI"
]

FEATURES = [c for c in candidate_features if c in df.columns and c != TARGET]
n_features = len(FEATURES)

if n_features == 0:
    raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ Featureê°€ ì—†ìŠµë‹ˆë‹¤. candidate_featuresë¥¼ í™•ì¸í•˜ì„¸ìš”.")

print("ì‚¬ìš© Feature:", FEATURES)
print("Feature ê°œìˆ˜:", n_features)

# 3) GRU í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì‹œí€€ìŠ¤/ìŠ¤ì¼€ì¼ëŸ¬ ë³µì›
def make_sequences(sub_df):
    X_list, y_list = [], []

    for pid, g in sub_df.groupby("pid"):
        g = g.sort_values("date").reset_index(drop=True)

        X_raw = g[FEATURES].values
        y_raw = g[TARGET].values

        if len(g) < LOOKBACK + HORIZON:
            continue

        for i in range(LOOKBACK, len(g) - HORIZON + 1):
            X_list.append(X_raw[i-LOOKBACK:i])
            y_list.append(y_raw[i:i+HORIZON])

    if len(X_list) == 0:
        return None, None

    return np.array(X_list, dtype="float32"), np.array(y_list, dtype="float32")

X_all, y_all = make_sequences(df)
if X_all is None:
    raise ValueError("ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë°ì´í„° ê¸¸ì´ ë¶€ì¡±)")

N = X_all.shape[0]
idx = int(N * 0.8)
X_train = X_all[:idx]

scaler = StandardScaler()
X_train_2d = X_train.reshape(-1, n_features)
scaler.fit(X_train_2d)

def transform_X(z):
    z2 = z.reshape(-1, n_features)
    z2 = scaler.transform(z2)
    return z2.reshape(z.shape)

# 4) ì£¼ë³€ í•™ìŠµ ì¢Œí‘œ íƒìƒ‰
pid_locs = df.groupby("pid")[["lat", "lon"]].first().reset_index()

def haversine(lat1, lon1, lat2, lon2):
    R = 6371.0
    dlat = np.radians(lat2 - lat1)
    dlon = np.radians(lon2 - lon1)
    a = (
        np.sin(dlat/2)**2
        + np.cos(np.radians(lat1))*np.cos(np.radians(lat2))
        * np.sin(dlon/2)**2
    )
    return 2 * R * np.arcsin(np.sqrt(a))

pid_locs["dist_km"] = pid_locs.apply(
    lambda row: haversine(NEW_LAT, NEW_LON, row["lat"], row["lon"]),
    axis=1
)

near = pid_locs[pid_locs["dist_km"] <= NEIGHBOR_KM]

if near.empty:
    print(f"ì£¼ë³€ {NEIGHBOR_KM}km ë‚´ í•™ìŠµì¢Œí‘œ ì—†ìŒ â†’ ì „ì²´ pid ì‚¬ìš©")
    near_pids = pid_locs["pid"].tolist()
else:
    print(f"{len(near)}ê°œ í•™ìŠµì¢Œí‘œê°€ {NEIGHBOR_KM}km ë‚´ ì¡´ì¬")
    near_pids = near["pid"].tolist()

# 5) DOYë³„ Feature í´ë¼ì´ëª¨
clim_features = df.groupby(["pid", "doy"])[FEATURES].mean()

def get_climo_row_for_date(d):
    doy = d.timetuple().tm_yday
    rows = []

    for pid in near_pids:
        key = (pid, doy)
        if key in clim_features.index:
            rows.append(clim_features.loc[key].values)

    if rows:
        return np.mean(rows, axis=0).astype("float32")
    return clim_features.mean().values.astype("float32")

# 6) Open-Meteoì—ì„œ ì˜¤ëŠ˜ ê¸°ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
def fetch_today_weather(lat, lon):
    url = "https://api.open-meteo.com/v1/forecast"
    params = {
        "latitude": lat,
        "longitude": lon,
        "hourly": "temperature_2m,relative_humidity_2m,wind_speed_10m,precipitation",
        "forecast_days": 1,
        "timezone": TIMEZONE,
    }
    res = requests.get(url, params=params)
    res.raise_for_status()
    data = res.json()

    h = pd.DataFrame(data["hourly"])
    h["time"] = pd.to_datetime(h["time"])
    h["date"] = h["time"].dt.date
    today_block = h[h["date"] == today]

    if today_block.empty:
        raise RuntimeError("Open-Meteo ê²°ê³¼ì—ì„œ ì˜¤ëŠ˜ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    return {
        "T_max":   float(today_block["temperature_2m"].max()),
        "RH_mean": float(today_block["relative_humidity_2m"].mean()),
        "WSPD_mean": float(today_block["wind_speed_10m"].mean()),
        "TP_sum":  float(today_block["precipitation"].sum())
    }

# 7) ì˜¤ëŠ˜ DWI ê³„ì‚° (ê³„ì ˆë³„ ì‹)
def pre_spring(T, RH, W):
    return 1/(1+np.exp(2.706 + 0.088*T - 0.055*RH - 0.023*RH - 0.104*W))

def pre_fall(T, RH, W):
    return 1/(1+np.exp(1.099 + 0.117*T - 0.069*RH - 0.182*W))

def compute_DWI(weather):
    if SEASON == "spring":
        pre = pre_spring(weather["T_max"], weather["RH_mean"], weather["WSPD_mean"])
    else:
        pre = pre_fall(weather["T_max"], weather["RH_mean"], weather["WSPD_mean"])

    tp = weather["TP_sum"]
    if tp >= 10:
        rne = 0.3
    elif tp >= 1:
        rne = 0.7
    else:
        rne = 1.0
    return pre * rne

weather_today = fetch_today_weather(NEW_LAT, NEW_LON)
dwi_today = compute_DWI(weather_today)

# 8) FFDRI_new ê³„ì‚° í•¨ìˆ˜
def compute_FFDRI_new(dwi, fmi, tmi, sunlight, ndvi):
    return (
        5.3012
        + 6.6232*dwi
        + 0.7657*fmi
        + 1.0994*tmi
        + 0.0906*sunlight
        - 10.6796*ndvi
    )

# 9) ìƒˆ ì¢Œí‘œì˜ 14ì¼ì¹˜ Feature ì‹œí€€ìŠ¤ ë§Œë“¤ê¸°
feat_index = {name: i for i, name in enumerate(FEATURES)}
seq_rows = []

for offset in range(LOOKBACK-1, 0, -1):
    d = today - timedelta(days=offset)
    row = get_climo_row_for_date(d)
    seq_rows.append(row)

today_feat = get_climo_row_for_date(today)

if "Tmean" in feat_index:
    today_feat[feat_index["Tmean"]] = weather_today["T_max"]
if "RH" in feat_index:
    today_feat[feat_index["RH"]] = weather_today["RH_mean"]
if "WSPD" in feat_index:
    today_feat[feat_index["WSPD"]] = weather_today["WSPD_mean"]
if "TP_mm" in feat_index:
    today_feat[feat_index["TP_mm"]] = weather_today["TP_sum"]
if "DWI" in feat_index:
    today_feat[feat_index["DWI"]] = dwi_today

seq_rows.append(today_feat)

seq_feat = np.stack(seq_rows, axis=0)
seq_feat = seq_feat.reshape(1, LOOKBACK, n_features)

# 10) ì…ë ¥ Feature í‘œì¤€í™”
seq_feat_s = transform_X(seq_feat)

# 11) GRU ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡
if IN_COLAB:
    print("GRU ëª¨ë¸(.h5) ì—…ë¡œë“œ (ì˜ˆ: gru_fall.h5)")
    uploaded_model = files.upload()
    model_name = list(uploaded_model.keys())[0]
else:
    model_name = f"gru_{SEASON}.h5"

model = load_model(model_name, compile=False)
print("model.input_shape:", model.input_shape)
print("ì…ë ¥ seq_feat_s.shape:", seq_feat_s.shape)

y_pred = model.predict(seq_feat_s)[0]

# 12) ì˜¤ëŠ˜ FFDRI_new ê³„ì‚° (ë³´ê³ ìš©)
FMI_   = today_feat[feat_index["FMI"]]   if "FMI" in feat_index else np.nan
TMI_   = today_feat[feat_index["TMI"]]   if "TMI" in feat_index else np.nan
NDVI_  = today_feat[feat_index["NDVI"]]  if "NDVI" in feat_index else np.nan
SUN_   = today_feat[feat_index["sunlight_era5"]] if "sunlight_era5" in feat_index else np.nan

ff_today = compute_FFDRI_new(dwi_today, FMI_, TMI_, SUN_, NDVI_)

# 13) ìœ„í—˜ë“±ê¸‰ êµ¬ê°„ ì •ì˜ (ë¶„ìœ„ìˆ˜ ê¸°ë°˜)
q25 = df["FFDRI_new"].quantile(0.25)
q50 = df["FFDRI_new"].quantile(0.50)
q75 = df["FFDRI_new"].quantile(0.75)
q90 = df["FFDRI_new"].quantile(0.90)

print("FFDRI_new quantiles (season =", SEASON, ")")
print("Q25:", q25, " Q50:", q50, " Q75:", q75, " Q90:", q90)

def risk(v):
    if v < q25:
        return "low"
    elif v < q50:
        return "moderate"
    elif v < q75:
        return "high"
    elif v < q90:
        return "very_high"
    else:
        return "extreme"

# 14) ê²°ê³¼ ì¶œë ¥
print("\nì˜ˆì¸¡ ê²°ê³¼ (ìƒˆ ì¢Œí‘œ)")
print("ì…ë ¥ ì¢Œí‘œ :", NEW_LAT, NEW_LON)
print("ì˜¤ëŠ˜ ë‚ ì§œ :", today)
print("ì˜¤ëŠ˜ FFDRI_new (ê³„ì‚°ê°’) :", ff_today)
print("Open-Meteo ì˜¤ëŠ˜ T/RH/WSPD/TP:", weather_today)
print("ì˜¤ëŠ˜ DWI :", dwi_today)

for h in range(HORIZON):
    print(f"D+{h+1} = {y_pred[h]:.3f} | ìœ„í—˜ë“±ê¸‰: {risk(y_pred[h])}")

# 15) CSV ì €ì¥
out = pd.DataFrame({
    "base_date": [today],
    "lat": [NEW_LAT],
    "lon": [NEW_LON],
    "FFDRI_new_today": [ff_today],
})

for h in range(HORIZON):
    out[f"FFDRI_new_model_{h+1}d"] = [y_pred[h]]
    out[f"risk_{h+1}d"] = [risk(y_pred[h])]

out.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

if IN_COLAB:
    files.download(OUTPUT_CSV)



### ê²½ë¶Â·ëŒ€êµ¬ ëŒ€í‘œì‚° ìœ„í—˜ë„ ì˜ˆì¸¡ ë° ì§€ë„ ì‹œê°í™”

import os
import requests
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

import matplotlib.pyplot as plt

try:
    from google.colab import files
except ImportError:
    files = None

import tensorflow as tf
from tensorflow.keras.models import load_model

import geopandas as gpd
from shapely.geometry import Point
from matplotlib.colors import ListedColormap, BoundaryNorm

print("TF version:", tf.__version__)

# 0) ì‚¬ìš©ì ì„¤ì •
SEASON   = "fall"
SEQ_LEN  = 30
HORIZON  = 7
TARGET_H = 3
TIMEZONE = "Asia/Seoul"
NEIGHBOR_KM = 5

today = datetime.now().date()

SITES_INFO = [
    {"name": "Hamji-san",       "lat": 35.9181780434288, "lon": 128.566700685493},
    {"name": "Palgong-san",     "lat": 36.0225318418839, "lon": 128.736206404928},
    {"name": "Geumo-san",       "lat": 36.0923500736404, "lon": 128.301361851721},
    {"name": "Juwang-san",      "lat": 36.3938590686737, "lon": 129.142072525794},
    {"name": "Cheongnyang-san", "lat": 36.8013774501963, "lon": 128.939160702182},
]

def get_season_from_date(d):
    m = d.month
    return "spring" if 1 <= m <= 7 else "fall"

# 1) lstm_dataset.csv ì—…ë¡œë“œ
print("\ncsv íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
uploaded = files.upload()
csv_name = list(uploaded.keys())[0]
df = pd.read_csv(csv_name, parse_dates=["date"])
df = df.sort_values(["pid", "date"])

if "season" in df.columns and SEASON in ("spring", "fall"):
    df = df[df["season"] == SEASON]

df["doy"] = df["date"].dt.dayofyear

# 2) FFDRI_new ê³„ì‚°
sun_cols = [c for c in df.columns if "sun" in c.lower()]
ndvi_cols = [c for c in df.columns if "ndvi" in c.lower()]

SUN_COL = sun_cols[0]
NDVI_COL = ndvi_cols[0]

def compute_FFDRI_new(dwi, fmi, tmi, sun, ndvi):
    return (
        5.3012
        + 6.6232*dwi
        + 0.7657*fmi
        + 1.0994*tmi
        + 0.0906*sun
        - 10.6796*ndvi
    )

df["FFDRI_new"] = compute_FFDRI_new(
    df["DWI"], df["FMI"], df["TMI"], df[SUN_COL], df[NDVI_COL]
)

ffdri_mean = df["FFDRI_new"].mean()
ffdri_std  = df["FFDRI_new"].std()

def standardize(x):
    return (x - ffdri_mean) / ffdri_std

def inv_standardize(x):
    return x * ffdri_std + ffdri_mean

# 3) GRU ëª¨ë¸ ì—…ë¡œë“œ
print("\nGRU ëª¨ë¸(.h5)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
uploaded_model = files.upload()
model_name = list(uploaded_model.keys())[0]
model = load_model(model_name, compile=False)

# 4) Open-Meteo ê¸°ë°˜ ì˜¤ëŠ˜ ê¸°ìƒ â†’ DWI ê³„ì‚°
def fetch_today_weather(lat, lon):
    url = "https://api.open-meteo.com/v1/forecast"
    params = {
        "latitude": lat,
        "longitude": lon,
        "hourly": "temperature_2m,relative_humidity_2m,wind_speed_10m,precipitation",
        "forecast_days": 1,
        "timezone": TIMEZONE,
    }
    data = requests.get(url, params=params).json()
    h = pd.DataFrame(data["hourly"])
    h["time"] = pd.to_datetime(h["time"])
    h["date"] = h["time"].dt.date
    block = h[h["date"] == today]
    if block.empty:
        block = h
    return {
        "T_max": block["temperature_2m"].max(),
        "RH_mean": block["relative_humidity_2m"].mean(),
        "WSPD_mean": block["wind_speed_10m"].mean(),
        "TP_sum": block["precipitation"].sum(),
    }

def pre_spring(T, RH, EH):
    return 1/(1+np.exp(2.706 + 0.088*T - 0.055*RH - 0.023*EH))

def pre_fall(T, RH, EH):
    return 1/(1+np.exp(1.099 + 0.117*T - 0.069*RH - 0.182*EH))

def compute_DWI_from_weather(w, season):
    if season == "auto":
        season = get_season_from_date(today)
    if season == "spring":
        pre = pre_spring(w["T_max"], w["RH_mean"], w["WSPD_mean"])
    else:
        pre = pre_fall(w["T_max"], w["RH_mean"], w["WSPD_mean"])
    tp = w["TP_sum"]
    if tp >= 10:
        rne = 0.3
    elif tp >= 1:
        rne = 0.7
    else:
        rne = 1.0
    return pre * rne

# 5) 5ê°œ ì‚°ì— ëŒ€í•´ 7ì¼ ì˜ˆì¸¡ ìˆ˜í–‰
pid_locs = df.groupby("pid")[["lat", "lon"]].first().reset_index()
clim_ffdri = df.groupby(["pid", "doy"])["FFDRI_new"].mean()

def haversine(lat1, lon1, lat2, lon2):
    R = 6371
    dlat = np.radians(lat2 - lat1)
    dlon = np.radians(lon2 - lon1)
    a = np.sin(dlat/2)**2 + np.cos(np.radians(lat1))*np.cos(np.radians(lat2))*np.sin(dlon/2)**2
    return 2 * R * np.arcsin(np.sqrt(a))

q25 = df["FFDRI_new"].quantile(0.25)
q50 = df["FFDRI_new"].quantile(0.50)
q75 = df["FFDRI_new"].quantile(0.75)
q90 = df["FFDRI_new"].quantile(0.90)

def risk_level(v):
    if v < q25:
        return "low"
    elif v < q50:
        return "moderate"
    elif v < q75:
        return "high"
    elif v < q90:
        return "very_high"
    else:
        return "extreme"

sites_records = []

for site in SITES_INFO:
    lat, lon = site["lat"], site["lon"]

    pid_locs["dist"] = pid_locs.apply(
        lambda r: haversine(lat, lon, r["lat"], r["lon"]), axis=1
    )
    near = pid_locs[pid_locs["dist"] <= NEIGHBOR_KM]
    if near.empty:
        near_pids = pid_locs["pid"].tolist()
    else:
        near_pids = near["pid"].tolist()

    w = fetch_today_weather(lat, lon)
    dwi_today = compute_DWI_from_weather(w, SEASON)

    doy_today = today.timetuple().tm_yday
    sub = df[df["pid"].isin(near_pids)]
    sub_today = sub[sub["doy"] == doy_today]

    if sub_today.empty:
        FMI_  = sub["FMI"].mean()
        TMI_  = sub["TMI"].mean()
        NDVI_ = sub[NDVI_COL].mean()
        SUN_  = sub[SUN_COL].mean()
    else:
        FMI_  = sub_today["FMI"].mean()
        TMI_  = sub_today["TMI"].mean()
        NDVI_ = sub_today[NDVI_COL].mean()
        SUN_  = sub_today[SUN_COL].mean()

    ff_today = compute_FFDRI_new(dwi_today, FMI_, TMI_, SUN_, NDVI_)

    seq = []
    for off in range(SEQ_LEN-1, 0, -1):
        d = today - timedelta(days=off)
        doy = d.timetuple().tm_yday
        vals = []
        for pid in near_pids:
            key = (pid, doy)
            if key in clim_ffdri.index:
                vals.append(float(clim_ffdri.loc[key]))
        seq.append(np.mean(vals) if vals else clim_ffdri.mean())
    seq.append(ff_today)

    seq_std = standardize(np.array(seq)).reshape(1, SEQ_LEN, 1)
    y_std = model.predict(seq_std, verbose=0)
    y_pred = inv_standardize(y_std)[0]

    rec = {"name": site["name"], "lat": lat, "lon": lon, "FFDRI_today": ff_today}
    for i in range(HORIZON):
        rec[f"FFDRI_D{i+1}"] = float(y_pred[i])
        rec[f"risk_D{i+1}"] = risk_level(float(y_pred[i]))
    sites_records.append(rec)

sites_df = pd.DataFrame(sites_records)

# 6) D+TARGET_H ìœ„í—˜ë„ ì¸ë±ìŠ¤
risk_order = ["low", "moderate", "high", "very_high", "extreme"]
risk_to_idx = {r: i+1 for i, r in enumerate(risk_order)}

sites_df["risk_index"] = sites_df[f"risk_D{TARGET_H}"].map(risk_to_idx)

# 7) í–‰ì •ê²½ê³„ Shapefile ì—…ë¡œë“œ ë° ì¢Œí‘œê³„ ì„¤ì •
print("\nShapefile (.shp, .shx, .dbf, .prj)ì„ ëª¨ë‘ ì—…ë¡œë“œí•˜ì„¸ìš”.")
uploaded_shp = files.upload()

shp_files = [n for n in uploaded_shp.keys() if n.lower().endswith(".shp")]
boundary_shp = shp_files[0]

os.environ["SHAPE_RESTORE_SHX"] = "YES"
boundary = gpd.read_file(boundary_shp)

boundary = boundary.set_crs(epsg=5179, allow_override=True)
boundary_4326 = boundary.to_crs(epsg=4326)

b = boundary_4326.geometry.bounds
boundary_4326 = boundary_4326[b["maxx"] < 131]

g_sites = gpd.GeoDataFrame(
    sites_df,
    geometry=[Point(xy) for xy in zip(sites_df["lon"], sites_df["lat"])],
    crs="EPSG:4326"
)

# 8) ì§€ë„ ì‹œê°í™” (ì˜ë¬¸ íƒ€ì´í‹€)
fig, ax = plt.subplots(figsize=(10, 7))

boundary_4326.plot(ax=ax, color="white", edgecolor="black")

risk_colors = [
    "#FFCCCC",
    "#FF6666",
    "#FF0000",
    "#CC0000",
    "#800000",
]
cmap = ListedColormap(risk_colors)
bounds = np.arange(0.5, 6.5, 1)
norm = BoundaryNorm(bounds, cmap.N)

g_sites.plot(
    ax=ax,
    column="risk_index",
    cmap=cmap,
    norm=norm,
    markersize=180,
    edgecolor="black",
    linewidth=0.8,
    legend=True,
    vmin=1,
    vmax=5,
)

for _, row in g_sites.iterrows():
    ax.text(
        row.geometry.x + 0.02,
        row.geometry.y + 0.02,
        row["name"],
        fontsize=12
    )

ax.set_aspect("equal", adjustable="datalim")
ax.set_xlabel("Longitude", fontsize=12)
ax.set_ylabel("Latitude", fontsize=12)

forecast_date = today + timedelta(days=TARGET_H)

ax.set_title(
    f"FFDRI_new Risk Map (Forecast Date: {forecast_date}, D+{TARGET_H})",
    fontsize=16
)

minx, miny, maxx, maxy = boundary_4326.total_bounds
ax.set_xlim(minx-0.05, maxx+0.05)
ax.set_ylim(miny+0.02, maxy-0.02)

plt.tight_layout()
plt.show()

# 9) CSV ì €ì¥
out_csv = f"FFD_forecast_D{TARGET_H}_{today}.csv"
sites_df.to_csv(out_csv, index=False, encoding="utf-8-sig")
files.download(out_csv)

# 10) PNG ì €ì¥
out_png = f"FFD_RISKMAP_D{TARGET_H}_{today}.png"
fig.savefig(out_png, dpi=300, bbox_inches="tight")
files.download(out_png)

